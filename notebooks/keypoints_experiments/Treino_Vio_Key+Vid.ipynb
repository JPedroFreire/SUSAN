{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Feature vectors"
      ],
      "metadata": {
        "id": "qW7NGwo4fiMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Quve2Ppjltl0",
        "outputId": "e6f9ec20-b5c9-49d0-dc3c-08fbddc49b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "QKGRay9KlyAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "import cv2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6JFGDTGblUz",
        "outputId": "9085eb4a-5254-4f0a-8453-a1508186e896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_files(folder_path):\n",
        "    video_files = []\n",
        "\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        subfolder_list = root.split(os.path.sep)\n",
        "        v_nv = subfolder_list[-2]\n",
        "        c1_c2 = subfolder_list[-1]\n",
        "\n",
        "        video_files.extend([(v_nv, c1_c2, file) for file in files if file.lower().endswith(('.mp4', '.jpg'))])\n",
        "\n",
        "    return video_files"
      ],
      "metadata": {
        "id": "F1QLkJHNl8Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyframes_dataset_path = '/content/drive/MyDrive/AIRTLab/'\n",
        "\n",
        "image_files_list = get_data_files(keyframes_dataset_path)"
      ],
      "metadata": {
        "id": "ytuZnsQ0l1E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ('filename','keypoints', 'label', 'set_split')\n",
        "\n",
        "data = []\n",
        "\n",
        "model = YOLO(\"yolov8n-pose.pt\")\n",
        "\n",
        "for set_split, label, image_file in image_files_list:\n",
        "\n",
        "  results  = []\n",
        "\n",
        "  file_path = f'{keyframes_dataset_path}/{set_split}/{label}/{image_file}'\n",
        "\n",
        "  estimation_result = model(file_path)\n",
        "\n",
        "  for r in estimation_result:\n",
        "    results.append(r.keypoints.xy.cpu().numpy())\n",
        "\n",
        "  data.append((image_file, results, label, set_split))\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data, columns=columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BJMZ-u3l9Da",
        "outputId": "d7b0e01f-e6c0-4963-9a16-e9d981ac0b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f112.jpg: 384x640 4 persons, 186.0ms\n",
            "Speed: 3.1ms preprocess, 186.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f109.jpg: 384x640 4 persons, 172.9ms\n",
            "Speed: 3.2ms preprocess, 172.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f114.jpg: 384x640 4 persons, 170.4ms\n",
            "Speed: 4.5ms preprocess, 170.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f105.jpg: 384x640 3 persons, 162.6ms\n",
            "Speed: 3.2ms preprocess, 162.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f83.jpg: 384x640 4 persons, 168.3ms\n",
            "Speed: 3.1ms preprocess, 168.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f72.jpg: 384x640 4 persons, 162.7ms\n",
            "Speed: 3.1ms preprocess, 162.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f108.jpg: 384x640 4 persons, 166.8ms\n",
            "Speed: 3.1ms preprocess, 166.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f159.jpg: 384x640 4 persons, 185.0ms\n",
            "Speed: 3.1ms preprocess, 185.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f164.jpg: 384x640 4 persons, 161.4ms\n",
            "Speed: 3.0ms preprocess, 161.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f5.jpg: 384x640 4 persons, 180.0ms\n",
            "Speed: 3.7ms preprocess, 180.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f6.jpg: 384x640 4 persons, 171.1ms\n",
            "Speed: 3.1ms preprocess, 171.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f148.jpg: 384x640 4 persons, 185.2ms\n",
            "Speed: 2.9ms preprocess, 185.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f14.jpg: 384x640 5 persons, 165.7ms\n",
            "Speed: 3.1ms preprocess, 165.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f143.jpg: 384x640 4 persons, 188.8ms\n",
            "Speed: 3.3ms preprocess, 188.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f149.jpg: 384x640 4 persons, 259.5ms\n",
            "Speed: 6.8ms preprocess, 259.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f3.jpg: 384x640 4 persons, 253.3ms\n",
            "Speed: 4.6ms preprocess, 253.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v61f165.jpg: 384x640 4 persons, 250.5ms\n",
            "Speed: 4.4ms preprocess, 250.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f47.jpg: 384x640 4 persons, 253.2ms\n",
            "Speed: 4.4ms preprocess, 253.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f55.jpg: 384x640 5 persons, 159.4ms\n",
            "Speed: 3.8ms preprocess, 159.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f34.jpg: 384x640 6 persons, 163.1ms\n",
            "Speed: 3.2ms preprocess, 163.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f49.jpg: 384x640 5 persons, 171.1ms\n",
            "Speed: 4.4ms preprocess, 171.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f30.jpg: 384x640 5 persons, 157.2ms\n",
            "Speed: 3.0ms preprocess, 157.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f41.jpg: 384x640 6 persons, 159.6ms\n",
            "Speed: 3.0ms preprocess, 159.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f58.jpg: 384x640 5 persons, 161.3ms\n",
            "Speed: 3.2ms preprocess, 161.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f60.jpg: 384x640 6 persons, 166.0ms\n",
            "Speed: 3.1ms preprocess, 166.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f61.jpg: 384x640 6 persons, 182.3ms\n",
            "Speed: 3.1ms preprocess, 182.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f45.jpg: 384x640 6 persons, 155.1ms\n",
            "Speed: 3.5ms preprocess, 155.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f70.jpg: 384x640 5 persons, 186.7ms\n",
            "Speed: 3.1ms preprocess, 186.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f94.jpg: 384x640 3 persons, 161.5ms\n",
            "Speed: 3.1ms preprocess, 161.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f104.jpg: 384x640 5 persons, 181.6ms\n",
            "Speed: 3.0ms preprocess, 181.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f102.jpg: 384x640 5 persons, 159.8ms\n",
            "Speed: 2.9ms preprocess, 159.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f88.jpg: 384x640 6 persons, 173.6ms\n",
            "Speed: 4.4ms preprocess, 173.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f83.jpg: 384x640 5 persons, 173.4ms\n",
            "Speed: 3.1ms preprocess, 173.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f119.jpg: 384x640 4 persons, 190.1ms\n",
            "Speed: 3.1ms preprocess, 190.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f124.jpg: 384x640 4 persons, 248.8ms\n",
            "Speed: 5.0ms preprocess, 248.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f67.jpg: 384x640 5 persons, 282.0ms\n",
            "Speed: 4.6ms preprocess, 282.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f109.jpg: 384x640 6 persons, 256.3ms\n",
            "Speed: 4.6ms preprocess, 256.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f5.jpg: 384x640 4 persons, 296.4ms\n",
            "Speed: 5.2ms preprocess, 296.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f149.jpg: 384x640 4 persons, 172.5ms\n",
            "Speed: 3.3ms preprocess, 172.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f151.jpg: 384x640 4 persons, 157.4ms\n",
            "Speed: 2.9ms preprocess, 157.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f125.jpg: 384x640 4 persons, 169.6ms\n",
            "Speed: 4.4ms preprocess, 169.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f128.jpg: 384x640 4 persons, 161.9ms\n",
            "Speed: 3.1ms preprocess, 161.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f142.jpg: 384x640 3 persons, 156.2ms\n",
            "Speed: 3.0ms preprocess, 156.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f146.jpg: 384x640 3 persons, 208.1ms\n",
            "Speed: 3.1ms preprocess, 208.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f18.jpg: 384x640 4 persons, 171.5ms\n",
            "Speed: 3.4ms preprocess, 171.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f0.jpg: 384x640 4 persons, 191.7ms\n",
            "Speed: 3.1ms preprocess, 191.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f130.jpg: 384x640 4 persons, 172.5ms\n",
            "Speed: 3.2ms preprocess, 172.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f7.jpg: 384x640 4 persons, 196.3ms\n",
            "Speed: 3.5ms preprocess, 196.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f14.jpg: 384x640 4 persons, 166.3ms\n",
            "Speed: 3.0ms preprocess, 166.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v9f145.jpg: 384x640 3 persons, 177.4ms\n",
            "Speed: 3.6ms preprocess, 177.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f11.jpg: 384x640 4 persons, 215.9ms\n",
            "Speed: 3.9ms preprocess, 215.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f45.jpg: 384x640 4 persons, 179.8ms\n",
            "Speed: 3.3ms preprocess, 179.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f67.jpg: 384x640 4 persons, 212.0ms\n",
            "Speed: 3.1ms preprocess, 212.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f54.jpg: 384x640 4 persons, 167.7ms\n",
            "Speed: 3.0ms preprocess, 167.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f50.jpg: 384x640 5 persons, 285.8ms\n",
            "Speed: 5.6ms preprocess, 285.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f56.jpg: 384x640 4 persons, 298.7ms\n",
            "Speed: 5.2ms preprocess, 298.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f24.jpg: 384x640 4 persons, 315.6ms\n",
            "Speed: 5.7ms preprocess, 315.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f25.jpg: 384x640 4 persons, 235.9ms\n",
            "Speed: 5.0ms preprocess, 235.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f66.jpg: 384x640 4 persons, 175.0ms\n",
            "Speed: 4.2ms preprocess, 175.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f68.jpg: 384x640 4 persons, 165.1ms\n",
            "Speed: 3.2ms preprocess, 165.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f20.jpg: 384x640 4 persons, 170.1ms\n",
            "Speed: 4.5ms preprocess, 170.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f74.jpg: 384x640 4 persons, 189.3ms\n",
            "Speed: 3.1ms preprocess, 189.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f26.jpg: 384x640 3 persons, 175.8ms\n",
            "Speed: 3.1ms preprocess, 175.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v70f85.jpg: 384x640 4 persons, 195.1ms\n",
            "Speed: 3.1ms preprocess, 195.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f2.jpg: 384x640 2 persons, 168.9ms\n",
            "Speed: 3.1ms preprocess, 168.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f6.jpg: 384x640 2 persons, 168.8ms\n",
            "Speed: 3.1ms preprocess, 168.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f20.jpg: 384x640 2 persons, 168.8ms\n",
            "Speed: 3.4ms preprocess, 168.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f32.jpg: 384x640 2 persons, 169.5ms\n",
            "Speed: 3.8ms preprocess, 169.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f19.jpg: 384x640 2 persons, 168.7ms\n",
            "Speed: 3.4ms preprocess, 168.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f14.jpg: 384x640 2 persons, 171.3ms\n",
            "Speed: 3.4ms preprocess, 171.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f52.jpg: 384x640 1 person, 202.6ms\n",
            "Speed: 3.5ms preprocess, 202.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f58.jpg: 384x640 1 person, 200.9ms\n",
            "Speed: 3.4ms preprocess, 200.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f82.jpg: 384x640 2 persons, 195.2ms\n",
            "Speed: 3.5ms preprocess, 195.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f76.jpg: 384x640 2 persons, 181.7ms\n",
            "Speed: 3.6ms preprocess, 181.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f69.jpg: 384x640 1 person, 254.6ms\n",
            "Speed: 4.9ms preprocess, 254.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f85.jpg: 384x640 2 persons, 255.0ms\n",
            "Speed: 4.5ms preprocess, 255.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f74.jpg: 384x640 1 person, 259.4ms\n",
            "Speed: 4.5ms preprocess, 259.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f38.jpg: 384x640 3 persons, 260.1ms\n",
            "Speed: 5.2ms preprocess, 260.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f54.jpg: 384x640 1 person, 188.9ms\n",
            "Speed: 4.5ms preprocess, 188.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f68.jpg: 384x640 2 persons, 163.9ms\n",
            "Speed: 3.1ms preprocess, 163.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f72.jpg: 384x640 1 person, 171.6ms\n",
            "Speed: 3.1ms preprocess, 171.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f42.jpg: 384x640 2 persons, 168.6ms\n",
            "Speed: 3.0ms preprocess, 168.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f51.jpg: 384x640 1 person, 164.5ms\n",
            "Speed: 3.0ms preprocess, 164.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f92.jpg: 384x640 2 persons, 174.1ms\n",
            "Speed: 3.4ms preprocess, 174.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f98.jpg: 384x640 3 persons, 161.7ms\n",
            "Speed: 3.3ms preprocess, 161.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f17.jpg: 384x640 4 persons, 165.9ms\n",
            "Speed: 3.0ms preprocess, 165.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f104.jpg: 384x640 3 persons, 176.5ms\n",
            "Speed: 3.6ms preprocess, 176.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f100.jpg: 384x640 4 persons, 209.5ms\n",
            "Speed: 3.7ms preprocess, 209.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f4.jpg: 384x640 4 persons, 192.3ms\n",
            "Speed: 3.4ms preprocess, 192.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f96.jpg: 384x640 2 persons, 202.8ms\n",
            "Speed: 3.4ms preprocess, 202.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f87.jpg: 384x640 2 persons, 202.2ms\n",
            "Speed: 3.4ms preprocess, 202.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v52f90.jpg: 384x640 2 persons, 204.8ms\n",
            "Speed: 3.4ms preprocess, 204.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f40.jpg: 384x640 4 persons, 202.9ms\n",
            "Speed: 3.9ms preprocess, 202.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f67.jpg: 384x640 4 persons, 215.3ms\n",
            "Speed: 4.2ms preprocess, 215.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f51.jpg: 384x640 4 persons, 278.0ms\n",
            "Speed: 3.9ms preprocess, 278.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f47.jpg: 384x640 4 persons, 825.3ms\n",
            "Speed: 9.2ms preprocess, 825.3ms inference, 10.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f36.jpg: 384x640 4 persons, 489.3ms\n",
            "Speed: 14.9ms preprocess, 489.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f104.jpg: 384x640 3 persons, 553.9ms\n",
            "Speed: 9.0ms preprocess, 553.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f93.jpg: 384x640 3 persons, 254.7ms\n",
            "Speed: 7.1ms preprocess, 254.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f114.jpg: 384x640 3 persons, 257.8ms\n",
            "Speed: 4.5ms preprocess, 257.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f92.jpg: 384x640 3 persons, 169.8ms\n",
            "Speed: 4.3ms preprocess, 169.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f137.jpg: 384x640 4 persons, 209.3ms\n",
            "Speed: 3.7ms preprocess, 209.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f147.jpg: 384x640 6 persons, 162.3ms\n",
            "Speed: 3.0ms preprocess, 162.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f136.jpg: 384x640 4 persons, 161.8ms\n",
            "Speed: 3.1ms preprocess, 161.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f217.jpg: 384x640 3 persons, 151.5ms\n",
            "Speed: 3.0ms preprocess, 151.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f189.jpg: 384x640 3 persons, 173.5ms\n",
            "Speed: 3.0ms preprocess, 173.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f206.jpg: 384x640 3 persons, 179.7ms\n",
            "Speed: 4.3ms preprocess, 179.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f201.jpg: 384x640 2 persons, 160.0ms\n",
            "Speed: 3.2ms preprocess, 160.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f248.jpg: 384x640 3 persons, 159.5ms\n",
            "Speed: 3.6ms preprocess, 159.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f241.jpg: 384x640 2 persons, 160.4ms\n",
            "Speed: 3.0ms preprocess, 160.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f265.jpg: 384x640 3 persons, 160.5ms\n",
            "Speed: 3.5ms preprocess, 160.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f275.jpg: 384x640 4 persons, 154.1ms\n",
            "Speed: 2.9ms preprocess, 154.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f303.jpg: 384x640 3 persons, 157.4ms\n",
            "Speed: 3.8ms preprocess, 157.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f297.jpg: 384x640 3 persons, 186.0ms\n",
            "Speed: 4.1ms preprocess, 186.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f348.jpg: 384x640 4 persons, 259.9ms\n",
            "Speed: 4.8ms preprocess, 259.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f16.jpg: 384x640 4 persons, 263.1ms\n",
            "Speed: 4.7ms preprocess, 263.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f333.jpg: 384x640 5 persons, 246.2ms\n",
            "Speed: 5.3ms preprocess, 246.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f10.jpg: 384x640 4 persons, 254.0ms\n",
            "Speed: 4.5ms preprocess, 254.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f317.jpg: 384x640 3 persons, 256.5ms\n",
            "Speed: 5.2ms preprocess, 256.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f319.jpg: 384x640 3 persons, 171.6ms\n",
            "Speed: 4.3ms preprocess, 171.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v86f339.jpg: 384x640 4 persons, 159.8ms\n",
            "Speed: 3.0ms preprocess, 159.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f4.jpg: 384x640 4 persons, 163.3ms\n",
            "Speed: 3.0ms preprocess, 163.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f24.jpg: 384x640 5 persons, 176.3ms\n",
            "Speed: 3.1ms preprocess, 176.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f46.jpg: 384x640 3 persons, 159.2ms\n",
            "Speed: 3.3ms preprocess, 159.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f18.jpg: 384x640 4 persons, 178.2ms\n",
            "Speed: 3.0ms preprocess, 178.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f47.jpg: 384x640 3 persons, 162.7ms\n",
            "Speed: 3.0ms preprocess, 162.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f27.jpg: 384x640 4 persons, 158.3ms\n",
            "Speed: 3.0ms preprocess, 158.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f28.jpg: 384x640 4 persons, 165.1ms\n",
            "Speed: 3.4ms preprocess, 165.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f22.jpg: 384x640 4 persons, 169.8ms\n",
            "Speed: 3.1ms preprocess, 169.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f34.jpg: 384x640 4 persons, 162.4ms\n",
            "Speed: 3.4ms preprocess, 162.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f38.jpg: 384x640 4 persons, 159.8ms\n",
            "Speed: 3.0ms preprocess, 159.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f51.jpg: 384x640 3 persons, 166.7ms\n",
            "Speed: 4.3ms preprocess, 166.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f57.jpg: 384x640 4 persons, 160.3ms\n",
            "Speed: 3.0ms preprocess, 160.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f53.jpg: 384x640 3 persons, 163.3ms\n",
            "Speed: 3.2ms preprocess, 163.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f59.jpg: 384x640 4 persons, 249.2ms\n",
            "Speed: 4.4ms preprocess, 249.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f90.jpg: 384x640 4 persons, 270.4ms\n",
            "Speed: 4.4ms preprocess, 270.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f83.jpg: 384x640 4 persons, 249.9ms\n",
            "Speed: 4.9ms preprocess, 249.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f66.jpg: 384x640 4 persons, 233.0ms\n",
            "Speed: 4.6ms preprocess, 233.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f81.jpg: 384x640 4 persons, 157.6ms\n",
            "Speed: 3.3ms preprocess, 157.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f102.jpg: 384x640 4 persons, 186.7ms\n",
            "Speed: 3.4ms preprocess, 186.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f72.jpg: 384x640 4 persons, 185.9ms\n",
            "Speed: 3.2ms preprocess, 185.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f77.jpg: 384x640 4 persons, 165.6ms\n",
            "Speed: 3.4ms preprocess, 165.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f61.jpg: 384x640 4 persons, 155.9ms\n",
            "Speed: 3.0ms preprocess, 155.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f80.jpg: 384x640 4 persons, 160.0ms\n",
            "Speed: 3.1ms preprocess, 160.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f100.jpg: 384x640 4 persons, 169.0ms\n",
            "Speed: 3.0ms preprocess, 169.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f119.jpg: 384x640 4 persons, 183.3ms\n",
            "Speed: 3.1ms preprocess, 183.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f145.jpg: 384x640 4 persons, 157.5ms\n",
            "Speed: 3.1ms preprocess, 157.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f149.jpg: 384x640 4 persons, 155.5ms\n",
            "Speed: 3.0ms preprocess, 155.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f135.jpg: 384x640 5 persons, 162.6ms\n",
            "Speed: 3.1ms preprocess, 162.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f105.jpg: 384x640 4 persons, 165.8ms\n",
            "Speed: 3.2ms preprocess, 165.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f107.jpg: 384x640 4 persons, 166.8ms\n",
            "Speed: 3.8ms preprocess, 166.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f151.jpg: 384x640 4 persons, 161.1ms\n",
            "Speed: 3.1ms preprocess, 161.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f2.jpg: 384x640 2 persons, 173.2ms\n",
            "Speed: 4.3ms preprocess, 173.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f7.jpg: 384x640 3 persons, 249.4ms\n",
            "Speed: 4.6ms preprocess, 249.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f21.jpg: 384x640 2 persons, 234.8ms\n",
            "Speed: 4.5ms preprocess, 234.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f23.jpg: 384x640 2 persons, 251.5ms\n",
            "Speed: 5.8ms preprocess, 251.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f162.jpg: 384x640 4 persons, 244.8ms\n",
            "Speed: 4.8ms preprocess, 244.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v47f160.jpg: 384x640 4 persons, 160.7ms\n",
            "Speed: 3.1ms preprocess, 160.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f22.jpg: 384x640 2 persons, 189.2ms\n",
            "Speed: 3.0ms preprocess, 189.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f32.jpg: 384x640 2 persons, 182.3ms\n",
            "Speed: 3.0ms preprocess, 182.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f75.jpg: 384x640 2 persons, 163.1ms\n",
            "Speed: 3.0ms preprocess, 163.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f76.jpg: 384x640 2 persons, 167.8ms\n",
            "Speed: 3.0ms preprocess, 167.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f121.jpg: 384x640 3 persons, 157.1ms\n",
            "Speed: 3.3ms preprocess, 157.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f78.jpg: 384x640 2 persons, 163.7ms\n",
            "Speed: 3.3ms preprocess, 163.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f111.jpg: 384x640 3 persons, 157.1ms\n",
            "Speed: 3.0ms preprocess, 157.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f120.jpg: 384x640 3 persons, 183.0ms\n",
            "Speed: 3.2ms preprocess, 183.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f112.jpg: 384x640 3 persons, 158.6ms\n",
            "Speed: 3.5ms preprocess, 158.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f109.jpg: 384x640 3 persons, 165.7ms\n",
            "Speed: 3.4ms preprocess, 165.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f99.jpg: 384x640 2 persons, 162.2ms\n",
            "Speed: 3.1ms preprocess, 162.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f113.jpg: 384x640 3 persons, 164.5ms\n",
            "Speed: 3.1ms preprocess, 164.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f166.jpg: 384x640 2 persons, 160.8ms\n",
            "Speed: 3.0ms preprocess, 160.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f130.jpg: 384x640 3 persons, 161.4ms\n",
            "Speed: 3.0ms preprocess, 161.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f146.jpg: 384x640 4 persons, 166.6ms\n",
            "Speed: 3.0ms preprocess, 166.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f148.jpg: 384x640 3 persons, 162.9ms\n",
            "Speed: 3.9ms preprocess, 162.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f178.jpg: 384x640 3 persons, 166.3ms\n",
            "Speed: 3.1ms preprocess, 166.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v7f0.jpg: 384x640 3 persons, 262.7ms\n",
            "Speed: 4.7ms preprocess, 262.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f174.jpg: 384x640 3 persons, 261.4ms\n",
            "Speed: 4.7ms preprocess, 261.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v18f133.jpg: 384x640 3 persons, 249.7ms\n",
            "Speed: 4.9ms preprocess, 249.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v7f22.jpg: 384x640 2 persons, 195.3ms\n",
            "Speed: 4.3ms preprocess, 195.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v7f71.jpg: 384x640 3 persons, 180.7ms\n",
            "Speed: 3.0ms preprocess, 180.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v7f63.jpg: 384x640 2 persons, 166.5ms\n",
            "Speed: 3.0ms preprocess, 166.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v7f88.jpg: 384x640 4 persons, 179.8ms\n",
            "Speed: 3.4ms preprocess, 179.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v7f56.jpg: 384x640 2 persons, 191.6ms\n",
            "Speed: 3.0ms preprocess, 191.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f3.jpg: 384x640 4 persons, 154.8ms\n",
            "Speed: 3.0ms preprocess, 154.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v7f113.jpg: 384x640 3 persons, 157.8ms\n",
            "Speed: 3.0ms preprocess, 157.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f10.jpg: 384x640 4 persons, 159.7ms\n",
            "Speed: 3.1ms preprocess, 159.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f4.jpg: 384x640 5 persons, 161.0ms\n",
            "Speed: 3.2ms preprocess, 161.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v7f124.jpg: 384x640 3 persons, 177.7ms\n",
            "Speed: 3.1ms preprocess, 177.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f9.jpg: 384x640 4 persons, 183.8ms\n",
            "Speed: 3.1ms preprocess, 183.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f59.jpg: 384x640 4 persons, 161.0ms\n",
            "Speed: 3.0ms preprocess, 161.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f17.jpg: 384x640 4 persons, 180.2ms\n",
            "Speed: 3.3ms preprocess, 180.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f18.jpg: 384x640 4 persons, 155.9ms\n",
            "Speed: 3.0ms preprocess, 155.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f58.jpg: 384x640 4 persons, 159.5ms\n",
            "Speed: 3.4ms preprocess, 159.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f39.jpg: 384x640 4 persons, 178.6ms\n",
            "Speed: 3.1ms preprocess, 178.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f26.jpg: 384x640 4 persons, 235.8ms\n",
            "Speed: 3.2ms preprocess, 235.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f25.jpg: 384x640 4 persons, 273.7ms\n",
            "Speed: 7.4ms preprocess, 273.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f53.jpg: 384x640 4 persons, 262.3ms\n",
            "Speed: 4.7ms preprocess, 262.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f28.jpg: 384x640 4 persons, 290.6ms\n",
            "Speed: 4.8ms preprocess, 290.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f46.jpg: 384x640 4 persons, 172.6ms\n",
            "Speed: 3.0ms preprocess, 172.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v48f3.jpg: 384x640 4 persons, 181.8ms\n",
            "Speed: 3.1ms preprocess, 181.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v48f24.jpg: 384x640 4 persons, 173.2ms\n",
            "Speed: 3.1ms preprocess, 173.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f68.jpg: 384x640 4 persons, 171.0ms\n",
            "Speed: 4.4ms preprocess, 171.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f78.jpg: 384x640 4 persons, 158.7ms\n",
            "Speed: 3.2ms preprocess, 158.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/violent/v93f74.jpg: 384x640 4 persons, 157.4ms\n",
            "Speed: 3.1ms preprocess, 157.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f155.jpg: 384x640 1 person, 206.8ms\n",
            "Speed: 3.3ms preprocess, 206.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f161.jpg: 384x640 1 person, 174.0ms\n",
            "Speed: 3.1ms preprocess, 174.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f167.jpg: 384x640 2 persons, 187.0ms\n",
            "Speed: 3.2ms preprocess, 187.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f145.jpg: 384x640 2 persons, 166.6ms\n",
            "Speed: 3.2ms preprocess, 166.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f153.jpg: 384x640 1 person, 162.2ms\n",
            "Speed: 3.0ms preprocess, 162.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f165.jpg: 384x640 2 persons, 177.6ms\n",
            "Speed: 3.6ms preprocess, 177.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f151.jpg: 384x640 1 person, 161.5ms\n",
            "Speed: 3.3ms preprocess, 161.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f156.jpg: 384x640 1 person, 161.8ms\n",
            "Speed: 3.4ms preprocess, 161.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f13.jpg: 384x640 4 persons, 275.9ms\n",
            "Speed: 4.6ms preprocess, 275.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f26.jpg: 384x640 4 persons, 254.3ms\n",
            "Speed: 8.8ms preprocess, 254.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f27.jpg: 384x640 4 persons, 281.6ms\n",
            "Speed: 4.7ms preprocess, 281.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f32.jpg: 384x640 4 persons, 266.8ms\n",
            "Speed: 5.8ms preprocess, 266.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f19.jpg: 384x640 4 persons, 254.7ms\n",
            "Speed: 4.5ms preprocess, 254.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f39.jpg: 384x640 4 persons, 176.0ms\n",
            "Speed: 3.1ms preprocess, 176.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f28.jpg: 384x640 4 persons, 202.1ms\n",
            "Speed: 3.5ms preprocess, 202.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f182.jpg: 384x640 3 persons, 156.3ms\n",
            "Speed: 3.0ms preprocess, 156.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f17.jpg: 384x640 4 persons, 166.4ms\n",
            "Speed: 3.0ms preprocess, 166.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f190.jpg: 384x640 4 persons, 188.6ms\n",
            "Speed: 3.2ms preprocess, 188.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f186.jpg: 384x640 4 persons, 159.4ms\n",
            "Speed: 3.1ms preprocess, 159.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f1.jpg: 384x640 4 persons, 156.1ms\n",
            "Speed: 3.1ms preprocess, 156.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f191.jpg: 384x640 3 persons, 164.0ms\n",
            "Speed: 3.0ms preprocess, 164.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f183.jpg: 384x640 3 persons, 166.2ms\n",
            "Speed: 3.2ms preprocess, 166.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f31.jpg: 384x640 4 persons, 163.4ms\n",
            "Speed: 3.1ms preprocess, 163.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f12.jpg: 384x640 4 persons, 170.3ms\n",
            "Speed: 3.1ms preprocess, 170.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f72.jpg: 384x640 4 persons, 161.3ms\n",
            "Speed: 3.0ms preprocess, 161.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f81.jpg: 384x640 7 persons, 168.8ms\n",
            "Speed: 3.0ms preprocess, 168.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f68.jpg: 384x640 4 persons, 163.9ms\n",
            "Speed: 3.1ms preprocess, 163.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f48.jpg: 384x640 4 persons, 254.2ms\n",
            "Speed: 4.3ms preprocess, 254.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f42.jpg: 384x640 4 persons, 264.3ms\n",
            "Speed: 4.5ms preprocess, 264.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f77.jpg: 384x640 4 persons, 244.8ms\n",
            "Speed: 4.5ms preprocess, 244.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f54.jpg: 384x640 4 persons, 214.9ms\n",
            "Speed: 4.4ms preprocess, 214.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f91.jpg: 384x640 5 persons, 157.8ms\n",
            "Speed: 3.3ms preprocess, 157.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f89.jpg: 384x640 5 persons, 157.7ms\n",
            "Speed: 3.1ms preprocess, 157.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f86.jpg: 384x640 4 persons, 162.0ms\n",
            "Speed: 3.4ms preprocess, 162.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f67.jpg: 384x640 4 persons, 154.0ms\n",
            "Speed: 3.1ms preprocess, 154.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f78.jpg: 384x640 4 persons, 176.9ms\n",
            "Speed: 5.8ms preprocess, 176.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f65.jpg: 384x640 4 persons, 156.2ms\n",
            "Speed: 3.1ms preprocess, 156.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f125.jpg: 384x640 4 persons, 168.8ms\n",
            "Speed: 3.3ms preprocess, 168.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f100.jpg: 384x640 3 persons, 158.7ms\n",
            "Speed: 2.9ms preprocess, 158.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f116.jpg: 384x640 5 persons, 157.2ms\n",
            "Speed: 3.1ms preprocess, 157.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f120.jpg: 384x640 4 persons, 171.1ms\n",
            "Speed: 3.5ms preprocess, 171.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f111.jpg: 384x640 3 persons, 164.8ms\n",
            "Speed: 3.1ms preprocess, 164.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f138.jpg: 384x640 5 persons, 169.9ms\n",
            "Speed: 3.2ms preprocess, 169.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f110.jpg: 384x640 3 persons, 169.1ms\n",
            "Speed: 3.0ms preprocess, 169.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f140.jpg: 384x640 4 persons, 166.5ms\n",
            "Speed: 3.5ms preprocess, 166.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f115.jpg: 384x640 4 persons, 275.3ms\n",
            "Speed: 5.1ms preprocess, 275.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f126.jpg: 384x640 4 persons, 262.9ms\n",
            "Speed: 4.6ms preprocess, 262.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f134.jpg: 384x640 4 persons, 257.2ms\n",
            "Speed: 4.9ms preprocess, 257.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f92.jpg: 384x640 4 persons, 261.3ms\n",
            "Speed: 4.4ms preprocess, 261.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f99.jpg: 384x640 4 persons, 179.0ms\n",
            "Speed: 3.1ms preprocess, 179.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f183.jpg: 384x640 3 persons, 163.2ms\n",
            "Speed: 3.5ms preprocess, 163.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f154.jpg: 384x640 4 persons, 167.0ms\n",
            "Speed: 3.0ms preprocess, 167.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f157.jpg: 384x640 4 persons, 156.7ms\n",
            "Speed: 3.0ms preprocess, 156.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f151.jpg: 384x640 4 persons, 167.2ms\n",
            "Speed: 3.4ms preprocess, 167.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f150.jpg: 384x640 4 persons, 159.7ms\n",
            "Speed: 3.3ms preprocess, 159.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f166.jpg: 384x640 4 persons, 165.0ms\n",
            "Speed: 3.9ms preprocess, 165.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f174.jpg: 384x640 5 persons, 162.4ms\n",
            "Speed: 3.3ms preprocess, 162.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f155.jpg: 384x640 4 persons, 175.6ms\n",
            "Speed: 4.3ms preprocess, 175.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f158.jpg: 384x640 4 persons, 179.5ms\n",
            "Speed: 3.1ms preprocess, 179.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f172.jpg: 384x640 4 persons, 165.3ms\n",
            "Speed: 3.0ms preprocess, 165.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f152.jpg: 384x640 4 persons, 166.9ms\n",
            "Speed: 3.0ms preprocess, 166.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f189.jpg: 384x640 3 persons, 180.5ms\n",
            "Speed: 4.3ms preprocess, 180.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f161.jpg: 384x640 4 persons, 190.2ms\n",
            "Speed: 3.5ms preprocess, 190.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f178.jpg: 384x640 4 persons, 183.9ms\n",
            "Speed: 3.4ms preprocess, 183.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f164.jpg: 384x640 5 persons, 239.4ms\n",
            "Speed: 4.5ms preprocess, 239.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f187.jpg: 384x640 3 persons, 265.9ms\n",
            "Speed: 4.3ms preprocess, 265.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f167.jpg: 384x640 4 persons, 252.5ms\n",
            "Speed: 4.6ms preprocess, 252.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f156.jpg: 384x640 4 persons, 213.1ms\n",
            "Speed: 4.5ms preprocess, 213.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f175.jpg: 384x640 5 persons, 157.3ms\n",
            "Speed: 3.1ms preprocess, 157.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v18f168.jpg: 384x640 4 persons, 160.9ms\n",
            "Speed: 3.0ms preprocess, 160.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f30.jpg: 384x640 3 persons, 182.7ms\n",
            "Speed: 3.0ms preprocess, 182.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f46.jpg: 384x640 4 persons, 158.3ms\n",
            "Speed: 3.0ms preprocess, 158.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f29.jpg: 384x640 4 persons, 161.6ms\n",
            "Speed: 3.3ms preprocess, 161.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f23.jpg: 384x640 4 persons, 163.2ms\n",
            "Speed: 3.1ms preprocess, 163.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f27.jpg: 384x640 4 persons, 162.6ms\n",
            "Speed: 3.1ms preprocess, 162.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f22.jpg: 384x640 4 persons, 158.6ms\n",
            "Speed: 3.1ms preprocess, 158.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f28.jpg: 384x640 4 persons, 160.1ms\n",
            "Speed: 3.1ms preprocess, 160.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f7.jpg: 384x640 3 persons, 160.1ms\n",
            "Speed: 3.1ms preprocess, 160.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f20.jpg: 384x640 4 persons, 162.8ms\n",
            "Speed: 3.0ms preprocess, 162.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f18.jpg: 384x640 3 persons, 187.8ms\n",
            "Speed: 3.1ms preprocess, 187.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f15.jpg: 384x640 3 persons, 161.8ms\n",
            "Speed: 3.2ms preprocess, 161.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f41.jpg: 384x640 3 persons, 161.9ms\n",
            "Speed: 3.1ms preprocess, 161.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f11.jpg: 384x640 3 persons, 165.1ms\n",
            "Speed: 2.9ms preprocess, 165.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f31.jpg: 384x640 3 persons, 245.7ms\n",
            "Speed: 4.4ms preprocess, 245.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f32.jpg: 384x640 3 persons, 252.4ms\n",
            "Speed: 4.5ms preprocess, 252.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f34.jpg: 384x640 3 persons, 236.0ms\n",
            "Speed: 5.4ms preprocess, 236.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f49.jpg: 384x640 4 persons, 224.5ms\n",
            "Speed: 4.2ms preprocess, 224.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f13.jpg: 384x640 3 persons, 157.4ms\n",
            "Speed: 3.0ms preprocess, 157.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f25.jpg: 384x640 4 persons, 181.3ms\n",
            "Speed: 3.0ms preprocess, 181.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f106.jpg: 384x640 3 persons, 174.9ms\n",
            "Speed: 3.1ms preprocess, 174.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f79.jpg: 384x640 3 persons, 157.2ms\n",
            "Speed: 3.0ms preprocess, 157.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f95.jpg: 384x640 3 persons, 162.8ms\n",
            "Speed: 3.0ms preprocess, 162.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f94.jpg: 384x640 3 persons, 156.1ms\n",
            "Speed: 3.0ms preprocess, 156.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f58.jpg: 384x640 4 persons, 183.1ms\n",
            "Speed: 3.0ms preprocess, 183.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f87.jpg: 384x640 3 persons, 185.9ms\n",
            "Speed: 3.0ms preprocess, 185.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f75.jpg: 384x640 3 persons, 158.2ms\n",
            "Speed: 3.0ms preprocess, 158.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f51.jpg: 384x640 4 persons, 161.0ms\n",
            "Speed: 3.2ms preprocess, 161.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f74.jpg: 384x640 3 persons, 159.8ms\n",
            "Speed: 3.1ms preprocess, 159.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f71.jpg: 384x640 3 persons, 166.5ms\n",
            "Speed: 4.5ms preprocess, 166.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f68.jpg: 384x640 3 persons, 160.6ms\n",
            "Speed: 3.0ms preprocess, 160.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f85.jpg: 384x640 3 persons, 201.0ms\n",
            "Speed: 3.0ms preprocess, 201.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f78.jpg: 384x640 3 persons, 269.7ms\n",
            "Speed: 6.2ms preprocess, 269.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f90.jpg: 384x640 3 persons, 248.9ms\n",
            "Speed: 10.2ms preprocess, 248.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f50.jpg: 384x640 3 persons, 253.3ms\n",
            "Speed: 4.3ms preprocess, 253.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f89.jpg: 384x640 3 persons, 255.5ms\n",
            "Speed: 4.5ms preprocess, 255.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f82.jpg: 384x640 3 persons, 158.5ms\n",
            "Speed: 3.0ms preprocess, 158.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f67.jpg: 384x640 3 persons, 178.0ms\n",
            "Speed: 2.9ms preprocess, 178.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f101.jpg: 384x640 3 persons, 165.2ms\n",
            "Speed: 3.1ms preprocess, 165.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f113.jpg: 384x640 3 persons, 157.7ms\n",
            "Speed: 2.9ms preprocess, 157.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f8.jpg: 384x640 3 persons, 161.5ms\n",
            "Speed: 3.3ms preprocess, 161.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f23.jpg: 384x640 3 persons, 162.6ms\n",
            "Speed: 3.1ms preprocess, 162.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f108.jpg: 384x640 3 persons, 184.9ms\n",
            "Speed: 3.1ms preprocess, 184.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f15.jpg: 384x640 3 persons, 160.0ms\n",
            "Speed: 2.9ms preprocess, 160.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f33.jpg: 384x640 3 persons, 165.5ms\n",
            "Speed: 4.0ms preprocess, 165.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f29.jpg: 384x640 3 persons, 170.6ms\n",
            "Speed: 3.3ms preprocess, 170.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f17.jpg: 384x640 3 persons, 161.1ms\n",
            "Speed: 3.0ms preprocess, 161.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f12.jpg: 384x640 3 persons, 173.1ms\n",
            "Speed: 3.2ms preprocess, 173.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v48f112.jpg: 384x640 3 persons, 185.9ms\n",
            "Speed: 3.0ms preprocess, 185.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f1.jpg: 384x640 3 persons, 182.2ms\n",
            "Speed: 3.1ms preprocess, 182.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f19.jpg: 384x640 3 persons, 257.1ms\n",
            "Speed: 4.8ms preprocess, 257.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f0.jpg: 384x640 3 persons, 240.1ms\n",
            "Speed: 4.9ms preprocess, 240.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f10.jpg: 384x640 3 persons, 272.5ms\n",
            "Speed: 4.7ms preprocess, 272.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f38.jpg: 384x640 3 persons, 251.5ms\n",
            "Speed: 4.5ms preprocess, 251.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f4.jpg: 384x640 3 persons, 156.4ms\n",
            "Speed: 3.0ms preprocess, 156.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f58.jpg: 384x640 3 persons, 159.2ms\n",
            "Speed: 3.1ms preprocess, 159.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f82.jpg: 384x640 3 persons, 175.1ms\n",
            "Speed: 4.1ms preprocess, 175.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f43.jpg: 384x640 3 persons, 176.9ms\n",
            "Speed: 3.0ms preprocess, 176.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f70.jpg: 384x640 4 persons, 161.3ms\n",
            "Speed: 3.1ms preprocess, 161.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f41.jpg: 384x640 3 persons, 161.3ms\n",
            "Speed: 3.0ms preprocess, 161.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f68.jpg: 384x640 4 persons, 168.4ms\n",
            "Speed: 3.0ms preprocess, 168.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f75.jpg: 384x640 3 persons, 168.3ms\n",
            "Speed: 3.1ms preprocess, 168.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f62.jpg: 384x640 3 persons, 181.0ms\n",
            "Speed: 3.1ms preprocess, 181.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f47.jpg: 384x640 3 persons, 168.3ms\n",
            "Speed: 3.1ms preprocess, 168.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f42.jpg: 384x640 3 persons, 163.3ms\n",
            "Speed: 3.1ms preprocess, 163.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f61.jpg: 384x640 3 persons, 183.2ms\n",
            "Speed: 2.9ms preprocess, 183.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f80.jpg: 384x640 3 persons, 173.7ms\n",
            "Speed: 3.2ms preprocess, 173.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f78.jpg: 384x640 4 persons, 160.5ms\n",
            "Speed: 3.4ms preprocess, 160.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f81.jpg: 384x640 3 persons, 267.0ms\n",
            "Speed: 4.9ms preprocess, 267.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f79.jpg: 384x640 3 persons, 243.6ms\n",
            "Speed: 4.9ms preprocess, 243.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f60.jpg: 384x640 3 persons, 241.3ms\n",
            "Speed: 4.4ms preprocess, 241.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f49.jpg: 384x640 3 persons, 243.8ms\n",
            "Speed: 3.5ms preprocess, 243.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f89.jpg: 384x640 3 persons, 153.7ms\n",
            "Speed: 3.0ms preprocess, 153.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f53.jpg: 384x640 3 persons, 166.7ms\n",
            "Speed: 3.6ms preprocess, 166.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f50.jpg: 384x640 3 persons, 154.7ms\n",
            "Speed: 3.7ms preprocess, 154.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f52.jpg: 384x640 3 persons, 172.9ms\n",
            "Speed: 3.0ms preprocess, 172.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f54.jpg: 384x640 3 persons, 159.3ms\n",
            "Speed: 3.0ms preprocess, 159.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f107.jpg: 384x640 4 persons, 157.9ms\n",
            "Speed: 2.9ms preprocess, 157.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f109.jpg: 384x640 4 persons, 159.4ms\n",
            "Speed: 3.1ms preprocess, 159.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f113.jpg: 384x640 3 persons, 157.2ms\n",
            "Speed: 3.0ms preprocess, 157.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f99.jpg: 384x640 4 persons, 158.1ms\n",
            "Speed: 2.9ms preprocess, 158.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f8.jpg: 384x640 4 persons, 161.1ms\n",
            "Speed: 3.0ms preprocess, 161.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f12.jpg: 384x640 4 persons, 163.3ms\n",
            "Speed: 3.4ms preprocess, 163.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f106.jpg: 384x640 4 persons, 160.5ms\n",
            "Speed: 4.4ms preprocess, 160.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f122.jpg: 384x640 3 persons, 162.4ms\n",
            "Speed: 3.0ms preprocess, 162.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f1.jpg: 384x640 4 persons, 175.9ms\n",
            "Speed: 2.9ms preprocess, 175.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f5.jpg: 384x640 4 persons, 163.7ms\n",
            "Speed: 3.0ms preprocess, 163.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f126.jpg: 384x640 3 persons, 258.9ms\n",
            "Speed: 3.2ms preprocess, 258.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f129.jpg: 384x640 3 persons, 249.6ms\n",
            "Speed: 4.4ms preprocess, 249.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f119.jpg: 384x640 3 persons, 281.6ms\n",
            "Speed: 4.7ms preprocess, 281.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f108.jpg: 384x640 4 persons, 251.8ms\n",
            "Speed: 5.7ms preprocess, 251.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f110.jpg: 384x640 4 persons, 161.5ms\n",
            "Speed: 3.1ms preprocess, 161.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f10.jpg: 384x640 4 persons, 159.1ms\n",
            "Speed: 3.5ms preprocess, 159.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f121.jpg: 384x640 3 persons, 161.0ms\n",
            "Speed: 3.1ms preprocess, 161.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v55f104.jpg: 384x640 4 persons, 159.7ms\n",
            "Speed: 3.2ms preprocess, 159.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f16.jpg: 384x640 4 persons, 158.1ms\n",
            "Speed: 3.0ms preprocess, 158.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f54.jpg: 384x640 4 persons, 162.1ms\n",
            "Speed: 3.4ms preprocess, 162.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f61.jpg: 384x640 4 persons, 162.9ms\n",
            "Speed: 3.1ms preprocess, 162.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f24.jpg: 384x640 4 persons, 170.0ms\n",
            "Speed: 3.1ms preprocess, 170.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f38.jpg: 384x640 4 persons, 164.4ms\n",
            "Speed: 3.0ms preprocess, 164.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f45.jpg: 384x640 4 persons, 164.5ms\n",
            "Speed: 3.0ms preprocess, 164.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f51.jpg: 384x640 4 persons, 164.5ms\n",
            "Speed: 3.1ms preprocess, 164.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f37.jpg: 384x640 4 persons, 169.5ms\n",
            "Speed: 3.1ms preprocess, 169.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f22.jpg: 384x640 5 persons, 164.0ms\n",
            "Speed: 3.1ms preprocess, 164.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f53.jpg: 384x640 4 persons, 160.0ms\n",
            "Speed: 3.1ms preprocess, 160.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f59.jpg: 384x640 5 persons, 173.8ms\n",
            "Speed: 3.1ms preprocess, 173.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f20.jpg: 384x640 5 persons, 239.4ms\n",
            "Speed: 3.2ms preprocess, 239.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f18.jpg: 384x640 4 persons, 246.3ms\n",
            "Speed: 9.1ms preprocess, 246.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f41.jpg: 384x640 4 persons, 254.9ms\n",
            "Speed: 4.7ms preprocess, 254.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f33.jpg: 384x640 4 persons, 274.9ms\n",
            "Speed: 3.3ms preprocess, 274.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f60.jpg: 384x640 5 persons, 178.3ms\n",
            "Speed: 3.0ms preprocess, 178.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f83.jpg: 384x640 4 persons, 158.4ms\n",
            "Speed: 3.1ms preprocess, 158.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f84.jpg: 384x640 4 persons, 175.4ms\n",
            "Speed: 4.6ms preprocess, 175.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f85.jpg: 384x640 4 persons, 169.2ms\n",
            "Speed: 3.1ms preprocess, 169.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f96.jpg: 384x640 4 persons, 159.3ms\n",
            "Speed: 3.1ms preprocess, 159.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f66.jpg: 384x640 4 persons, 166.6ms\n",
            "Speed: 3.2ms preprocess, 166.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f95.jpg: 384x640 4 persons, 181.9ms\n",
            "Speed: 3.3ms preprocess, 181.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f87.jpg: 384x640 5 persons, 182.0ms\n",
            "Speed: 3.3ms preprocess, 182.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f103.jpg: 384x640 4 persons, 180.1ms\n",
            "Speed: 3.0ms preprocess, 180.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f63.jpg: 384x640 4 persons, 165.0ms\n",
            "Speed: 3.1ms preprocess, 165.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f99.jpg: 384x640 4 persons, 159.6ms\n",
            "Speed: 3.2ms preprocess, 159.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f76.jpg: 384x640 4 persons, 155.3ms\n",
            "Speed: 3.1ms preprocess, 155.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f70.jpg: 384x640 4 persons, 169.1ms\n",
            "Speed: 3.0ms preprocess, 169.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f91.jpg: 384x640 4 persons, 158.6ms\n",
            "Speed: 2.9ms preprocess, 158.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f69.jpg: 384x640 4 persons, 240.0ms\n",
            "Speed: 4.5ms preprocess, 240.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f73.jpg: 384x640 4 persons, 241.4ms\n",
            "Speed: 4.5ms preprocess, 241.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f101.jpg: 384x640 4 persons, 244.2ms\n",
            "Speed: 4.5ms preprocess, 244.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f77.jpg: 384x640 4 persons, 249.2ms\n",
            "Speed: 4.5ms preprocess, 249.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f88.jpg: 384x640 5 persons, 173.1ms\n",
            "Speed: 3.0ms preprocess, 173.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f148.jpg: 384x640 4 persons, 165.8ms\n",
            "Speed: 3.2ms preprocess, 165.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f126.jpg: 384x640 4 persons, 172.0ms\n",
            "Speed: 3.3ms preprocess, 172.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f110.jpg: 384x640 4 persons, 161.9ms\n",
            "Speed: 3.3ms preprocess, 161.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f149.jpg: 384x640 3 persons, 160.4ms\n",
            "Speed: 3.0ms preprocess, 160.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f104.jpg: 384x640 4 persons, 167.1ms\n",
            "Speed: 3.1ms preprocess, 167.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f107.jpg: 384x640 4 persons, 181.7ms\n",
            "Speed: 3.4ms preprocess, 181.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f132.jpg: 384x640 4 persons, 158.8ms\n",
            "Speed: 3.1ms preprocess, 158.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f112.jpg: 384x640 5 persons, 164.8ms\n",
            "Speed: 3.5ms preprocess, 164.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f128.jpg: 384x640 4 persons, 156.1ms\n",
            "Speed: 3.6ms preprocess, 156.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f138.jpg: 384x640 6 persons, 181.6ms\n",
            "Speed: 3.2ms preprocess, 181.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f113.jpg: 384x640 5 persons, 163.5ms\n",
            "Speed: 3.0ms preprocess, 163.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f144.jpg: 384x640 4 persons, 159.8ms\n",
            "Speed: 3.0ms preprocess, 159.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f139.jpg: 384x640 3 persons, 175.4ms\n",
            "Speed: 3.0ms preprocess, 175.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f130.jpg: 384x640 4 persons, 260.9ms\n",
            "Speed: 4.6ms preprocess, 260.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f116.jpg: 384x640 5 persons, 257.4ms\n",
            "Speed: 4.5ms preprocess, 257.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f106.jpg: 384x640 4 persons, 264.5ms\n",
            "Speed: 4.7ms preprocess, 264.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f142.jpg: 384x640 3 persons, 256.7ms\n",
            "Speed: 7.8ms preprocess, 256.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f122.jpg: 384x640 4 persons, 167.0ms\n",
            "Speed: 3.0ms preprocess, 167.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f114.jpg: 384x640 5 persons, 169.1ms\n",
            "Speed: 3.7ms preprocess, 169.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f176.jpg: 384x640 4 persons, 184.3ms\n",
            "Speed: 3.2ms preprocess, 184.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f181.jpg: 384x640 4 persons, 163.3ms\n",
            "Speed: 3.1ms preprocess, 163.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f166.jpg: 384x640 5 persons, 165.6ms\n",
            "Speed: 3.1ms preprocess, 165.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f178.jpg: 384x640 4 persons, 163.5ms\n",
            "Speed: 2.9ms preprocess, 163.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f154.jpg: 384x640 3 persons, 186.2ms\n",
            "Speed: 3.0ms preprocess, 186.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f160.jpg: 384x640 4 persons, 159.3ms\n",
            "Speed: 3.1ms preprocess, 159.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f195.jpg: 384x640 3 persons, 185.6ms\n",
            "Speed: 3.2ms preprocess, 185.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f157.jpg: 384x640 4 persons, 156.7ms\n",
            "Speed: 3.3ms preprocess, 156.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f177.jpg: 384x640 4 persons, 163.8ms\n",
            "Speed: 3.0ms preprocess, 163.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f203.jpg: 384x640 4 persons, 172.3ms\n",
            "Speed: 4.3ms preprocess, 172.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f204.jpg: 384x640 4 persons, 159.3ms\n",
            "Speed: 3.0ms preprocess, 159.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f205.jpg: 384x640 5 persons, 162.2ms\n",
            "Speed: 3.5ms preprocess, 162.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f196.jpg: 384x640 3 persons, 243.6ms\n",
            "Speed: 4.5ms preprocess, 243.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f202.jpg: 384x640 5 persons, 245.8ms\n",
            "Speed: 4.4ms preprocess, 245.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f164.jpg: 384x640 5 persons, 243.3ms\n",
            "Speed: 4.4ms preprocess, 243.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f213.jpg: 384x640 4 persons, 251.7ms\n",
            "Speed: 4.4ms preprocess, 251.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f210.jpg: 384x640 4 persons, 156.5ms\n",
            "Speed: 3.4ms preprocess, 156.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f246.jpg: 384x640 5 persons, 163.1ms\n",
            "Speed: 3.0ms preprocess, 163.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f215.jpg: 384x640 5 persons, 156.5ms\n",
            "Speed: 3.6ms preprocess, 156.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f249.jpg: 384x640 4 persons, 156.1ms\n",
            "Speed: 3.0ms preprocess, 156.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f225.jpg: 384x640 4 persons, 158.6ms\n",
            "Speed: 3.1ms preprocess, 158.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f207.jpg: 384x640 4 persons, 174.2ms\n",
            "Speed: 2.9ms preprocess, 174.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f237.jpg: 384x640 4 persons, 160.4ms\n",
            "Speed: 3.0ms preprocess, 160.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f227.jpg: 384x640 4 persons, 158.3ms\n",
            "Speed: 3.0ms preprocess, 158.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f254.jpg: 384x640 4 persons, 159.8ms\n",
            "Speed: 3.1ms preprocess, 159.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f218.jpg: 384x640 4 persons, 188.0ms\n",
            "Speed: 2.9ms preprocess, 188.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f233.jpg: 384x640 4 persons, 154.7ms\n",
            "Speed: 3.0ms preprocess, 154.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f247.jpg: 384x640 4 persons, 158.9ms\n",
            "Speed: 3.0ms preprocess, 158.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f226.jpg: 384x640 4 persons, 161.2ms\n",
            "Speed: 3.2ms preprocess, 161.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f229.jpg: 384x640 4 persons, 160.4ms\n",
            "Speed: 2.9ms preprocess, 160.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f243.jpg: 384x640 4 persons, 247.3ms\n",
            "Speed: 4.5ms preprocess, 247.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f0.jpg: 384x640 4 persons, 270.5ms\n",
            "Speed: 4.4ms preprocess, 270.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f42.jpg: 384x640 4 persons, 263.9ms\n",
            "Speed: 4.9ms preprocess, 263.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v13f256.jpg: 384x640 4 persons, 256.2ms\n",
            "Speed: 4.3ms preprocess, 256.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f27.jpg: 384x640 4 persons, 199.4ms\n",
            "Speed: 3.1ms preprocess, 199.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f29.jpg: 384x640 4 persons, 157.2ms\n",
            "Speed: 3.1ms preprocess, 157.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f14.jpg: 384x640 4 persons, 156.6ms\n",
            "Speed: 3.1ms preprocess, 156.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f2.jpg: 384x640 4 persons, 158.1ms\n",
            "Speed: 4.2ms preprocess, 158.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f20.jpg: 384x640 4 persons, 160.3ms\n",
            "Speed: 3.1ms preprocess, 160.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f12.jpg: 384x640 4 persons, 162.2ms\n",
            "Speed: 3.2ms preprocess, 162.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f30.jpg: 384x640 4 persons, 156.8ms\n",
            "Speed: 3.0ms preprocess, 156.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f3.jpg: 384x640 4 persons, 166.0ms\n",
            "Speed: 3.1ms preprocess, 166.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f10.jpg: 384x640 4 persons, 161.3ms\n",
            "Speed: 3.2ms preprocess, 161.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f72.jpg: 384x640 4 persons, 178.8ms\n",
            "Speed: 3.0ms preprocess, 178.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f87.jpg: 384x640 4 persons, 162.6ms\n",
            "Speed: 3.1ms preprocess, 162.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f78.jpg: 384x640 4 persons, 160.0ms\n",
            "Speed: 3.2ms preprocess, 160.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f60.jpg: 384x640 4 persons, 163.7ms\n",
            "Speed: 3.0ms preprocess, 163.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f86.jpg: 384x640 4 persons, 161.4ms\n",
            "Speed: 3.1ms preprocess, 161.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f82.jpg: 384x640 4 persons, 170.3ms\n",
            "Speed: 3.6ms preprocess, 170.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f59.jpg: 384x640 4 persons, 159.0ms\n",
            "Speed: 3.1ms preprocess, 159.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f58.jpg: 384x640 4 persons, 249.6ms\n",
            "Speed: 4.7ms preprocess, 249.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f73.jpg: 384x640 4 persons, 256.0ms\n",
            "Speed: 4.6ms preprocess, 256.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f52.jpg: 384x640 4 persons, 241.3ms\n",
            "Speed: 4.5ms preprocess, 241.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f44.jpg: 384x640 4 persons, 157.6ms\n",
            "Speed: 3.1ms preprocess, 157.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f83.jpg: 384x640 4 persons, 167.0ms\n",
            "Speed: 3.1ms preprocess, 167.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f67.jpg: 384x640 4 persons, 159.3ms\n",
            "Speed: 3.0ms preprocess, 159.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f65.jpg: 384x640 4 persons, 169.5ms\n",
            "Speed: 3.1ms preprocess, 169.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f56.jpg: 384x640 4 persons, 198.5ms\n",
            "Speed: 3.2ms preprocess, 198.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f70.jpg: 384x640 4 persons, 167.6ms\n",
            "Speed: 3.1ms preprocess, 167.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f80.jpg: 384x640 4 persons, 162.6ms\n",
            "Speed: 3.2ms preprocess, 162.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f53.jpg: 384x640 4 persons, 167.0ms\n",
            "Speed: 3.3ms preprocess, 167.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f50.jpg: 384x640 4 persons, 160.6ms\n",
            "Speed: 3.0ms preprocess, 160.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f75.jpg: 384x640 4 persons, 174.4ms\n",
            "Speed: 4.4ms preprocess, 174.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f48.jpg: 384x640 4 persons, 164.6ms\n",
            "Speed: 3.0ms preprocess, 164.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f63.jpg: 384x640 4 persons, 181.5ms\n",
            "Speed: 3.0ms preprocess, 181.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f100.jpg: 384x640 4 persons, 161.0ms\n",
            "Speed: 3.1ms preprocess, 161.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f125.jpg: 384x640 4 persons, 172.1ms\n",
            "Speed: 3.0ms preprocess, 172.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f131.jpg: 384x640 4 persons, 162.4ms\n",
            "Speed: 3.1ms preprocess, 162.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f111.jpg: 384x640 4 persons, 175.7ms\n",
            "Speed: 3.0ms preprocess, 175.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f124.jpg: 384x640 4 persons, 273.8ms\n",
            "Speed: 4.5ms preprocess, 273.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f103.jpg: 384x640 4 persons, 256.9ms\n",
            "Speed: 4.4ms preprocess, 256.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f89.jpg: 384x640 4 persons, 246.6ms\n",
            "Speed: 4.4ms preprocess, 246.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f104.jpg: 384x640 4 persons, 264.9ms\n",
            "Speed: 4.4ms preprocess, 264.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f90.jpg: 384x640 4 persons, 164.4ms\n",
            "Speed: 3.1ms preprocess, 164.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f98.jpg: 384x640 4 persons, 160.6ms\n",
            "Speed: 3.3ms preprocess, 160.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f107.jpg: 384x640 4 persons, 162.1ms\n",
            "Speed: 3.1ms preprocess, 162.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f118.jpg: 384x640 4 persons, 171.9ms\n",
            "Speed: 3.1ms preprocess, 171.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f139.jpg: 384x640 4 persons, 204.6ms\n",
            "Speed: 3.7ms preprocess, 204.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f115.jpg: 384x640 4 persons, 167.8ms\n",
            "Speed: 3.2ms preprocess, 167.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f134.jpg: 384x640 4 persons, 158.4ms\n",
            "Speed: 3.0ms preprocess, 158.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f109.jpg: 384x640 4 persons, 159.3ms\n",
            "Speed: 3.1ms preprocess, 159.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f128.jpg: 384x640 4 persons, 168.4ms\n",
            "Speed: 3.4ms preprocess, 168.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f120.jpg: 384x640 4 persons, 160.0ms\n",
            "Speed: 3.2ms preprocess, 160.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f112.jpg: 384x640 4 persons, 175.1ms\n",
            "Speed: 4.4ms preprocess, 175.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f151.jpg: 384x640 4 persons, 163.1ms\n",
            "Speed: 3.0ms preprocess, 163.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f5.jpg: 384x640 5 persons, 154.1ms\n",
            "Speed: 3.1ms preprocess, 154.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f29.jpg: 384x640 4 persons, 164.3ms\n",
            "Speed: 3.1ms preprocess, 164.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f13.jpg: 384x640 4 persons, 163.2ms\n",
            "Speed: 3.4ms preprocess, 163.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f12.jpg: 384x640 4 persons, 167.7ms\n",
            "Speed: 3.2ms preprocess, 167.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f26.jpg: 384x640 4 persons, 205.5ms\n",
            "Speed: 3.2ms preprocess, 205.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f40.jpg: 384x640 4 persons, 245.4ms\n",
            "Speed: 4.5ms preprocess, 245.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f28.jpg: 384x640 4 persons, 242.6ms\n",
            "Speed: 4.5ms preprocess, 242.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f42.jpg: 384x640 4 persons, 249.9ms\n",
            "Speed: 4.7ms preprocess, 249.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f23.jpg: 384x640 4 persons, 161.7ms\n",
            "Speed: 3.3ms preprocess, 161.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f39.jpg: 384x640 4 persons, 188.7ms\n",
            "Speed: 3.1ms preprocess, 188.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f21.jpg: 384x640 4 persons, 174.7ms\n",
            "Speed: 3.0ms preprocess, 174.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f19.jpg: 384x640 4 persons, 185.2ms\n",
            "Speed: 3.0ms preprocess, 185.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f149.jpg: 384x640 4 persons, 158.8ms\n",
            "Speed: 3.3ms preprocess, 158.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f4.jpg: 384x640 4 persons, 180.4ms\n",
            "Speed: 3.1ms preprocess, 180.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f14.jpg: 384x640 4 persons, 160.5ms\n",
            "Speed: 3.1ms preprocess, 160.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v21f143.jpg: 384x640 4 persons, 166.4ms\n",
            "Speed: 3.4ms preprocess, 166.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f27.jpg: 384x640 4 persons, 199.7ms\n",
            "Speed: 4.5ms preprocess, 199.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f84.jpg: 384x640 4 persons, 166.0ms\n",
            "Speed: 3.1ms preprocess, 166.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f53.jpg: 384x640 4 persons, 178.5ms\n",
            "Speed: 4.2ms preprocess, 178.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f62.jpg: 384x640 4 persons, 180.3ms\n",
            "Speed: 3.3ms preprocess, 180.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f87.jpg: 384x640 4 persons, 178.0ms\n",
            "Speed: 3.1ms preprocess, 178.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f69.jpg: 384x640 4 persons, 167.1ms\n",
            "Speed: 3.5ms preprocess, 167.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f64.jpg: 384x640 4 persons, 154.5ms\n",
            "Speed: 3.0ms preprocess, 154.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f67.jpg: 384x640 4 persons, 160.2ms\n",
            "Speed: 3.0ms preprocess, 160.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f83.jpg: 384x640 4 persons, 274.4ms\n",
            "Speed: 4.5ms preprocess, 274.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f76.jpg: 384x640 4 persons, 240.0ms\n",
            "Speed: 4.9ms preprocess, 240.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f81.jpg: 384x640 4 persons, 253.1ms\n",
            "Speed: 4.5ms preprocess, 253.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f58.jpg: 384x640 4 persons, 205.5ms\n",
            "Speed: 4.3ms preprocess, 205.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f61.jpg: 384x640 4 persons, 168.1ms\n",
            "Speed: 3.0ms preprocess, 168.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f88.jpg: 384x640 4 persons, 170.9ms\n",
            "Speed: 5.3ms preprocess, 170.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f99.jpg: 384x640 4 persons, 166.8ms\n",
            "Speed: 3.0ms preprocess, 166.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f98.jpg: 384x640 4 persons, 172.6ms\n",
            "Speed: 4.4ms preprocess, 172.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f65.jpg: 384x640 4 persons, 162.8ms\n",
            "Speed: 3.1ms preprocess, 162.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f49.jpg: 384x640 4 persons, 167.0ms\n",
            "Speed: 3.1ms preprocess, 167.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f80.jpg: 384x640 4 persons, 182.3ms\n",
            "Speed: 3.4ms preprocess, 182.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v20f66.jpg: 384x640 4 persons, 159.8ms\n",
            "Speed: 3.0ms preprocess, 159.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f14.jpg: 384x640 4 persons, 157.0ms\n",
            "Speed: 3.2ms preprocess, 157.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f1.jpg: 384x640 4 persons, 179.1ms\n",
            "Speed: 3.1ms preprocess, 179.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f29.jpg: 384x640 3 persons, 169.2ms\n",
            "Speed: 3.0ms preprocess, 169.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f18.jpg: 384x640 2 persons, 168.6ms\n",
            "Speed: 3.0ms preprocess, 168.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f33.jpg: 384x640 2 persons, 162.3ms\n",
            "Speed: 3.8ms preprocess, 162.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f41.jpg: 384x640 3 persons, 159.0ms\n",
            "Speed: 3.2ms preprocess, 159.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f24.jpg: 384x640 3 persons, 183.1ms\n",
            "Speed: 3.1ms preprocess, 183.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f46.jpg: 384x640 4 persons, 253.1ms\n",
            "Speed: 4.6ms preprocess, 253.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f15.jpg: 384x640 4 persons, 266.2ms\n",
            "Speed: 4.5ms preprocess, 266.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f26.jpg: 384x640 3 persons, 262.8ms\n",
            "Speed: 4.6ms preprocess, 262.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f3.jpg: 384x640 5 persons, 161.2ms\n",
            "Speed: 3.1ms preprocess, 161.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f19.jpg: 384x640 2 persons, 158.6ms\n",
            "Speed: 3.5ms preprocess, 158.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f37.jpg: 384x640 4 persons, 164.8ms\n",
            "Speed: 3.1ms preprocess, 164.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f21.jpg: 384x640 2 persons, 162.2ms\n",
            "Speed: 3.0ms preprocess, 162.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f11.jpg: 384x640 2 persons, 154.0ms\n",
            "Speed: 3.4ms preprocess, 154.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f42.jpg: 384x640 2 persons, 157.3ms\n",
            "Speed: 3.4ms preprocess, 157.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f16.jpg: 384x640 2 persons, 171.3ms\n",
            "Speed: 3.3ms preprocess, 171.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f17.jpg: 384x640 2 persons, 160.0ms\n",
            "Speed: 3.1ms preprocess, 160.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f2.jpg: 384x640 4 persons, 164.9ms\n",
            "Speed: 3.0ms preprocess, 164.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f106.jpg: 384x640 2 persons, 160.6ms\n",
            "Speed: 3.0ms preprocess, 160.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f82.jpg: 384x640 2 persons, 173.7ms\n",
            "Speed: 3.5ms preprocess, 173.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f93.jpg: 384x640 3 persons, 160.6ms\n",
            "Speed: 3.0ms preprocess, 160.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f49.jpg: 384x640 2 persons, 164.1ms\n",
            "Speed: 3.6ms preprocess, 164.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f110.jpg: 384x640 2 persons, 167.1ms\n",
            "Speed: 3.0ms preprocess, 167.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f54.jpg: 384x640 4 persons, 162.1ms\n",
            "Speed: 3.4ms preprocess, 162.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f89.jpg: 384x640 3 persons, 254.2ms\n",
            "Speed: 4.2ms preprocess, 254.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f86.jpg: 384x640 2 persons, 254.3ms\n",
            "Speed: 8.6ms preprocess, 254.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f51.jpg: 384x640 3 persons, 252.6ms\n",
            "Speed: 4.5ms preprocess, 252.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f85.jpg: 384x640 2 persons, 251.7ms\n",
            "Speed: 4.5ms preprocess, 251.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f114.jpg: 384x640 2 persons, 160.7ms\n",
            "Speed: 3.2ms preprocess, 160.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f99.jpg: 384x640 2 persons, 157.7ms\n",
            "Speed: 3.1ms preprocess, 157.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f64.jpg: 384x640 2 persons, 164.4ms\n",
            "Speed: 3.0ms preprocess, 164.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f112.jpg: 384x640 2 persons, 162.1ms\n",
            "Speed: 3.0ms preprocess, 162.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f77.jpg: 384x640 3 persons, 182.2ms\n",
            "Speed: 3.7ms preprocess, 182.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f55.jpg: 384x640 3 persons, 157.4ms\n",
            "Speed: 3.0ms preprocess, 157.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f117.jpg: 384x640 2 persons, 160.9ms\n",
            "Speed: 3.0ms preprocess, 160.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f154.jpg: 384x640 2 persons, 170.6ms\n",
            "Speed: 3.1ms preprocess, 170.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f166.jpg: 384x640 4 persons, 163.7ms\n",
            "Speed: 3.0ms preprocess, 163.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f123.jpg: 384x640 2 persons, 162.1ms\n",
            "Speed: 3.0ms preprocess, 162.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f119.jpg: 384x640 2 persons, 157.7ms\n",
            "Speed: 3.1ms preprocess, 157.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f147.jpg: 384x640 2 persons, 173.1ms\n",
            "Speed: 4.3ms preprocess, 173.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f118.jpg: 384x640 2 persons, 170.3ms\n",
            "Speed: 3.0ms preprocess, 170.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f139.jpg: 384x640 2 persons, 183.2ms\n",
            "Speed: 4.4ms preprocess, 183.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f155.jpg: 384x640 2 persons, 163.7ms\n",
            "Speed: 3.1ms preprocess, 163.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f149.jpg: 384x640 2 persons, 244.2ms\n",
            "Speed: 4.5ms preprocess, 244.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f134.jpg: 384x640 2 persons, 254.1ms\n",
            "Speed: 4.5ms preprocess, 254.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f127.jpg: 384x640 1 person, 292.8ms\n",
            "Speed: 9.2ms preprocess, 292.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f163.jpg: 384x640 2 persons, 257.4ms\n",
            "Speed: 4.5ms preprocess, 257.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f164.jpg: 384x640 2 persons, 158.7ms\n",
            "Speed: 3.1ms preprocess, 158.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f120.jpg: 384x640 2 persons, 175.4ms\n",
            "Speed: 3.2ms preprocess, 175.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f132.jpg: 384x640 2 persons, 158.4ms\n",
            "Speed: 3.4ms preprocess, 158.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f126.jpg: 384x640 2 persons, 155.8ms\n",
            "Speed: 3.2ms preprocess, 155.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f162.jpg: 384x640 3 persons, 157.8ms\n",
            "Speed: 3.7ms preprocess, 157.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f144.jpg: 384x640 2 persons, 164.0ms\n",
            "Speed: 3.2ms preprocess, 164.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f133.jpg: 384x640 2 persons, 166.9ms\n",
            "Speed: 3.1ms preprocess, 166.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f165.jpg: 384x640 2 persons, 154.6ms\n",
            "Speed: 3.0ms preprocess, 154.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f152.jpg: 384x640 2 persons, 160.8ms\n",
            "Speed: 3.1ms preprocess, 160.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f39.jpg: 384x640 4 persons, 156.1ms\n",
            "Speed: 3.0ms preprocess, 156.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f17.jpg: 384x640 4 persons, 154.5ms\n",
            "Speed: 3.1ms preprocess, 154.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f50.jpg: 384x640 4 persons, 165.3ms\n",
            "Speed: 3.1ms preprocess, 165.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f34.jpg: 384x640 4 persons, 155.8ms\n",
            "Speed: 3.0ms preprocess, 155.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v35f167.jpg: 384x640 4 persons, 262.2ms\n",
            "Speed: 4.1ms preprocess, 262.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f19.jpg: 384x640 4 persons, 270.8ms\n",
            "Speed: 6.0ms preprocess, 270.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f3.jpg: 384x640 5 persons, 255.5ms\n",
            "Speed: 6.7ms preprocess, 255.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f36.jpg: 384x640 4 persons, 256.7ms\n",
            "Speed: 11.7ms preprocess, 256.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f40.jpg: 384x640 4 persons, 268.6ms\n",
            "Speed: 4.0ms preprocess, 268.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f14.jpg: 384x640 4 persons, 174.9ms\n",
            "Speed: 3.1ms preprocess, 174.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f47.jpg: 384x640 4 persons, 163.6ms\n",
            "Speed: 3.1ms preprocess, 163.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f28.jpg: 384x640 4 persons, 156.3ms\n",
            "Speed: 3.0ms preprocess, 156.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f1.jpg: 384x640 5 persons, 182.1ms\n",
            "Speed: 3.1ms preprocess, 182.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f21.jpg: 384x640 4 persons, 161.8ms\n",
            "Speed: 3.3ms preprocess, 161.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f11.jpg: 384x640 5 persons, 164.2ms\n",
            "Speed: 2.9ms preprocess, 164.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f20.jpg: 384x640 4 persons, 157.3ms\n",
            "Speed: 2.9ms preprocess, 157.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f92.jpg: 384x640 4 persons, 154.4ms\n",
            "Speed: 3.0ms preprocess, 154.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f73.jpg: 384x640 4 persons, 162.4ms\n",
            "Speed: 3.0ms preprocess, 162.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f87.jpg: 384x640 4 persons, 159.3ms\n",
            "Speed: 3.0ms preprocess, 159.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f90.jpg: 384x640 4 persons, 159.4ms\n",
            "Speed: 3.0ms preprocess, 159.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f64.jpg: 384x640 5 persons, 164.9ms\n",
            "Speed: 3.0ms preprocess, 164.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f88.jpg: 384x640 4 persons, 258.6ms\n",
            "Speed: 3.8ms preprocess, 258.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f68.jpg: 384x640 4 persons, 242.6ms\n",
            "Speed: 4.5ms preprocess, 242.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f82.jpg: 384x640 4 persons, 243.5ms\n",
            "Speed: 4.4ms preprocess, 243.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f62.jpg: 384x640 4 persons, 209.6ms\n",
            "Speed: 4.4ms preprocess, 209.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f79.jpg: 384x640 4 persons, 163.1ms\n",
            "Speed: 3.2ms preprocess, 163.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f94.jpg: 384x640 3 persons, 169.3ms\n",
            "Speed: 3.0ms preprocess, 169.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f95.jpg: 384x640 3 persons, 155.5ms\n",
            "Speed: 3.6ms preprocess, 155.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f70.jpg: 384x640 4 persons, 162.9ms\n",
            "Speed: 3.1ms preprocess, 162.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f66.jpg: 384x640 4 persons, 161.0ms\n",
            "Speed: 3.4ms preprocess, 161.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f93.jpg: 384x640 4 persons, 158.1ms\n",
            "Speed: 3.1ms preprocess, 158.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f86.jpg: 384x640 4 persons, 161.3ms\n",
            "Speed: 3.2ms preprocess, 161.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f100.jpg: 384x640 3 persons, 157.1ms\n",
            "Speed: 3.2ms preprocess, 157.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f85.jpg: 384x640 4 persons, 155.7ms\n",
            "Speed: 3.2ms preprocess, 155.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f76.jpg: 384x640 4 persons, 155.3ms\n",
            "Speed: 3.0ms preprocess, 155.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f120.jpg: 384x640 4 persons, 156.9ms\n",
            "Speed: 3.0ms preprocess, 156.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f134.jpg: 384x640 4 persons, 152.6ms\n",
            "Speed: 3.0ms preprocess, 152.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f113.jpg: 384x640 4 persons, 158.2ms\n",
            "Speed: 3.1ms preprocess, 158.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f146.jpg: 384x640 4 persons, 155.6ms\n",
            "Speed: 3.8ms preprocess, 155.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f121.jpg: 384x640 4 persons, 202.3ms\n",
            "Speed: 3.1ms preprocess, 202.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f139.jpg: 384x640 4 persons, 249.9ms\n",
            "Speed: 4.5ms preprocess, 249.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f111.jpg: 384x640 3 persons, 240.1ms\n",
            "Speed: 3.8ms preprocess, 240.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f145.jpg: 384x640 4 persons, 253.9ms\n",
            "Speed: 4.5ms preprocess, 253.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f106.jpg: 384x640 3 persons, 167.7ms\n",
            "Speed: 3.2ms preprocess, 167.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f128.jpg: 384x640 4 persons, 156.7ms\n",
            "Speed: 3.0ms preprocess, 156.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f138.jpg: 384x640 4 persons, 162.2ms\n",
            "Speed: 3.0ms preprocess, 162.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f129.jpg: 384x640 4 persons, 176.1ms\n",
            "Speed: 3.2ms preprocess, 176.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f107.jpg: 384x640 3 persons, 157.2ms\n",
            "Speed: 3.0ms preprocess, 157.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f151.jpg: 384x640 4 persons, 171.1ms\n",
            "Speed: 3.1ms preprocess, 171.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f153.jpg: 384x640 4 persons, 154.4ms\n",
            "Speed: 3.0ms preprocess, 154.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f116.jpg: 384x640 5 persons, 158.2ms\n",
            "Speed: 3.0ms preprocess, 158.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v5f105.jpg: 384x640 3 persons, 159.9ms\n",
            "Speed: 3.0ms preprocess, 159.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f48.jpg: 384x640 4 persons, 240.8ms\n",
            "Speed: 4.8ms preprocess, 240.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f24.jpg: 384x640 4 persons, 241.3ms\n",
            "Speed: 5.2ms preprocess, 241.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f25.jpg: 384x640 4 persons, 251.7ms\n",
            "Speed: 4.1ms preprocess, 251.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f15.jpg: 384x640 5 persons, 245.4ms\n",
            "Speed: 4.6ms preprocess, 245.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f34.jpg: 384x640 4 persons, 153.7ms\n",
            "Speed: 3.0ms preprocess, 153.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f49.jpg: 384x640 4 persons, 166.6ms\n",
            "Speed: 3.0ms preprocess, 166.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f43.jpg: 384x640 3 persons, 158.3ms\n",
            "Speed: 3.0ms preprocess, 158.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f10.jpg: 384x640 4 persons, 158.1ms\n",
            "Speed: 3.0ms preprocess, 158.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f27.jpg: 384x640 4 persons, 168.1ms\n",
            "Speed: 3.1ms preprocess, 168.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f44.jpg: 384x640 3 persons, 162.7ms\n",
            "Speed: 3.1ms preprocess, 162.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f65.jpg: 384x640 4 persons, 179.2ms\n",
            "Speed: 3.1ms preprocess, 179.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f18.jpg: 384x640 4 persons, 157.6ms\n",
            "Speed: 3.1ms preprocess, 157.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f59.jpg: 384x640 4 persons, 161.0ms\n",
            "Speed: 3.1ms preprocess, 161.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f14.jpg: 384x640 5 persons, 155.0ms\n",
            "Speed: 3.2ms preprocess, 155.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f11.jpg: 384x640 4 persons, 173.8ms\n",
            "Speed: 3.1ms preprocess, 173.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f111.jpg: 384x640 4 persons, 175.4ms\n",
            "Speed: 3.5ms preprocess, 175.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f80.jpg: 384x640 4 persons, 180.5ms\n",
            "Speed: 3.2ms preprocess, 180.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f90.jpg: 384x640 4 persons, 155.9ms\n",
            "Speed: 3.1ms preprocess, 155.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f102.jpg: 384x640 4 persons, 177.4ms\n",
            "Speed: 2.9ms preprocess, 177.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f105.jpg: 384x640 4 persons, 262.7ms\n",
            "Speed: 5.9ms preprocess, 262.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f87.jpg: 384x640 4 persons, 258.0ms\n",
            "Speed: 4.3ms preprocess, 258.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f77.jpg: 384x640 4 persons, 247.4ms\n",
            "Speed: 4.4ms preprocess, 247.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f71.jpg: 384x640 4 persons, 261.5ms\n",
            "Speed: 4.9ms preprocess, 261.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f74.jpg: 384x640 4 persons, 158.4ms\n",
            "Speed: 3.0ms preprocess, 158.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f75.jpg: 384x640 4 persons, 159.4ms\n",
            "Speed: 3.3ms preprocess, 159.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f104.jpg: 384x640 4 persons, 158.3ms\n",
            "Speed: 3.0ms preprocess, 158.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f81.jpg: 384x640 5 persons, 165.4ms\n",
            "Speed: 3.1ms preprocess, 165.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f78.jpg: 384x640 4 persons, 159.9ms\n",
            "Speed: 3.0ms preprocess, 159.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f70.jpg: 384x640 4 persons, 178.8ms\n",
            "Speed: 3.1ms preprocess, 178.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f76.jpg: 384x640 4 persons, 166.4ms\n",
            "Speed: 3.0ms preprocess, 166.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f83.jpg: 384x640 5 persons, 159.7ms\n",
            "Speed: 3.1ms preprocess, 159.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f114.jpg: 384x640 4 persons, 162.7ms\n",
            "Speed: 3.1ms preprocess, 162.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f98.jpg: 384x640 5 persons, 167.3ms\n",
            "Speed: 3.2ms preprocess, 167.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f9.jpg: 384x640 5 persons, 159.3ms\n",
            "Speed: 3.1ms preprocess, 159.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f128.jpg: 384x640 4 persons, 161.6ms\n",
            "Speed: 3.1ms preprocess, 161.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f3.jpg: 384x640 5 persons, 166.6ms\n",
            "Speed: 3.4ms preprocess, 166.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f121.jpg: 384x640 4 persons, 178.0ms\n",
            "Speed: 3.1ms preprocess, 178.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f5.jpg: 384x640 5 persons, 163.5ms\n",
            "Speed: 3.1ms preprocess, 163.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f131.jpg: 384x640 4 persons, 267.5ms\n",
            "Speed: 4.6ms preprocess, 267.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f125.jpg: 384x640 4 persons, 240.7ms\n",
            "Speed: 4.8ms preprocess, 240.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v22f123.jpg: 384x640 4 persons, 257.4ms\n",
            "Speed: 4.5ms preprocess, 257.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f12.jpg: 384x640 6 persons, 232.6ms\n",
            "Speed: 4.3ms preprocess, 232.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f16.jpg: 384x640 5 persons, 157.1ms\n",
            "Speed: 3.0ms preprocess, 157.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f20.jpg: 384x640 5 persons, 160.7ms\n",
            "Speed: 4.0ms preprocess, 160.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f35.jpg: 384x640 4 persons, 158.7ms\n",
            "Speed: 3.0ms preprocess, 158.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f63.jpg: 384x640 4 persons, 157.8ms\n",
            "Speed: 3.1ms preprocess, 157.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f29.jpg: 384x640 4 persons, 180.6ms\n",
            "Speed: 3.0ms preprocess, 180.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f34.jpg: 384x640 4 persons, 156.4ms\n",
            "Speed: 3.0ms preprocess, 156.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f37.jpg: 384x640 4 persons, 154.1ms\n",
            "Speed: 2.9ms preprocess, 154.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f24.jpg: 384x640 5 persons, 159.4ms\n",
            "Speed: 3.0ms preprocess, 159.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f53.jpg: 384x640 6 persons, 170.6ms\n",
            "Speed: 3.0ms preprocess, 170.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f46.jpg: 384x640 5 persons, 155.0ms\n",
            "Speed: 3.1ms preprocess, 155.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f60.jpg: 384x640 4 persons, 159.2ms\n",
            "Speed: 3.0ms preprocess, 159.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f65.jpg: 384x640 4 persons, 162.8ms\n",
            "Speed: 3.0ms preprocess, 162.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f62.jpg: 384x640 5 persons, 162.8ms\n",
            "Speed: 3.1ms preprocess, 162.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f52.jpg: 384x640 5 persons, 155.0ms\n",
            "Speed: 3.1ms preprocess, 155.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f66.jpg: 384x640 5 persons, 251.7ms\n",
            "Speed: 3.6ms preprocess, 251.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f40.jpg: 384x640 4 persons, 246.7ms\n",
            "Speed: 4.7ms preprocess, 246.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f48.jpg: 384x640 6 persons, 280.9ms\n",
            "Speed: 4.7ms preprocess, 280.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f57.jpg: 384x640 5 persons, 254.1ms\n",
            "Speed: 4.4ms preprocess, 254.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f25.jpg: 384x640 5 persons, 161.5ms\n",
            "Speed: 3.0ms preprocess, 161.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f49.jpg: 384x640 6 persons, 155.6ms\n",
            "Speed: 3.0ms preprocess, 155.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f101.jpg: 384x640 6 persons, 186.0ms\n",
            "Speed: 3.1ms preprocess, 186.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f99.jpg: 384x640 6 persons, 157.3ms\n",
            "Speed: 3.1ms preprocess, 157.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f1.jpg: 384x640 3 persons, 161.6ms\n",
            "Speed: 3.0ms preprocess, 161.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f93.jpg: 384x640 5 persons, 163.9ms\n",
            "Speed: 3.4ms preprocess, 163.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f100.jpg: 384x640 6 persons, 157.4ms\n",
            "Speed: 3.1ms preprocess, 157.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f68.jpg: 384x640 5 persons, 163.7ms\n",
            "Speed: 3.0ms preprocess, 163.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f92.jpg: 384x640 6 persons, 160.5ms\n",
            "Speed: 3.0ms preprocess, 160.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f72.jpg: 384x640 5 persons, 163.9ms\n",
            "Speed: 3.1ms preprocess, 163.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f104.jpg: 384x640 5 persons, 159.4ms\n",
            "Speed: 3.1ms preprocess, 159.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f73.jpg: 384x640 5 persons, 169.4ms\n",
            "Speed: 3.0ms preprocess, 169.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f83.jpg: 384x640 5 persons, 164.2ms\n",
            "Speed: 3.0ms preprocess, 164.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f90.jpg: 384x640 5 persons, 184.0ms\n",
            "Speed: 3.0ms preprocess, 184.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f94.jpg: 384x640 5 persons, 258.0ms\n",
            "Speed: 4.5ms preprocess, 258.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f82.jpg: 384x640 6 persons, 267.8ms\n",
            "Speed: 4.6ms preprocess, 267.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f0.jpg: 384x640 3 persons, 255.5ms\n",
            "Speed: 4.5ms preprocess, 255.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f76.jpg: 384x640 6 persons, 158.6ms\n",
            "Speed: 3.1ms preprocess, 158.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f103.jpg: 384x640 5 persons, 175.2ms\n",
            "Speed: 3.1ms preprocess, 175.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f84.jpg: 384x640 5 persons, 157.3ms\n",
            "Speed: 3.1ms preprocess, 157.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f102.jpg: 384x640 5 persons, 160.6ms\n",
            "Speed: 3.0ms preprocess, 160.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f4.jpg: 384x640 3 persons, 154.4ms\n",
            "Speed: 3.2ms preprocess, 154.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v4f70.jpg: 384x640 4 persons, 156.5ms\n",
            "Speed: 2.9ms preprocess, 156.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f27.jpg: 384x640 3 persons, 167.0ms\n",
            "Speed: 3.0ms preprocess, 167.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f24.jpg: 384x640 3 persons, 153.9ms\n",
            "Speed: 3.0ms preprocess, 153.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f54.jpg: 384x640 3 persons, 163.3ms\n",
            "Speed: 3.1ms preprocess, 163.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f19.jpg: 384x640 3 persons, 424.5ms\n",
            "Speed: 3.3ms preprocess, 424.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f37.jpg: 384x640 3 persons, 249.9ms\n",
            "Speed: 3.6ms preprocess, 249.9ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f28.jpg: 384x640 3 persons, 257.0ms\n",
            "Speed: 3.2ms preprocess, 257.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f11.jpg: 384x640 3 persons, 169.1ms\n",
            "Speed: 4.2ms preprocess, 169.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f34.jpg: 384x640 3 persons, 156.6ms\n",
            "Speed: 3.1ms preprocess, 156.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f18.jpg: 384x640 3 persons, 204.0ms\n",
            "Speed: 3.3ms preprocess, 204.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f51.jpg: 384x640 3 persons, 253.4ms\n",
            "Speed: 4.7ms preprocess, 253.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f41.jpg: 384x640 3 persons, 272.2ms\n",
            "Speed: 4.6ms preprocess, 272.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f21.jpg: 384x640 3 persons, 460.7ms\n",
            "Speed: 10.5ms preprocess, 460.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f44.jpg: 384x640 3 persons, 175.5ms\n",
            "Speed: 4.3ms preprocess, 175.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f23.jpg: 384x640 3 persons, 164.0ms\n",
            "Speed: 3.4ms preprocess, 164.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f46.jpg: 384x640 3 persons, 164.5ms\n",
            "Speed: 3.0ms preprocess, 164.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f36.jpg: 384x640 3 persons, 161.0ms\n",
            "Speed: 3.1ms preprocess, 161.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f25.jpg: 384x640 3 persons, 162.8ms\n",
            "Speed: 3.1ms preprocess, 162.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f13.jpg: 384x640 3 persons, 159.9ms\n",
            "Speed: 3.0ms preprocess, 159.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f22.jpg: 384x640 3 persons, 158.6ms\n",
            "Speed: 3.2ms preprocess, 158.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f10.jpg: 384x640 3 persons, 160.5ms\n",
            "Speed: 3.2ms preprocess, 160.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f17.jpg: 384x640 3 persons, 235.2ms\n",
            "Speed: 3.0ms preprocess, 235.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f92.jpg: 384x640 3 persons, 167.2ms\n",
            "Speed: 4.3ms preprocess, 167.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f79.jpg: 384x640 3 persons, 173.7ms\n",
            "Speed: 3.5ms preprocess, 173.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f77.jpg: 384x640 3 persons, 171.0ms\n",
            "Speed: 3.0ms preprocess, 171.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f105.jpg: 384x640 3 persons, 167.3ms\n",
            "Speed: 3.6ms preprocess, 167.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f106.jpg: 384x640 3 persons, 163.2ms\n",
            "Speed: 3.1ms preprocess, 163.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f112.jpg: 384x640 3 persons, 240.6ms\n",
            "Speed: 4.5ms preprocess, 240.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f75.jpg: 384x640 3 persons, 250.8ms\n",
            "Speed: 4.4ms preprocess, 250.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f108.jpg: 384x640 3 persons, 259.6ms\n",
            "Speed: 4.5ms preprocess, 259.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f60.jpg: 384x640 3 persons, 168.4ms\n",
            "Speed: 4.6ms preprocess, 168.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f101.jpg: 384x640 3 persons, 164.6ms\n",
            "Speed: 3.0ms preprocess, 164.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f103.jpg: 384x640 3 persons, 181.5ms\n",
            "Speed: 4.2ms preprocess, 181.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f104.jpg: 384x640 3 persons, 174.5ms\n",
            "Speed: 3.4ms preprocess, 174.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f86.jpg: 384x640 3 persons, 163.9ms\n",
            "Speed: 3.0ms preprocess, 163.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f113.jpg: 384x640 3 persons, 178.0ms\n",
            "Speed: 2.9ms preprocess, 178.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f91.jpg: 384x640 3 persons, 207.0ms\n",
            "Speed: 3.0ms preprocess, 207.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f78.jpg: 384x640 3 persons, 163.1ms\n",
            "Speed: 3.1ms preprocess, 163.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f114.jpg: 384x640 3 persons, 163.3ms\n",
            "Speed: 3.2ms preprocess, 163.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f80.jpg: 384x640 3 persons, 198.4ms\n",
            "Speed: 3.4ms preprocess, 198.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f97.jpg: 384x640 3 persons, 174.6ms\n",
            "Speed: 3.0ms preprocess, 174.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f94.jpg: 384x640 3 persons, 169.2ms\n",
            "Speed: 4.6ms preprocess, 169.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f130.jpg: 384x640 3 persons, 193.9ms\n",
            "Speed: 3.3ms preprocess, 193.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f140.jpg: 384x640 3 persons, 190.7ms\n",
            "Speed: 3.9ms preprocess, 190.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f120.jpg: 384x640 3 persons, 186.5ms\n",
            "Speed: 3.6ms preprocess, 186.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f148.jpg: 384x640 3 persons, 259.4ms\n",
            "Speed: 4.5ms preprocess, 259.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f136.jpg: 384x640 3 persons, 258.5ms\n",
            "Speed: 4.7ms preprocess, 258.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f137.jpg: 384x640 3 persons, 245.7ms\n",
            "Speed: 4.5ms preprocess, 245.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f146.jpg: 384x640 3 persons, 266.2ms\n",
            "Speed: 4.9ms preprocess, 266.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f144.jpg: 384x640 3 persons, 159.7ms\n",
            "Speed: 3.3ms preprocess, 159.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f147.jpg: 384x640 3 persons, 160.5ms\n",
            "Speed: 3.5ms preprocess, 160.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f118.jpg: 384x640 3 persons, 154.9ms\n",
            "Speed: 3.0ms preprocess, 154.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f123.jpg: 384x640 3 persons, 178.9ms\n",
            "Speed: 3.0ms preprocess, 178.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f152.jpg: 384x640 3 persons, 162.2ms\n",
            "Speed: 3.3ms preprocess, 162.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f131.jpg: 384x640 3 persons, 166.6ms\n",
            "Speed: 3.0ms preprocess, 166.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f121.jpg: 384x640 3 persons, 163.8ms\n",
            "Speed: 3.1ms preprocess, 163.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f155.jpg: 384x640 3 persons, 162.5ms\n",
            "Speed: 3.2ms preprocess, 162.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f157.jpg: 384x640 3 persons, 188.5ms\n",
            "Speed: 4.0ms preprocess, 188.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f119.jpg: 384x640 3 persons, 178.5ms\n",
            "Speed: 3.0ms preprocess, 178.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f138.jpg: 384x640 3 persons, 175.6ms\n",
            "Speed: 3.3ms preprocess, 175.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f180.jpg: 384x640 4 persons, 186.1ms\n",
            "Speed: 3.2ms preprocess, 186.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f182.jpg: 384x640 3 persons, 164.4ms\n",
            "Speed: 3.0ms preprocess, 164.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f163.jpg: 384x640 3 persons, 178.7ms\n",
            "Speed: 3.4ms preprocess, 178.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f206.jpg: 384x640 3 persons, 246.1ms\n",
            "Speed: 4.6ms preprocess, 246.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f193.jpg: 384x640 3 persons, 259.7ms\n",
            "Speed: 4.8ms preprocess, 259.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f211.jpg: 384x640 3 persons, 252.2ms\n",
            "Speed: 4.8ms preprocess, 252.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f190.jpg: 384x640 3 persons, 239.2ms\n",
            "Speed: 4.8ms preprocess, 239.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f161.jpg: 384x640 3 persons, 161.1ms\n",
            "Speed: 3.0ms preprocess, 161.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f203.jpg: 384x640 3 persons, 152.7ms\n",
            "Speed: 3.1ms preprocess, 152.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f216.jpg: 384x640 3 persons, 157.5ms\n",
            "Speed: 3.1ms preprocess, 157.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f174.jpg: 384x640 3 persons, 168.9ms\n",
            "Speed: 3.1ms preprocess, 168.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f213.jpg: 384x640 3 persons, 175.0ms\n",
            "Speed: 3.1ms preprocess, 175.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f159.jpg: 384x640 3 persons, 161.9ms\n",
            "Speed: 2.9ms preprocess, 161.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f191.jpg: 384x640 3 persons, 170.1ms\n",
            "Speed: 3.1ms preprocess, 170.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f183.jpg: 384x640 3 persons, 180.2ms\n",
            "Speed: 3.1ms preprocess, 180.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f19.jpg: 384x640 3 persons, 156.9ms\n",
            "Speed: 3.1ms preprocess, 156.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f22.jpg: 384x640 3 persons, 183.6ms\n",
            "Speed: 4.4ms preprocess, 183.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f234.jpg: 384x640 3 persons, 161.9ms\n",
            "Speed: 3.2ms preprocess, 161.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f14.jpg: 384x640 3 persons, 169.9ms\n",
            "Speed: 3.3ms preprocess, 169.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f11.jpg: 384x640 3 persons, 158.8ms\n",
            "Speed: 3.0ms preprocess, 158.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f16.jpg: 384x640 3 persons, 164.3ms\n",
            "Speed: 3.4ms preprocess, 164.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f220.jpg: 384x640 3 persons, 247.6ms\n",
            "Speed: 4.6ms preprocess, 247.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f12.jpg: 384x640 3 persons, 238.1ms\n",
            "Speed: 4.7ms preprocess, 238.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f9.jpg: 384x640 3 persons, 244.6ms\n",
            "Speed: 4.9ms preprocess, 244.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f4.jpg: 384x640 3 persons, 253.7ms\n",
            "Speed: 4.9ms preprocess, 253.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f223.jpg: 384x640 3 persons, 203.5ms\n",
            "Speed: 5.2ms preprocess, 203.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f217.jpg: 384x640 3 persons, 163.5ms\n",
            "Speed: 3.1ms preprocess, 163.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f13.jpg: 384x640 3 persons, 165.4ms\n",
            "Speed: 3.3ms preprocess, 165.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f226.jpg: 384x640 3 persons, 180.8ms\n",
            "Speed: 3.1ms preprocess, 180.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f229.jpg: 384x640 3 persons, 181.4ms\n",
            "Speed: 3.0ms preprocess, 181.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f221.jpg: 384x640 3 persons, 156.4ms\n",
            "Speed: 3.0ms preprocess, 156.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f3.jpg: 384x640 3 persons, 156.8ms\n",
            "Speed: 3.1ms preprocess, 156.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v59f227.jpg: 384x640 3 persons, 167.4ms\n",
            "Speed: 2.9ms preprocess, 167.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f1.jpg: 384x640 3 persons, 161.2ms\n",
            "Speed: 3.0ms preprocess, 161.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f15.jpg: 384x640 3 persons, 164.1ms\n",
            "Speed: 4.4ms preprocess, 164.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f10.jpg: 384x640 3 persons, 179.4ms\n",
            "Speed: 4.2ms preprocess, 179.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f63.jpg: 384x640 3 persons, 165.9ms\n",
            "Speed: 3.5ms preprocess, 165.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f72.jpg: 384x640 2 persons, 157.5ms\n",
            "Speed: 3.1ms preprocess, 157.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f48.jpg: 384x640 3 persons, 160.6ms\n",
            "Speed: 3.1ms preprocess, 160.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f70.jpg: 384x640 2 persons, 251.1ms\n",
            "Speed: 5.2ms preprocess, 251.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f38.jpg: 384x640 3 persons, 260.8ms\n",
            "Speed: 5.7ms preprocess, 260.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f43.jpg: 384x640 3 persons, 244.7ms\n",
            "Speed: 4.6ms preprocess, 244.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f56.jpg: 384x640 3 persons, 154.9ms\n",
            "Speed: 3.1ms preprocess, 154.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f69.jpg: 384x640 2 persons, 170.1ms\n",
            "Speed: 4.3ms preprocess, 170.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f57.jpg: 384x640 2 persons, 165.2ms\n",
            "Speed: 3.1ms preprocess, 165.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f37.jpg: 384x640 3 persons, 166.0ms\n",
            "Speed: 3.0ms preprocess, 166.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f50.jpg: 384x640 3 persons, 165.2ms\n",
            "Speed: 3.1ms preprocess, 165.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f25.jpg: 384x640 3 persons, 165.5ms\n",
            "Speed: 3.1ms preprocess, 165.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f39.jpg: 384x640 3 persons, 197.9ms\n",
            "Speed: 3.8ms preprocess, 197.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f76.jpg: 384x640 3 persons, 182.4ms\n",
            "Speed: 3.1ms preprocess, 182.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f71.jpg: 384x640 3 persons, 181.3ms\n",
            "Speed: 3.1ms preprocess, 181.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f26.jpg: 384x640 3 persons, 164.2ms\n",
            "Speed: 3.6ms preprocess, 164.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f102.jpg: 384x640 5 persons, 165.3ms\n",
            "Speed: 3.4ms preprocess, 165.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f95.jpg: 384x640 3 persons, 186.8ms\n",
            "Speed: 3.1ms preprocess, 186.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f109.jpg: 384x640 5 persons, 167.1ms\n",
            "Speed: 3.1ms preprocess, 167.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f116.jpg: 384x640 3 persons, 162.3ms\n",
            "Speed: 3.5ms preprocess, 162.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f111.jpg: 384x640 3 persons, 178.7ms\n",
            "Speed: 4.4ms preprocess, 178.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f101.jpg: 384x640 5 persons, 178.2ms\n",
            "Speed: 3.2ms preprocess, 178.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f114.jpg: 384x640 3 persons, 245.7ms\n",
            "Speed: 4.3ms preprocess, 245.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f86.jpg: 384x640 4 persons, 248.1ms\n",
            "Speed: 4.7ms preprocess, 248.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f108.jpg: 384x640 5 persons, 260.6ms\n",
            "Speed: 5.1ms preprocess, 260.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f103.jpg: 384x640 5 persons, 168.4ms\n",
            "Speed: 3.0ms preprocess, 168.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f87.jpg: 384x640 4 persons, 166.2ms\n",
            "Speed: 3.1ms preprocess, 166.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f129.jpg: 384x640 3 persons, 164.4ms\n",
            "Speed: 4.5ms preprocess, 164.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f84.jpg: 384x640 4 persons, 163.3ms\n",
            "Speed: 3.0ms preprocess, 163.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f127.jpg: 384x640 3 persons, 172.6ms\n",
            "Speed: 3.1ms preprocess, 172.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f115.jpg: 384x640 3 persons, 182.3ms\n",
            "Speed: 3.0ms preprocess, 182.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f88.jpg: 384x640 4 persons, 160.6ms\n",
            "Speed: 3.1ms preprocess, 160.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f117.jpg: 384x640 3 persons, 179.3ms\n",
            "Speed: 3.1ms preprocess, 179.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f161.jpg: 384x640 3 persons, 169.5ms\n",
            "Speed: 3.4ms preprocess, 169.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f130.jpg: 384x640 3 persons, 160.6ms\n",
            "Speed: 3.1ms preprocess, 160.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f153.jpg: 384x640 3 persons, 184.3ms\n",
            "Speed: 3.4ms preprocess, 184.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f0.jpg: 384x640 3 persons, 154.6ms\n",
            "Speed: 4.3ms preprocess, 154.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f2.jpg: 384x640 3 persons, 156.0ms\n",
            "Speed: 3.1ms preprocess, 156.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f149.jpg: 384x640 3 persons, 158.2ms\n",
            "Speed: 3.3ms preprocess, 158.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f132.jpg: 384x640 3 persons, 162.8ms\n",
            "Speed: 3.0ms preprocess, 162.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f154.jpg: 384x640 3 persons, 248.6ms\n",
            "Speed: 4.5ms preprocess, 248.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f140.jpg: 384x640 3 persons, 280.3ms\n",
            "Speed: 4.4ms preprocess, 280.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f138.jpg: 384x640 3 persons, 241.7ms\n",
            "Speed: 4.4ms preprocess, 241.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f1.jpg: 384x640 3 persons, 208.8ms\n",
            "Speed: 4.3ms preprocess, 208.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f7.jpg: 384x640 3 persons, 160.2ms\n",
            "Speed: 3.7ms preprocess, 160.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f5.jpg: 384x640 3 persons, 161.6ms\n",
            "Speed: 3.4ms preprocess, 161.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f160.jpg: 384x640 3 persons, 165.9ms\n",
            "Speed: 3.5ms preprocess, 165.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f157.jpg: 384x640 3 persons, 194.7ms\n",
            "Speed: 3.1ms preprocess, 194.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v47f136.jpg: 384x640 3 persons, 203.9ms\n",
            "Speed: 3.1ms preprocess, 203.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f52.jpg: 384x640 3 persons, 174.3ms\n",
            "Speed: 3.5ms preprocess, 174.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f41.jpg: 384x640 3 persons, 170.0ms\n",
            "Speed: 3.1ms preprocess, 170.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f21.jpg: 384x640 3 persons, 169.6ms\n",
            "Speed: 3.2ms preprocess, 169.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f53.jpg: 384x640 3 persons, 165.4ms\n",
            "Speed: 3.0ms preprocess, 165.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f63.jpg: 384x640 3 persons, 161.0ms\n",
            "Speed: 3.2ms preprocess, 161.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f42.jpg: 384x640 3 persons, 170.4ms\n",
            "Speed: 3.7ms preprocess, 170.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f38.jpg: 384x640 3 persons, 173.2ms\n",
            "Speed: 3.1ms preprocess, 173.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f29.jpg: 384x640 3 persons, 195.5ms\n",
            "Speed: 3.1ms preprocess, 195.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f50.jpg: 384x640 3 persons, 194.4ms\n",
            "Speed: 3.9ms preprocess, 194.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f15.jpg: 384x640 3 persons, 159.3ms\n",
            "Speed: 3.0ms preprocess, 159.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f30.jpg: 384x640 4 persons, 246.2ms\n",
            "Speed: 4.5ms preprocess, 246.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f33.jpg: 384x640 4 persons, 253.9ms\n",
            "Speed: 4.5ms preprocess, 253.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f55.jpg: 384x640 3 persons, 246.4ms\n",
            "Speed: 6.7ms preprocess, 246.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f28.jpg: 384x640 3 persons, 241.7ms\n",
            "Speed: 4.3ms preprocess, 241.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f48.jpg: 384x640 3 persons, 164.6ms\n",
            "Speed: 3.1ms preprocess, 164.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f43.jpg: 384x640 3 persons, 170.7ms\n",
            "Speed: 3.3ms preprocess, 170.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f23.jpg: 384x640 3 persons, 172.3ms\n",
            "Speed: 3.1ms preprocess, 172.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f13.jpg: 384x640 3 persons, 181.1ms\n",
            "Speed: 3.1ms preprocess, 181.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f65.jpg: 384x640 3 persons, 168.8ms\n",
            "Speed: 3.3ms preprocess, 168.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f31.jpg: 384x640 3 persons, 188.1ms\n",
            "Speed: 3.1ms preprocess, 188.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f76.jpg: 384x640 3 persons, 177.8ms\n",
            "Speed: 3.2ms preprocess, 177.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f122.jpg: 384x640 3 persons, 182.1ms\n",
            "Speed: 3.1ms preprocess, 182.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f72.jpg: 384x640 3 persons, 164.0ms\n",
            "Speed: 3.3ms preprocess, 164.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f90.jpg: 384x640 3 persons, 171.5ms\n",
            "Speed: 3.8ms preprocess, 171.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f97.jpg: 384x640 3 persons, 166.8ms\n",
            "Speed: 3.2ms preprocess, 166.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f96.jpg: 384x640 3 persons, 171.5ms\n",
            "Speed: 3.1ms preprocess, 171.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f112.jpg: 384x640 3 persons, 162.4ms\n",
            "Speed: 3.2ms preprocess, 162.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f68.jpg: 384x640 3 persons, 200.7ms\n",
            "Speed: 4.9ms preprocess, 200.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f78.jpg: 384x640 3 persons, 286.1ms\n",
            "Speed: 3.4ms preprocess, 286.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f129.jpg: 384x640 3 persons, 254.9ms\n",
            "Speed: 5.1ms preprocess, 254.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f110.jpg: 384x640 3 persons, 255.9ms\n",
            "Speed: 4.7ms preprocess, 255.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f82.jpg: 384x640 3 persons, 174.2ms\n",
            "Speed: 4.6ms preprocess, 174.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f126.jpg: 384x640 3 persons, 163.8ms\n",
            "Speed: 3.0ms preprocess, 163.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f101.jpg: 384x640 3 persons, 160.6ms\n",
            "Speed: 3.1ms preprocess, 160.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f77.jpg: 384x640 3 persons, 162.9ms\n",
            "Speed: 3.0ms preprocess, 162.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f79.jpg: 384x640 3 persons, 189.1ms\n",
            "Speed: 3.2ms preprocess, 189.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f118.jpg: 384x640 3 persons, 193.5ms\n",
            "Speed: 3.6ms preprocess, 193.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f71.jpg: 384x640 3 persons, 167.3ms\n",
            "Speed: 3.1ms preprocess, 167.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f100.jpg: 384x640 3 persons, 169.1ms\n",
            "Speed: 3.1ms preprocess, 169.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f109.jpg: 384x640 3 persons, 165.4ms\n",
            "Speed: 3.1ms preprocess, 165.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f119.jpg: 384x640 3 persons, 169.4ms\n",
            "Speed: 3.5ms preprocess, 169.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f120.jpg: 384x640 3 persons, 175.7ms\n",
            "Speed: 3.1ms preprocess, 175.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f175.jpg: 384x640 3 persons, 163.4ms\n",
            "Speed: 3.1ms preprocess, 163.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f139.jpg: 384x640 3 persons, 189.3ms\n",
            "Speed: 3.6ms preprocess, 189.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f155.jpg: 384x640 3 persons, 172.5ms\n",
            "Speed: 3.3ms preprocess, 172.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f156.jpg: 384x640 3 persons, 172.2ms\n",
            "Speed: 4.6ms preprocess, 172.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f153.jpg: 384x640 3 persons, 256.7ms\n",
            "Speed: 3.0ms preprocess, 256.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f141.jpg: 384x640 3 persons, 284.2ms\n",
            "Speed: 6.8ms preprocess, 284.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f163.jpg: 384x640 3 persons, 272.2ms\n",
            "Speed: 4.8ms preprocess, 272.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f151.jpg: 384x640 3 persons, 311.7ms\n",
            "Speed: 3.6ms preprocess, 311.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f150.jpg: 384x640 3 persons, 178.3ms\n",
            "Speed: 3.1ms preprocess, 178.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f177.jpg: 384x640 3 persons, 185.6ms\n",
            "Speed: 3.5ms preprocess, 185.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f159.jpg: 384x640 3 persons, 171.9ms\n",
            "Speed: 3.6ms preprocess, 171.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f130.jpg: 384x640 3 persons, 175.4ms\n",
            "Speed: 3.0ms preprocess, 175.4ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f152.jpg: 384x640 3 persons, 176.4ms\n",
            "Speed: 3.1ms preprocess, 176.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f176.jpg: 384x640 3 persons, 173.7ms\n",
            "Speed: 3.4ms preprocess, 173.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f164.jpg: 384x640 3 persons, 168.5ms\n",
            "Speed: 3.1ms preprocess, 168.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f143.jpg: 384x640 3 persons, 200.2ms\n",
            "Speed: 3.2ms preprocess, 200.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f136.jpg: 384x640 3 persons, 164.3ms\n",
            "Speed: 3.1ms preprocess, 164.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f157.jpg: 384x640 3 persons, 169.9ms\n",
            "Speed: 3.1ms preprocess, 169.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f148.jpg: 384x640 3 persons, 189.0ms\n",
            "Speed: 3.0ms preprocess, 189.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f171.jpg: 384x640 3 persons, 164.7ms\n",
            "Speed: 3.1ms preprocess, 164.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f138.jpg: 384x640 3 persons, 168.1ms\n",
            "Speed: 3.2ms preprocess, 168.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f36.jpg: 384x640 3 persons, 182.6ms\n",
            "Speed: 3.3ms preprocess, 182.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f33.jpg: 384x640 3 persons, 195.7ms\n",
            "Speed: 3.2ms preprocess, 195.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f180.jpg: 384x640 4 persons, 239.3ms\n",
            "Speed: 4.6ms preprocess, 239.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f1.jpg: 384x640 3 persons, 245.0ms\n",
            "Speed: 4.6ms preprocess, 245.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f181.jpg: 384x640 4 persons, 195.7ms\n",
            "Speed: 5.7ms preprocess, 195.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f15.jpg: 384x640 3 persons, 163.6ms\n",
            "Speed: 3.1ms preprocess, 163.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f19.jpg: 384x640 3 persons, 169.0ms\n",
            "Speed: 3.2ms preprocess, 169.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f4.jpg: 384x640 3 persons, 159.6ms\n",
            "Speed: 3.1ms preprocess, 159.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f25.jpg: 384x640 3 persons, 164.5ms\n",
            "Speed: 3.1ms preprocess, 164.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f2.jpg: 384x640 3 persons, 160.1ms\n",
            "Speed: 3.0ms preprocess, 160.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f37.jpg: 384x640 3 persons, 159.8ms\n",
            "Speed: 3.0ms preprocess, 159.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f31.jpg: 384x640 3 persons, 181.7ms\n",
            "Speed: 3.0ms preprocess, 181.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f3.jpg: 384x640 3 persons, 156.8ms\n",
            "Speed: 3.2ms preprocess, 156.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f17.jpg: 384x640 3 persons, 195.5ms\n",
            "Speed: 3.4ms preprocess, 195.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f38.jpg: 384x640 3 persons, 182.5ms\n",
            "Speed: 4.0ms preprocess, 182.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f189.jpg: 384x640 4 persons, 205.5ms\n",
            "Speed: 3.4ms preprocess, 205.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f30.jpg: 384x640 3 persons, 185.9ms\n",
            "Speed: 3.5ms preprocess, 185.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f26.jpg: 384x640 3 persons, 184.2ms\n",
            "Speed: 4.2ms preprocess, 184.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v50f187.jpg: 384x640 4 persons, 753.1ms\n",
            "Speed: 11.8ms preprocess, 753.1ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f94.jpg: 384x640 3 persons, 664.2ms\n",
            "Speed: 7.9ms preprocess, 664.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f63.jpg: 384x640 3 persons, 995.9ms\n",
            "Speed: 24.6ms preprocess, 995.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f59.jpg: 384x640 3 persons, 370.4ms\n",
            "Speed: 10.9ms preprocess, 370.4ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f43.jpg: 384x640 3 persons, 285.7ms\n",
            "Speed: 7.7ms preprocess, 285.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f52.jpg: 384x640 3 persons, 222.3ms\n",
            "Speed: 4.6ms preprocess, 222.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f85.jpg: 384x640 3 persons, 167.7ms\n",
            "Speed: 3.1ms preprocess, 167.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f93.jpg: 384x640 4 persons, 164.7ms\n",
            "Speed: 3.0ms preprocess, 164.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f91.jpg: 384x640 3 persons, 183.1ms\n",
            "Speed: 3.6ms preprocess, 183.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f60.jpg: 384x640 3 persons, 163.8ms\n",
            "Speed: 3.3ms preprocess, 163.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f47.jpg: 384x640 3 persons, 155.7ms\n",
            "Speed: 3.2ms preprocess, 155.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f69.jpg: 384x640 4 persons, 159.9ms\n",
            "Speed: 3.1ms preprocess, 159.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f79.jpg: 384x640 3 persons, 156.0ms\n",
            "Speed: 3.0ms preprocess, 156.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f65.jpg: 384x640 3 persons, 176.8ms\n",
            "Speed: 3.0ms preprocess, 176.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f68.jpg: 384x640 4 persons, 161.9ms\n",
            "Speed: 3.1ms preprocess, 161.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f53.jpg: 384x640 3 persons, 159.7ms\n",
            "Speed: 3.0ms preprocess, 159.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f127.jpg: 384x640 2 persons, 158.5ms\n",
            "Speed: 3.1ms preprocess, 158.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f150.jpg: 384x640 4 persons, 164.8ms\n",
            "Speed: 3.1ms preprocess, 164.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f128.jpg: 384x640 2 persons, 183.6ms\n",
            "Speed: 3.1ms preprocess, 183.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f125.jpg: 384x640 2 persons, 169.9ms\n",
            "Speed: 3.1ms preprocess, 169.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f105.jpg: 384x640 4 persons, 174.6ms\n",
            "Speed: 4.5ms preprocess, 174.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f148.jpg: 384x640 3 persons, 267.0ms\n",
            "Speed: 6.2ms preprocess, 267.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f121.jpg: 384x640 4 persons, 257.9ms\n",
            "Speed: 4.5ms preprocess, 257.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f136.jpg: 384x640 2 persons, 259.4ms\n",
            "Speed: 5.0ms preprocess, 259.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f106.jpg: 384x640 4 persons, 246.2ms\n",
            "Speed: 4.8ms preprocess, 246.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f144.jpg: 384x640 2 persons, 259.0ms\n",
            "Speed: 4.9ms preprocess, 259.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f108.jpg: 384x640 4 persons, 160.5ms\n",
            "Speed: 3.1ms preprocess, 160.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f110.jpg: 384x640 4 persons, 159.0ms\n",
            "Speed: 3.1ms preprocess, 159.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f149.jpg: 384x640 3 persons, 174.5ms\n",
            "Speed: 3.1ms preprocess, 174.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f141.jpg: 384x640 2 persons, 242.7ms\n",
            "Speed: 4.6ms preprocess, 242.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f102.jpg: 384x640 4 persons, 157.7ms\n",
            "Speed: 3.0ms preprocess, 157.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f146.jpg: 384x640 3 persons, 179.5ms\n",
            "Speed: 4.2ms preprocess, 179.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f99.jpg: 384x640 4 persons, 172.1ms\n",
            "Speed: 3.2ms preprocess, 172.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f145.jpg: 384x640 3 persons, 157.5ms\n",
            "Speed: 3.1ms preprocess, 157.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f193.jpg: 384x640 3 persons, 168.2ms\n",
            "Speed: 3.6ms preprocess, 168.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f186.jpg: 384x640 3 persons, 161.8ms\n",
            "Speed: 3.0ms preprocess, 161.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f155.jpg: 384x640 3 persons, 178.7ms\n",
            "Speed: 3.0ms preprocess, 178.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f173.jpg: 384x640 2 persons, 161.4ms\n",
            "Speed: 3.0ms preprocess, 161.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f190.jpg: 384x640 3 persons, 170.8ms\n",
            "Speed: 3.9ms preprocess, 170.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f158.jpg: 384x640 2 persons, 157.4ms\n",
            "Speed: 2.9ms preprocess, 157.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f159.jpg: 384x640 2 persons, 182.6ms\n",
            "Speed: 3.3ms preprocess, 182.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f162.jpg: 384x640 2 persons, 253.2ms\n",
            "Speed: 4.4ms preprocess, 253.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f191.jpg: 384x640 3 persons, 251.5ms\n",
            "Speed: 7.3ms preprocess, 251.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f194.jpg: 384x640 3 persons, 262.6ms\n",
            "Speed: 9.8ms preprocess, 262.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f163.jpg: 384x640 2 persons, 167.3ms\n",
            "Speed: 4.5ms preprocess, 167.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f156.jpg: 384x640 3 persons, 166.1ms\n",
            "Speed: 3.2ms preprocess, 166.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f160.jpg: 384x640 2 persons, 155.5ms\n",
            "Speed: 3.1ms preprocess, 155.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f154.jpg: 384x640 3 persons, 160.6ms\n",
            "Speed: 3.0ms preprocess, 160.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f188.jpg: 384x640 3 persons, 162.6ms\n",
            "Speed: 3.2ms preprocess, 162.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f169.jpg: 384x640 2 persons, 161.7ms\n",
            "Speed: 3.0ms preprocess, 161.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f182.jpg: 384x640 2 persons, 168.1ms\n",
            "Speed: 3.1ms preprocess, 168.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f179.jpg: 384x640 2 persons, 159.7ms\n",
            "Speed: 3.1ms preprocess, 159.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f178.jpg: 384x640 2 persons, 183.7ms\n",
            "Speed: 3.1ms preprocess, 183.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f32.jpg: 384x640 3 persons, 169.2ms\n",
            "Speed: 3.9ms preprocess, 169.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f27.jpg: 384x640 3 persons, 158.8ms\n",
            "Speed: 3.0ms preprocess, 158.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f195.jpg: 384x640 3 persons, 157.9ms\n",
            "Speed: 3.2ms preprocess, 157.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f50.jpg: 384x640 3 persons, 165.0ms\n",
            "Speed: 3.1ms preprocess, 165.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f51.jpg: 384x640 3 persons, 159.8ms\n",
            "Speed: 2.9ms preprocess, 159.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f197.jpg: 384x640 3 persons, 156.1ms\n",
            "Speed: 3.1ms preprocess, 156.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f13.jpg: 384x640 3 persons, 159.3ms\n",
            "Speed: 3.0ms preprocess, 159.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f196.jpg: 384x640 3 persons, 155.3ms\n",
            "Speed: 3.1ms preprocess, 155.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f42.jpg: 384x640 3 persons, 248.9ms\n",
            "Speed: 5.6ms preprocess, 248.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f11.jpg: 384x640 3 persons, 256.8ms\n",
            "Speed: 4.5ms preprocess, 256.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f19.jpg: 384x640 3 persons, 250.8ms\n",
            "Speed: 5.6ms preprocess, 250.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f7.jpg: 384x640 3 persons, 165.7ms\n",
            "Speed: 3.3ms preprocess, 165.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f28.jpg: 384x640 3 persons, 170.1ms\n",
            "Speed: 4.4ms preprocess, 170.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v38f198.jpg: 384x640 3 persons, 182.2ms\n",
            "Speed: 3.1ms preprocess, 182.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f21.jpg: 384x640 3 persons, 163.8ms\n",
            "Speed: 3.4ms preprocess, 163.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f36.jpg: 384x640 3 persons, 161.3ms\n",
            "Speed: 3.1ms preprocess, 161.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f44.jpg: 384x640 3 persons, 167.9ms\n",
            "Speed: 3.0ms preprocess, 167.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f31.jpg: 384x640 3 persons, 170.6ms\n",
            "Speed: 3.1ms preprocess, 170.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f40.jpg: 384x640 3 persons, 203.0ms\n",
            "Speed: 3.0ms preprocess, 203.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f22.jpg: 384x640 3 persons, 160.6ms\n",
            "Speed: 3.2ms preprocess, 160.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f65.jpg: 384x640 3 persons, 173.1ms\n",
            "Speed: 3.1ms preprocess, 173.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f80.jpg: 384x640 3 persons, 198.8ms\n",
            "Speed: 3.3ms preprocess, 198.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f94.jpg: 384x640 3 persons, 156.0ms\n",
            "Speed: 3.0ms preprocess, 156.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f102.jpg: 384x640 3 persons, 170.8ms\n",
            "Speed: 3.7ms preprocess, 170.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f85.jpg: 384x640 3 persons, 160.9ms\n",
            "Speed: 3.0ms preprocess, 160.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f75.jpg: 384x640 2 persons, 190.1ms\n",
            "Speed: 3.2ms preprocess, 190.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f67.jpg: 384x640 3 persons, 237.5ms\n",
            "Speed: 4.5ms preprocess, 237.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f96.jpg: 384x640 3 persons, 263.5ms\n",
            "Speed: 4.5ms preprocess, 263.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f61.jpg: 384x640 3 persons, 268.5ms\n",
            "Speed: 4.6ms preprocess, 268.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f73.jpg: 384x640 3 persons, 236.6ms\n",
            "Speed: 4.5ms preprocess, 236.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f97.jpg: 384x640 3 persons, 162.7ms\n",
            "Speed: 3.2ms preprocess, 162.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f101.jpg: 384x640 3 persons, 184.8ms\n",
            "Speed: 3.1ms preprocess, 184.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f99.jpg: 384x640 3 persons, 184.4ms\n",
            "Speed: 3.3ms preprocess, 184.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f57.jpg: 384x640 3 persons, 167.3ms\n",
            "Speed: 3.3ms preprocess, 167.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f88.jpg: 384x640 3 persons, 184.2ms\n",
            "Speed: 3.1ms preprocess, 184.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f76.jpg: 384x640 3 persons, 167.8ms\n",
            "Speed: 3.1ms preprocess, 167.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f98.jpg: 384x640 3 persons, 173.1ms\n",
            "Speed: 3.6ms preprocess, 173.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f90.jpg: 384x640 3 persons, 174.7ms\n",
            "Speed: 3.1ms preprocess, 174.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f70.jpg: 384x640 3 persons, 174.0ms\n",
            "Speed: 3.3ms preprocess, 174.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f89.jpg: 384x640 3 persons, 171.2ms\n",
            "Speed: 3.4ms preprocess, 171.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f60.jpg: 384x640 3 persons, 160.4ms\n",
            "Speed: 3.3ms preprocess, 160.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f10.jpg: 384x640 2 persons, 195.3ms\n",
            "Speed: 3.1ms preprocess, 195.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f127.jpg: 384x640 3 persons, 162.9ms\n",
            "Speed: 3.3ms preprocess, 162.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f123.jpg: 384x640 3 persons, 250.2ms\n",
            "Speed: 4.6ms preprocess, 250.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f125.jpg: 384x640 3 persons, 249.0ms\n",
            "Speed: 3.6ms preprocess, 249.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f147.jpg: 384x640 3 persons, 248.6ms\n",
            "Speed: 5.4ms preprocess, 248.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f11.jpg: 384x640 2 persons, 254.9ms\n",
            "Speed: 3.6ms preprocess, 254.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f109.jpg: 384x640 3 persons, 157.3ms\n",
            "Speed: 3.0ms preprocess, 157.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f108.jpg: 384x640 3 persons, 160.8ms\n",
            "Speed: 2.9ms preprocess, 160.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f131.jpg: 384x640 3 persons, 162.0ms\n",
            "Speed: 3.1ms preprocess, 162.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f111.jpg: 384x640 3 persons, 181.0ms\n",
            "Speed: 3.4ms preprocess, 181.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f148.jpg: 384x640 3 persons, 183.2ms\n",
            "Speed: 3.1ms preprocess, 183.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f126.jpg: 384x640 3 persons, 180.8ms\n",
            "Speed: 3.4ms preprocess, 180.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f2.jpg: 384x640 2 persons, 179.6ms\n",
            "Speed: 3.0ms preprocess, 179.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f149.jpg: 384x640 3 persons, 192.3ms\n",
            "Speed: 3.1ms preprocess, 192.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f124.jpg: 384x640 3 persons, 161.7ms\n",
            "Speed: 3.1ms preprocess, 161.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f12.jpg: 384x640 2 persons, 163.7ms\n",
            "Speed: 3.0ms preprocess, 163.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f137.jpg: 384x640 3 persons, 158.7ms\n",
            "Speed: 3.1ms preprocess, 158.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f136.jpg: 384x640 3 persons, 176.3ms\n",
            "Speed: 3.0ms preprocess, 176.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v53f140.jpg: 384x640 3 persons, 159.0ms\n",
            "Speed: 3.2ms preprocess, 159.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f38.jpg: 384x640 2 persons, 165.8ms\n",
            "Speed: 3.3ms preprocess, 165.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f37.jpg: 384x640 2 persons, 162.5ms\n",
            "Speed: 3.1ms preprocess, 162.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f30.jpg: 384x640 2 persons, 264.0ms\n",
            "Speed: 3.2ms preprocess, 264.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f25.jpg: 384x640 2 persons, 251.9ms\n",
            "Speed: 4.5ms preprocess, 251.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f31.jpg: 384x640 2 persons, 280.7ms\n",
            "Speed: 4.6ms preprocess, 280.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f44.jpg: 384x640 2 persons, 280.5ms\n",
            "Speed: 4.5ms preprocess, 280.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f20.jpg: 384x640 2 persons, 161.4ms\n",
            "Speed: 3.1ms preprocess, 161.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f47.jpg: 384x640 2 persons, 165.5ms\n",
            "Speed: 3.0ms preprocess, 165.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f28.jpg: 384x640 2 persons, 168.6ms\n",
            "Speed: 3.4ms preprocess, 168.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f21.jpg: 384x640 2 persons, 156.2ms\n",
            "Speed: 3.3ms preprocess, 156.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f48.jpg: 384x640 2 persons, 192.0ms\n",
            "Speed: 5.3ms preprocess, 192.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f46.jpg: 384x640 2 persons, 170.1ms\n",
            "Speed: 3.0ms preprocess, 170.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f26.jpg: 384x640 2 persons, 164.7ms\n",
            "Speed: 3.0ms preprocess, 164.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f42.jpg: 384x640 2 persons, 178.0ms\n",
            "Speed: 3.0ms preprocess, 178.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f51.jpg: 384x640 2 persons, 168.5ms\n",
            "Speed: 3.0ms preprocess, 168.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f17.jpg: 384x640 2 persons, 181.5ms\n",
            "Speed: 3.0ms preprocess, 181.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f49.jpg: 384x640 2 persons, 166.0ms\n",
            "Speed: 3.1ms preprocess, 166.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f60.jpg: 384x640 2 persons, 174.2ms\n",
            "Speed: 3.1ms preprocess, 174.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f59.jpg: 384x640 2 persons, 193.3ms\n",
            "Speed: 3.2ms preprocess, 193.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f29.jpg: 384x640 2 persons, 171.5ms\n",
            "Speed: 3.4ms preprocess, 171.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f56.jpg: 384x640 2 persons, 278.6ms\n",
            "Speed: 4.8ms preprocess, 278.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f24.jpg: 384x640 2 persons, 260.6ms\n",
            "Speed: 4.5ms preprocess, 260.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f45.jpg: 384x640 2 persons, 251.5ms\n",
            "Speed: 4.7ms preprocess, 251.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f52.jpg: 384x640 2 persons, 165.7ms\n",
            "Speed: 3.5ms preprocess, 165.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f85.jpg: 384x640 2 persons, 189.5ms\n",
            "Speed: 3.8ms preprocess, 189.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f110.jpg: 384x640 2 persons, 159.3ms\n",
            "Speed: 3.0ms preprocess, 159.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f68.jpg: 384x640 3 persons, 191.3ms\n",
            "Speed: 2.9ms preprocess, 191.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f66.jpg: 384x640 2 persons, 159.9ms\n",
            "Speed: 3.0ms preprocess, 159.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f88.jpg: 384x640 2 persons, 161.5ms\n",
            "Speed: 3.9ms preprocess, 161.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f108.jpg: 384x640 2 persons, 154.7ms\n",
            "Speed: 3.0ms preprocess, 154.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f65.jpg: 384x640 2 persons, 173.2ms\n",
            "Speed: 3.0ms preprocess, 173.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f73.jpg: 384x640 2 persons, 159.6ms\n",
            "Speed: 3.0ms preprocess, 159.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f89.jpg: 384x640 2 persons, 161.2ms\n",
            "Speed: 3.0ms preprocess, 161.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f70.jpg: 384x640 3 persons, 168.1ms\n",
            "Speed: 3.2ms preprocess, 168.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f86.jpg: 384x640 2 persons, 186.9ms\n",
            "Speed: 3.0ms preprocess, 186.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f92.jpg: 384x640 2 persons, 173.8ms\n",
            "Speed: 3.1ms preprocess, 173.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f79.jpg: 384x640 2 persons, 173.8ms\n",
            "Speed: 3.0ms preprocess, 173.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f100.jpg: 384x640 2 persons, 169.0ms\n",
            "Speed: 3.0ms preprocess, 169.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f78.jpg: 384x640 2 persons, 160.0ms\n",
            "Speed: 3.0ms preprocess, 160.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f103.jpg: 384x640 2 persons, 258.0ms\n",
            "Speed: 4.4ms preprocess, 258.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f96.jpg: 384x640 2 persons, 247.9ms\n",
            "Speed: 4.6ms preprocess, 247.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f87.jpg: 384x640 2 persons, 250.1ms\n",
            "Speed: 4.4ms preprocess, 250.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f91.jpg: 384x640 2 persons, 165.3ms\n",
            "Speed: 3.0ms preprocess, 165.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f128.jpg: 384x640 2 persons, 163.1ms\n",
            "Speed: 2.9ms preprocess, 163.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f1.jpg: 384x640 3 persons, 182.9ms\n",
            "Speed: 3.0ms preprocess, 182.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f5.jpg: 384x640 3 persons, 166.7ms\n",
            "Speed: 3.0ms preprocess, 166.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f11.jpg: 384x640 3 persons, 164.1ms\n",
            "Speed: 3.0ms preprocess, 164.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f114.jpg: 384x640 2 persons, 191.9ms\n",
            "Speed: 3.0ms preprocess, 191.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f4.jpg: 384x640 3 persons, 173.9ms\n",
            "Speed: 3.1ms preprocess, 173.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f32.jpg: 384x640 3 persons, 188.7ms\n",
            "Speed: 3.0ms preprocess, 188.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f19.jpg: 384x640 4 persons, 159.9ms\n",
            "Speed: 3.1ms preprocess, 159.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f24.jpg: 384x640 3 persons, 162.1ms\n",
            "Speed: 2.9ms preprocess, 162.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f2.jpg: 384x640 3 persons, 162.6ms\n",
            "Speed: 3.0ms preprocess, 162.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f6.jpg: 384x640 3 persons, 166.9ms\n",
            "Speed: 3.0ms preprocess, 166.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f129.jpg: 384x640 2 persons, 166.9ms\n",
            "Speed: 4.4ms preprocess, 166.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f22.jpg: 384x640 3 persons, 156.6ms\n",
            "Speed: 3.0ms preprocess, 156.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f132.jpg: 384x640 3 persons, 258.5ms\n",
            "Speed: 4.5ms preprocess, 258.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f10.jpg: 384x640 3 persons, 250.2ms\n",
            "Speed: 4.7ms preprocess, 250.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f122.jpg: 384x640 2 persons, 259.0ms\n",
            "Speed: 4.5ms preprocess, 259.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f23.jpg: 384x640 3 persons, 257.9ms\n",
            "Speed: 4.3ms preprocess, 257.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v42f120.jpg: 384x640 2 persons, 166.6ms\n",
            "Speed: 3.0ms preprocess, 166.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f79.jpg: 384x640 3 persons, 183.0ms\n",
            "Speed: 4.3ms preprocess, 183.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f62.jpg: 384x640 2 persons, 160.3ms\n",
            "Speed: 3.0ms preprocess, 160.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f75.jpg: 384x640 3 persons, 161.8ms\n",
            "Speed: 3.0ms preprocess, 161.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f70.jpg: 384x640 3 persons, 164.3ms\n",
            "Speed: 3.0ms preprocess, 164.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f54.jpg: 384x640 2 persons, 184.2ms\n",
            "Speed: 3.1ms preprocess, 184.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f38.jpg: 384x640 4 persons, 160.4ms\n",
            "Speed: 3.1ms preprocess, 160.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f51.jpg: 384x640 3 persons, 158.9ms\n",
            "Speed: 3.1ms preprocess, 158.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f72.jpg: 384x640 3 persons, 159.0ms\n",
            "Speed: 3.1ms preprocess, 159.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f64.jpg: 384x640 2 persons, 158.8ms\n",
            "Speed: 3.1ms preprocess, 158.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f69.jpg: 384x640 3 persons, 172.3ms\n",
            "Speed: 3.7ms preprocess, 172.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f78.jpg: 384x640 3 persons, 164.5ms\n",
            "Speed: 3.0ms preprocess, 164.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f57.jpg: 384x640 2 persons, 157.2ms\n",
            "Speed: 3.0ms preprocess, 157.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f77.jpg: 384x640 3 persons, 187.3ms\n",
            "Speed: 3.1ms preprocess, 187.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f65.jpg: 384x640 2 persons, 268.8ms\n",
            "Speed: 4.5ms preprocess, 268.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f37.jpg: 384x640 4 persons, 246.4ms\n",
            "Speed: 4.7ms preprocess, 246.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f63.jpg: 384x640 2 persons, 267.4ms\n",
            "Speed: 4.3ms preprocess, 267.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f74.jpg: 384x640 3 persons, 258.1ms\n",
            "Speed: 4.6ms preprocess, 258.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f71.jpg: 384x640 3 persons, 161.5ms\n",
            "Speed: 3.0ms preprocess, 161.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f33.jpg: 384x640 3 persons, 162.3ms\n",
            "Speed: 3.1ms preprocess, 162.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f76.jpg: 384x640 3 persons, 177.5ms\n",
            "Speed: 3.7ms preprocess, 177.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f93.jpg: 384x640 2 persons, 156.0ms\n",
            "Speed: 2.9ms preprocess, 156.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f85.jpg: 384x640 2 persons, 156.5ms\n",
            "Speed: 3.2ms preprocess, 156.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f103.jpg: 384x640 3 persons, 178.5ms\n",
            "Speed: 3.0ms preprocess, 178.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f88.jpg: 384x640 3 persons, 167.8ms\n",
            "Speed: 3.7ms preprocess, 167.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f126.jpg: 384x640 3 persons, 163.3ms\n",
            "Speed: 3.0ms preprocess, 163.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f113.jpg: 384x640 3 persons, 188.7ms\n",
            "Speed: 3.4ms preprocess, 188.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f118.jpg: 384x640 3 persons, 184.6ms\n",
            "Speed: 3.3ms preprocess, 184.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f132.jpg: 384x640 3 persons, 164.3ms\n",
            "Speed: 3.4ms preprocess, 164.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f101.jpg: 384x640 2 persons, 160.2ms\n",
            "Speed: 3.4ms preprocess, 160.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f106.jpg: 384x640 3 persons, 162.3ms\n",
            "Speed: 3.4ms preprocess, 162.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f130.jpg: 384x640 3 persons, 169.2ms\n",
            "Speed: 4.8ms preprocess, 169.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f111.jpg: 384x640 2 persons, 246.0ms\n",
            "Speed: 4.6ms preprocess, 246.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f87.jpg: 384x640 2 persons, 283.0ms\n",
            "Speed: 4.5ms preprocess, 283.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f102.jpg: 384x640 3 persons, 266.0ms\n",
            "Speed: 4.5ms preprocess, 266.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f90.jpg: 384x640 4 persons, 190.0ms\n",
            "Speed: 3.4ms preprocess, 190.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f86.jpg: 384x640 2 persons, 160.8ms\n",
            "Speed: 3.0ms preprocess, 160.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f83.jpg: 384x640 2 persons, 181.1ms\n",
            "Speed: 3.1ms preprocess, 181.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f112.jpg: 384x640 3 persons, 186.3ms\n",
            "Speed: 6.3ms preprocess, 186.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f107.jpg: 384x640 2 persons, 164.8ms\n",
            "Speed: 4.5ms preprocess, 164.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f89.jpg: 384x640 3 persons, 164.0ms\n",
            "Speed: 3.0ms preprocess, 164.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f133.jpg: 384x640 3 persons, 177.5ms\n",
            "Speed: 3.1ms preprocess, 177.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f7.jpg: 384x640 4 persons, 168.6ms\n",
            "Speed: 3.1ms preprocess, 168.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f140.jpg: 384x640 3 persons, 166.6ms\n",
            "Speed: 3.0ms preprocess, 166.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f16.jpg: 384x640 4 persons, 194.3ms\n",
            "Speed: 3.1ms preprocess, 194.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f14.jpg: 384x640 4 persons, 187.5ms\n",
            "Speed: 3.1ms preprocess, 187.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f34.jpg: 384x640 4 persons, 173.0ms\n",
            "Speed: 4.5ms preprocess, 173.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f35.jpg: 384x640 4 persons, 178.2ms\n",
            "Speed: 3.2ms preprocess, 178.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f20.jpg: 384x640 3 persons, 174.7ms\n",
            "Speed: 3.2ms preprocess, 174.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v58f141.jpg: 384x640 3 persons, 164.2ms\n",
            "Speed: 3.3ms preprocess, 164.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f33.jpg: 384x640 4 persons, 260.3ms\n",
            "Speed: 8.9ms preprocess, 260.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f12.jpg: 384x640 4 persons, 263.1ms\n",
            "Speed: 4.3ms preprocess, 263.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f24.jpg: 384x640 3 persons, 244.2ms\n",
            "Speed: 4.7ms preprocess, 244.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f17.jpg: 384x640 4 persons, 405.0ms\n",
            "Speed: 6.7ms preprocess, 405.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f18.jpg: 384x640 4 persons, 253.6ms\n",
            "Speed: 4.0ms preprocess, 253.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f27.jpg: 384x640 3 persons, 234.8ms\n",
            "Speed: 4.2ms preprocess, 234.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f1.jpg: 384x640 4 persons, 238.5ms\n",
            "Speed: 3.8ms preprocess, 238.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f15.jpg: 384x640 3 persons, 247.2ms\n",
            "Speed: 4.9ms preprocess, 247.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f4.jpg: 384x640 4 persons, 241.5ms\n",
            "Speed: 5.7ms preprocess, 241.5ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f58.jpg: 384x640 4 persons, 259.2ms\n",
            "Speed: 5.3ms preprocess, 259.2ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f46.jpg: 384x640 4 persons, 288.3ms\n",
            "Speed: 3.4ms preprocess, 288.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f49.jpg: 384x640 4 persons, 260.2ms\n",
            "Speed: 4.9ms preprocess, 260.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f56.jpg: 384x640 4 persons, 239.3ms\n",
            "Speed: 4.2ms preprocess, 239.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f64.jpg: 384x640 4 persons, 169.5ms\n",
            "Speed: 3.3ms preprocess, 169.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f37.jpg: 384x640 6 persons, 195.9ms\n",
            "Speed: 3.4ms preprocess, 195.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f69.jpg: 384x640 4 persons, 170.9ms\n",
            "Speed: 3.2ms preprocess, 170.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f60.jpg: 384x640 4 persons, 171.5ms\n",
            "Speed: 3.1ms preprocess, 171.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f53.jpg: 384x640 4 persons, 263.8ms\n",
            "Speed: 5.0ms preprocess, 263.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f63.jpg: 384x640 4 persons, 275.4ms\n",
            "Speed: 4.5ms preprocess, 275.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f51.jpg: 384x640 4 persons, 275.2ms\n",
            "Speed: 4.8ms preprocess, 275.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f52.jpg: 384x640 4 persons, 266.3ms\n",
            "Speed: 4.5ms preprocess, 266.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v19f62.jpg: 384x640 4 persons, 172.3ms\n",
            "Speed: 4.6ms preprocess, 172.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f6.jpg: 384x640 3 persons, 186.4ms\n",
            "Speed: 3.3ms preprocess, 186.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f11.jpg: 384x640 3 persons, 172.0ms\n",
            "Speed: 3.1ms preprocess, 172.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f50.jpg: 384x640 2 persons, 163.3ms\n",
            "Speed: 3.0ms preprocess, 163.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f54.jpg: 384x640 2 persons, 201.7ms\n",
            "Speed: 3.1ms preprocess, 201.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f40.jpg: 384x640 3 persons, 199.1ms\n",
            "Speed: 3.5ms preprocess, 199.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f39.jpg: 384x640 3 persons, 188.1ms\n",
            "Speed: 3.8ms preprocess, 188.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f48.jpg: 384x640 2 persons, 188.9ms\n",
            "Speed: 3.1ms preprocess, 188.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f24.jpg: 384x640 3 persons, 203.6ms\n",
            "Speed: 4.5ms preprocess, 203.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f46.jpg: 384x640 2 persons, 164.1ms\n",
            "Speed: 3.0ms preprocess, 164.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f17.jpg: 384x640 3 persons, 186.1ms\n",
            "Speed: 3.3ms preprocess, 186.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f59.jpg: 384x640 2 persons, 179.7ms\n",
            "Speed: 3.1ms preprocess, 179.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f30.jpg: 384x640 3 persons, 159.1ms\n",
            "Speed: 3.1ms preprocess, 159.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f53.jpg: 384x640 2 persons, 167.1ms\n",
            "Speed: 3.1ms preprocess, 167.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f10.jpg: 384x640 3 persons, 166.6ms\n",
            "Speed: 4.1ms preprocess, 166.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f20.jpg: 384x640 3 persons, 165.4ms\n",
            "Speed: 3.1ms preprocess, 165.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f18.jpg: 384x640 3 persons, 237.1ms\n",
            "Speed: 3.0ms preprocess, 237.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f34.jpg: 384x640 3 persons, 254.3ms\n",
            "Speed: 4.4ms preprocess, 254.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f106.jpg: 384x640 2 persons, 259.0ms\n",
            "Speed: 4.5ms preprocess, 259.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f63.jpg: 384x640 2 persons, 276.0ms\n",
            "Speed: 4.3ms preprocess, 276.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f66.jpg: 384x640 2 persons, 161.0ms\n",
            "Speed: 3.0ms preprocess, 161.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f73.jpg: 384x640 3 persons, 172.1ms\n",
            "Speed: 3.1ms preprocess, 172.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f91.jpg: 384x640 2 persons, 158.5ms\n",
            "Speed: 3.1ms preprocess, 158.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f105.jpg: 384x640 2 persons, 158.2ms\n",
            "Speed: 3.1ms preprocess, 158.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f93.jpg: 384x640 2 persons, 159.8ms\n",
            "Speed: 3.2ms preprocess, 159.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f102.jpg: 384x640 2 persons, 165.2ms\n",
            "Speed: 3.4ms preprocess, 165.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f88.jpg: 384x640 2 persons, 161.3ms\n",
            "Speed: 3.0ms preprocess, 161.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f100.jpg: 384x640 2 persons, 154.3ms\n",
            "Speed: 3.2ms preprocess, 154.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f112.jpg: 384x640 2 persons, 157.8ms\n",
            "Speed: 2.9ms preprocess, 157.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f113.jpg: 384x640 2 persons, 158.7ms\n",
            "Speed: 3.0ms preprocess, 158.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f108.jpg: 384x640 2 persons, 160.0ms\n",
            "Speed: 3.3ms preprocess, 160.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f96.jpg: 384x640 2 persons, 175.5ms\n",
            "Speed: 3.5ms preprocess, 175.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f110.jpg: 384x640 2 persons, 163.9ms\n",
            "Speed: 3.0ms preprocess, 163.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f67.jpg: 384x640 2 persons, 169.5ms\n",
            "Speed: 3.0ms preprocess, 169.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f117.jpg: 384x640 2 persons, 159.2ms\n",
            "Speed: 3.1ms preprocess, 159.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f62.jpg: 384x640 2 persons, 160.6ms\n",
            "Speed: 3.1ms preprocess, 160.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f68.jpg: 384x640 2 persons, 266.1ms\n",
            "Speed: 4.3ms preprocess, 266.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f104.jpg: 384x640 3 persons, 251.8ms\n",
            "Speed: 3.5ms preprocess, 251.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f85.jpg: 384x640 2 persons, 238.7ms\n",
            "Speed: 4.4ms preprocess, 238.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f121.jpg: 384x640 2 persons, 255.7ms\n",
            "Speed: 4.4ms preprocess, 255.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f142.jpg: 384x640 3 persons, 164.2ms\n",
            "Speed: 3.1ms preprocess, 164.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f128.jpg: 384x640 2 persons, 158.1ms\n",
            "Speed: 3.0ms preprocess, 158.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f14.jpg: 384x640 3 persons, 155.9ms\n",
            "Speed: 3.0ms preprocess, 155.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f118.jpg: 384x640 2 persons, 163.8ms\n",
            "Speed: 3.0ms preprocess, 163.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f133.jpg: 384x640 2 persons, 160.0ms\n",
            "Speed: 3.0ms preprocess, 160.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f138.jpg: 384x640 3 persons, 160.3ms\n",
            "Speed: 10.3ms preprocess, 160.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f19.jpg: 384x640 4 persons, 183.9ms\n",
            "Speed: 3.3ms preprocess, 183.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f127.jpg: 384x640 2 persons, 167.7ms\n",
            "Speed: 3.1ms preprocess, 167.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f22.jpg: 384x640 3 persons, 163.9ms\n",
            "Speed: 3.1ms preprocess, 163.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f20.jpg: 384x640 3 persons, 186.3ms\n",
            "Speed: 3.2ms preprocess, 186.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f125.jpg: 384x640 2 persons, 156.4ms\n",
            "Speed: 3.0ms preprocess, 156.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f17.jpg: 384x640 3 persons, 160.8ms\n",
            "Speed: 2.9ms preprocess, 160.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f10.jpg: 384x640 3 persons, 161.1ms\n",
            "Speed: 3.0ms preprocess, 161.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f13.jpg: 384x640 3 persons, 184.7ms\n",
            "Speed: 3.1ms preprocess, 184.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f139.jpg: 384x640 3 persons, 223.8ms\n",
            "Speed: 3.1ms preprocess, 223.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f123.jpg: 384x640 2 persons, 255.9ms\n",
            "Speed: 4.7ms preprocess, 255.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v37f136.jpg: 384x640 3 persons, 262.7ms\n",
            "Speed: 4.7ms preprocess, 262.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f4.jpg: 384x640 3 persons, 262.1ms\n",
            "Speed: 4.4ms preprocess, 262.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f11.jpg: 384x640 3 persons, 268.5ms\n",
            "Speed: 4.8ms preprocess, 268.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f21.jpg: 384x640 3 persons, 163.5ms\n",
            "Speed: 3.1ms preprocess, 163.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f37.jpg: 384x640 3 persons, 187.8ms\n",
            "Speed: 3.0ms preprocess, 187.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f67.jpg: 384x640 4 persons, 157.8ms\n",
            "Speed: 3.3ms preprocess, 157.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f61.jpg: 384x640 5 persons, 166.0ms\n",
            "Speed: 3.0ms preprocess, 166.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f33.jpg: 384x640 4 persons, 185.0ms\n",
            "Speed: 3.0ms preprocess, 185.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f64.jpg: 384x640 3 persons, 173.3ms\n",
            "Speed: 3.3ms preprocess, 173.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f44.jpg: 384x640 4 persons, 159.5ms\n",
            "Speed: 3.0ms preprocess, 159.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f63.jpg: 384x640 4 persons, 174.3ms\n",
            "Speed: 3.2ms preprocess, 174.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f48.jpg: 384x640 4 persons, 160.5ms\n",
            "Speed: 4.4ms preprocess, 160.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f68.jpg: 384x640 3 persons, 167.8ms\n",
            "Speed: 3.1ms preprocess, 167.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f26.jpg: 384x640 3 persons, 157.5ms\n",
            "Speed: 4.0ms preprocess, 157.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f24.jpg: 384x640 4 persons, 163.0ms\n",
            "Speed: 3.1ms preprocess, 163.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f46.jpg: 384x640 4 persons, 163.8ms\n",
            "Speed: 4.6ms preprocess, 163.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f55.jpg: 384x640 3 persons, 168.9ms\n",
            "Speed: 3.0ms preprocess, 168.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f79.jpg: 384x640 3 persons, 163.0ms\n",
            "Speed: 3.2ms preprocess, 163.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f36.jpg: 384x640 3 persons, 260.6ms\n",
            "Speed: 4.5ms preprocess, 260.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f57.jpg: 384x640 3 persons, 264.3ms\n",
            "Speed: 5.3ms preprocess, 264.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f31.jpg: 384x640 4 persons, 248.0ms\n",
            "Speed: 4.4ms preprocess, 248.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f74.jpg: 384x640 3 persons, 171.1ms\n",
            "Speed: 4.1ms preprocess, 171.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f34.jpg: 384x640 4 persons, 191.6ms\n",
            "Speed: 3.2ms preprocess, 191.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f72.jpg: 384x640 4 persons, 167.1ms\n",
            "Speed: 3.1ms preprocess, 167.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f71.jpg: 384x640 3 persons, 169.5ms\n",
            "Speed: 3.4ms preprocess, 169.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f95.jpg: 384x640 3 persons, 168.3ms\n",
            "Speed: 3.1ms preprocess, 168.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f85.jpg: 384x640 3 persons, 184.3ms\n",
            "Speed: 3.1ms preprocess, 184.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f89.jpg: 384x640 3 persons, 170.1ms\n",
            "Speed: 3.2ms preprocess, 170.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f102.jpg: 384x640 4 persons, 157.5ms\n",
            "Speed: 3.0ms preprocess, 157.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f104.jpg: 384x640 4 persons, 158.8ms\n",
            "Speed: 3.2ms preprocess, 158.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f105.jpg: 384x640 4 persons, 164.0ms\n",
            "Speed: 3.6ms preprocess, 164.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f92.jpg: 384x640 3 persons, 159.8ms\n",
            "Speed: 3.1ms preprocess, 159.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f112.jpg: 384x640 4 persons, 184.7ms\n",
            "Speed: 3.3ms preprocess, 184.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f120.jpg: 384x640 3 persons, 158.0ms\n",
            "Speed: 3.0ms preprocess, 158.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f113.jpg: 384x640 3 persons, 191.0ms\n",
            "Speed: 3.0ms preprocess, 191.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f93.jpg: 384x640 3 persons, 270.8ms\n",
            "Speed: 5.1ms preprocess, 270.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f115.jpg: 384x640 4 persons, 247.4ms\n",
            "Speed: 4.2ms preprocess, 247.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f114.jpg: 384x640 4 persons, 245.1ms\n",
            "Speed: 4.5ms preprocess, 245.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f109.jpg: 384x640 3 persons, 176.9ms\n",
            "Speed: 4.4ms preprocess, 176.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f110.jpg: 384x640 3 persons, 164.9ms\n",
            "Speed: 3.1ms preprocess, 164.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f91.jpg: 384x640 3 persons, 173.0ms\n",
            "Speed: 3.1ms preprocess, 173.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f160.jpg: 384x640 3 persons, 185.4ms\n",
            "Speed: 3.6ms preprocess, 185.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f171.jpg: 384x640 3 persons, 176.0ms\n",
            "Speed: 3.1ms preprocess, 176.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f156.jpg: 384x640 3 persons, 163.4ms\n",
            "Speed: 3.0ms preprocess, 163.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f124.jpg: 384x640 3 persons, 162.8ms\n",
            "Speed: 3.0ms preprocess, 162.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f164.jpg: 384x640 3 persons, 172.1ms\n",
            "Speed: 3.1ms preprocess, 172.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f149.jpg: 384x640 3 persons, 165.8ms\n",
            "Speed: 3.4ms preprocess, 165.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f127.jpg: 384x640 3 persons, 159.1ms\n",
            "Speed: 3.2ms preprocess, 159.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f141.jpg: 384x640 3 persons, 186.4ms\n",
            "Speed: 3.1ms preprocess, 186.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f135.jpg: 384x640 3 persons, 185.5ms\n",
            "Speed: 3.0ms preprocess, 185.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f162.jpg: 384x640 4 persons, 156.3ms\n",
            "Speed: 3.0ms preprocess, 156.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f143.jpg: 384x640 3 persons, 178.2ms\n",
            "Speed: 3.0ms preprocess, 178.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f147.jpg: 384x640 3 persons, 269.7ms\n",
            "Speed: 4.6ms preprocess, 269.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f145.jpg: 384x640 3 persons, 249.7ms\n",
            "Speed: 4.0ms preprocess, 249.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f167.jpg: 384x640 4 persons, 246.0ms\n",
            "Speed: 5.0ms preprocess, 246.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f133.jpg: 384x640 3 persons, 211.1ms\n",
            "Speed: 4.5ms preprocess, 211.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f130.jpg: 384x640 3 persons, 160.1ms\n",
            "Speed: 3.0ms preprocess, 160.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f159.jpg: 384x640 3 persons, 156.0ms\n",
            "Speed: 3.0ms preprocess, 156.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f152.jpg: 384x640 3 persons, 163.6ms\n",
            "Speed: 3.0ms preprocess, 163.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f166.jpg: 384x640 3 persons, 157.3ms\n",
            "Speed: 3.1ms preprocess, 157.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f22.jpg: 384x640 2 persons, 181.0ms\n",
            "Speed: 3.1ms preprocess, 181.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f6.jpg: 384x640 2 persons, 161.7ms\n",
            "Speed: 3.1ms preprocess, 161.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f27.jpg: 384x640 2 persons, 158.5ms\n",
            "Speed: 3.5ms preprocess, 158.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f2.jpg: 384x640 2 persons, 192.4ms\n",
            "Speed: 3.5ms preprocess, 192.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f40.jpg: 384x640 1 person, 160.2ms\n",
            "Speed: 3.1ms preprocess, 160.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f39.jpg: 384x640 1 person, 172.4ms\n",
            "Speed: 3.1ms preprocess, 172.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f24.jpg: 384x640 2 persons, 181.4ms\n",
            "Speed: 4.4ms preprocess, 181.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f26.jpg: 384x640 3 persons, 159.4ms\n",
            "Speed: 3.1ms preprocess, 159.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f175.jpg: 384x640 3 persons, 182.2ms\n",
            "Speed: 3.9ms preprocess, 182.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f181.jpg: 384x640 3 persons, 250.3ms\n",
            "Speed: 4.4ms preprocess, 250.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f17.jpg: 384x640 2 persons, 246.2ms\n",
            "Speed: 4.4ms preprocess, 246.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f8.jpg: 384x640 2 persons, 238.0ms\n",
            "Speed: 4.6ms preprocess, 238.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f29.jpg: 384x640 2 persons, 168.0ms\n",
            "Speed: 4.6ms preprocess, 168.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f1.jpg: 384x640 2 persons, 162.5ms\n",
            "Speed: 3.1ms preprocess, 162.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v54f185.jpg: 384x640 3 persons, 156.5ms\n",
            "Speed: 2.9ms preprocess, 156.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f33.jpg: 384x640 2 persons, 181.2ms\n",
            "Speed: 3.1ms preprocess, 181.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f61.jpg: 384x640 1 person, 179.7ms\n",
            "Speed: 3.0ms preprocess, 179.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f58.jpg: 384x640 1 person, 155.0ms\n",
            "Speed: 3.1ms preprocess, 155.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f64.jpg: 384x640 1 person, 160.6ms\n",
            "Speed: 4.3ms preprocess, 160.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f48.jpg: 384x640 1 person, 161.8ms\n",
            "Speed: 3.0ms preprocess, 161.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f50.jpg: 384x640 1 person, 195.6ms\n",
            "Speed: 3.1ms preprocess, 195.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f65.jpg: 384x640 1 person, 184.3ms\n",
            "Speed: 3.0ms preprocess, 184.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f49.jpg: 384x640 1 person, 157.3ms\n",
            "Speed: 3.1ms preprocess, 157.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f91.jpg: 384x640 1 person, 159.2ms\n",
            "Speed: 3.1ms preprocess, 159.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f62.jpg: 384x640 1 person, 158.8ms\n",
            "Speed: 3.6ms preprocess, 158.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f86.jpg: 384x640 1 person, 160.9ms\n",
            "Speed: 3.0ms preprocess, 160.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f81.jpg: 384x640 1 person, 161.3ms\n",
            "Speed: 3.3ms preprocess, 161.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f53.jpg: 384x640 1 person, 247.5ms\n",
            "Speed: 4.4ms preprocess, 247.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f83.jpg: 384x640 1 person, 257.0ms\n",
            "Speed: 4.5ms preprocess, 257.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f45.jpg: 384x640 1 person, 254.5ms\n",
            "Speed: 4.4ms preprocess, 254.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f57.jpg: 384x640 1 person, 161.4ms\n",
            "Speed: 3.1ms preprocess, 161.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f75.jpg: 384x640 1 person, 169.2ms\n",
            "Speed: 3.0ms preprocess, 169.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f89.jpg: 384x640 1 person, 158.9ms\n",
            "Speed: 3.1ms preprocess, 158.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f121.jpg: 384x640 2 persons, 166.3ms\n",
            "Speed: 3.1ms preprocess, 166.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f129.jpg: 384x640 2 persons, 157.3ms\n",
            "Speed: 3.2ms preprocess, 157.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f104.jpg: 384x640 2 persons, 159.6ms\n",
            "Speed: 3.1ms preprocess, 159.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f115.jpg: 384x640 2 persons, 170.9ms\n",
            "Speed: 4.4ms preprocess, 170.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f113.jpg: 384x640 2 persons, 161.2ms\n",
            "Speed: 3.0ms preprocess, 161.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f132.jpg: 384x640 2 persons, 154.0ms\n",
            "Speed: 3.1ms preprocess, 154.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f130.jpg: 384x640 2 persons, 161.6ms\n",
            "Speed: 3.3ms preprocess, 161.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f131.jpg: 384x640 2 persons, 179.3ms\n",
            "Speed: 3.1ms preprocess, 179.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f97.jpg: 384x640 1 person, 153.7ms\n",
            "Speed: 3.0ms preprocess, 153.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f125.jpg: 384x640 2 persons, 156.3ms\n",
            "Speed: 3.0ms preprocess, 156.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f112.jpg: 384x640 2 persons, 159.7ms\n",
            "Speed: 3.0ms preprocess, 159.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f128.jpg: 384x640 2 persons, 248.6ms\n",
            "Speed: 4.6ms preprocess, 248.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f127.jpg: 384x640 2 persons, 244.8ms\n",
            "Speed: 4.5ms preprocess, 244.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f107.jpg: 384x640 3 persons, 249.4ms\n",
            "Speed: 4.5ms preprocess, 249.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f110.jpg: 384x640 2 persons, 161.5ms\n",
            "Speed: 2.9ms preprocess, 161.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f139.jpg: 384x640 2 persons, 160.1ms\n",
            "Speed: 3.1ms preprocess, 160.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f118.jpg: 384x640 2 persons, 156.6ms\n",
            "Speed: 3.0ms preprocess, 156.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f103.jpg: 384x640 2 persons, 171.2ms\n",
            "Speed: 3.4ms preprocess, 171.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f96.jpg: 384x640 2 persons, 165.8ms\n",
            "Speed: 2.9ms preprocess, 165.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f0.jpg: 384x640 2 persons, 180.3ms\n",
            "Speed: 3.1ms preprocess, 180.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f17.jpg: 384x640 2 persons, 158.2ms\n",
            "Speed: 3.2ms preprocess, 158.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f20.jpg: 384x640 2 persons, 157.5ms\n",
            "Speed: 3.1ms preprocess, 157.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f9.jpg: 384x640 2 persons, 167.8ms\n",
            "Speed: 3.0ms preprocess, 167.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f8.jpg: 384x640 2 persons, 164.5ms\n",
            "Speed: 3.0ms preprocess, 164.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f11.jpg: 384x640 2 persons, 158.1ms\n",
            "Speed: 3.0ms preprocess, 158.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f32.jpg: 384x640 2 persons, 153.8ms\n",
            "Speed: 3.0ms preprocess, 153.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f23.jpg: 384x640 2 persons, 159.0ms\n",
            "Speed: 4.5ms preprocess, 159.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f7.jpg: 384x640 2 persons, 167.2ms\n",
            "Speed: 2.9ms preprocess, 167.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f13.jpg: 384x640 2 persons, 185.0ms\n",
            "Speed: 3.0ms preprocess, 185.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f24.jpg: 384x640 3 persons, 260.1ms\n",
            "Speed: 4.5ms preprocess, 260.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f12.jpg: 384x640 2 persons, 254.6ms\n",
            "Speed: 4.5ms preprocess, 254.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f31.jpg: 384x640 2 persons, 236.7ms\n",
            "Speed: 4.4ms preprocess, 236.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f21.jpg: 384x640 2 persons, 166.7ms\n",
            "Speed: 4.4ms preprocess, 166.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f25.jpg: 384x640 2 persons, 156.6ms\n",
            "Speed: 3.1ms preprocess, 156.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f5.jpg: 384x640 2 persons, 170.0ms\n",
            "Speed: 3.3ms preprocess, 170.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f16.jpg: 384x640 2 persons, 176.0ms\n",
            "Speed: 4.4ms preprocess, 176.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f6.jpg: 384x640 2 persons, 158.7ms\n",
            "Speed: 3.2ms preprocess, 158.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v28f140.jpg: 384x640 2 persons, 186.2ms\n",
            "Speed: 3.1ms preprocess, 186.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f3.jpg: 384x640 2 persons, 161.2ms\n",
            "Speed: 2.9ms preprocess, 161.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f26.jpg: 384x640 2 persons, 160.0ms\n",
            "Speed: 3.0ms preprocess, 160.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f2.jpg: 384x640 3 persons, 167.5ms\n",
            "Speed: 4.3ms preprocess, 167.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f37.jpg: 384x640 3 persons, 160.6ms\n",
            "Speed: 3.0ms preprocess, 160.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f52.jpg: 384x640 1 person, 161.7ms\n",
            "Speed: 3.1ms preprocess, 161.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f77.jpg: 384x640 2 persons, 166.8ms\n",
            "Speed: 3.0ms preprocess, 166.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f62.jpg: 384x640 1 person, 161.0ms\n",
            "Speed: 3.0ms preprocess, 161.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f45.jpg: 384x640 1 person, 172.5ms\n",
            "Speed: 4.3ms preprocess, 172.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f34.jpg: 384x640 2 persons, 254.7ms\n",
            "Speed: 5.2ms preprocess, 254.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f75.jpg: 384x640 2 persons, 248.8ms\n",
            "Speed: 4.3ms preprocess, 248.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f69.jpg: 384x640 1 person, 260.5ms\n",
            "Speed: 4.7ms preprocess, 260.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f50.jpg: 384x640 1 person, 245.2ms\n",
            "Speed: 5.0ms preprocess, 245.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f63.jpg: 384x640 1 person, 186.5ms\n",
            "Speed: 3.0ms preprocess, 186.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f66.jpg: 384x640 1 person, 159.5ms\n",
            "Speed: 2.9ms preprocess, 159.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f38.jpg: 384x640 3 persons, 165.6ms\n",
            "Speed: 3.0ms preprocess, 165.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f61.jpg: 384x640 1 person, 158.2ms\n",
            "Speed: 3.1ms preprocess, 158.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f72.jpg: 384x640 1 person, 186.9ms\n",
            "Speed: 3.0ms preprocess, 186.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f64.jpg: 384x640 1 person, 179.6ms\n",
            "Speed: 3.2ms preprocess, 179.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f44.jpg: 384x640 1 person, 171.1ms\n",
            "Speed: 3.0ms preprocess, 171.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f53.jpg: 384x640 1 person, 186.6ms\n",
            "Speed: 3.3ms preprocess, 186.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f35.jpg: 384x640 3 persons, 158.5ms\n",
            "Speed: 3.0ms preprocess, 158.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f65.jpg: 384x640 1 person, 161.1ms\n",
            "Speed: 3.7ms preprocess, 161.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f94.jpg: 384x640 2 persons, 175.2ms\n",
            "Speed: 3.5ms preprocess, 175.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f98.jpg: 384x640 2 persons, 155.3ms\n",
            "Speed: 3.0ms preprocess, 155.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f117.jpg: 384x640 2 persons, 158.8ms\n",
            "Speed: 3.0ms preprocess, 158.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f115.jpg: 384x640 2 persons, 159.0ms\n",
            "Speed: 3.4ms preprocess, 159.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f114.jpg: 384x640 1 person, 194.2ms\n",
            "Speed: 3.1ms preprocess, 194.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f124.jpg: 384x640 3 persons, 269.3ms\n",
            "Speed: 6.7ms preprocess, 269.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f82.jpg: 384x640 2 persons, 261.3ms\n",
            "Speed: 4.5ms preprocess, 261.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f92.jpg: 384x640 2 persons, 251.4ms\n",
            "Speed: 4.5ms preprocess, 251.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f102.jpg: 384x640 2 persons, 162.7ms\n",
            "Speed: 3.4ms preprocess, 162.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f87.jpg: 384x640 2 persons, 178.3ms\n",
            "Speed: 3.1ms preprocess, 178.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f91.jpg: 384x640 2 persons, 164.6ms\n",
            "Speed: 3.1ms preprocess, 164.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f123.jpg: 384x640 2 persons, 183.4ms\n",
            "Speed: 3.5ms preprocess, 183.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f110.jpg: 384x640 1 person, 158.2ms\n",
            "Speed: 3.2ms preprocess, 158.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f119.jpg: 384x640 3 persons, 165.8ms\n",
            "Speed: 3.4ms preprocess, 165.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f118.jpg: 384x640 2 persons, 163.3ms\n",
            "Speed: 3.0ms preprocess, 163.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f127.jpg: 384x640 2 persons, 186.0ms\n",
            "Speed: 3.3ms preprocess, 186.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f104.jpg: 384x640 3 persons, 167.3ms\n",
            "Speed: 3.4ms preprocess, 167.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f101.jpg: 384x640 1 person, 172.5ms\n",
            "Speed: 3.2ms preprocess, 172.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f144.jpg: 384x640 2 persons, 165.1ms\n",
            "Speed: 3.2ms preprocess, 165.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f4.jpg: 384x640 2 persons, 164.7ms\n",
            "Speed: 3.3ms preprocess, 164.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f10.jpg: 384x640 2 persons, 180.8ms\n",
            "Speed: 3.4ms preprocess, 180.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f7.jpg: 384x640 2 persons, 183.0ms\n",
            "Speed: 3.6ms preprocess, 183.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f141.jpg: 384x640 2 persons, 170.5ms\n",
            "Speed: 4.6ms preprocess, 170.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f145.jpg: 384x640 2 persons, 309.8ms\n",
            "Speed: 4.5ms preprocess, 309.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f130.jpg: 384x640 3 persons, 247.8ms\n",
            "Speed: 4.8ms preprocess, 247.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f28.jpg: 384x640 2 persons, 249.4ms\n",
            "Speed: 5.2ms preprocess, 249.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f13.jpg: 384x640 2 persons, 200.3ms\n",
            "Speed: 4.4ms preprocess, 200.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f131.jpg: 384x640 2 persons, 183.9ms\n",
            "Speed: 3.0ms preprocess, 183.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f148.jpg: 384x640 2 persons, 157.6ms\n",
            "Speed: 3.0ms preprocess, 157.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f19.jpg: 384x640 2 persons, 161.0ms\n",
            "Speed: 3.7ms preprocess, 161.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f5.jpg: 384x640 2 persons, 187.1ms\n",
            "Speed: 3.4ms preprocess, 187.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f139.jpg: 384x640 3 persons, 174.6ms\n",
            "Speed: 3.3ms preprocess, 174.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f9.jpg: 384x640 2 persons, 167.4ms\n",
            "Speed: 3.0ms preprocess, 167.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v49f128.jpg: 384x640 2 persons, 167.1ms\n",
            "Speed: 3.0ms preprocess, 167.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f69.jpg: 384x640 2 persons, 156.8ms\n",
            "Speed: 3.1ms preprocess, 156.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f67.jpg: 384x640 2 persons, 166.1ms\n",
            "Speed: 3.6ms preprocess, 166.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f56.jpg: 384x640 2 persons, 164.6ms\n",
            "Speed: 3.1ms preprocess, 164.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f34.jpg: 384x640 2 persons, 177.2ms\n",
            "Speed: 3.1ms preprocess, 177.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f72.jpg: 384x640 2 persons, 160.6ms\n",
            "Speed: 3.0ms preprocess, 160.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f63.jpg: 384x640 2 persons, 164.8ms\n",
            "Speed: 3.0ms preprocess, 164.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f29.jpg: 384x640 2 persons, 178.7ms\n",
            "Speed: 3.2ms preprocess, 178.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f51.jpg: 384x640 2 persons, 160.0ms\n",
            "Speed: 3.0ms preprocess, 160.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f53.jpg: 384x640 2 persons, 256.6ms\n",
            "Speed: 4.7ms preprocess, 256.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f45.jpg: 384x640 2 persons, 258.2ms\n",
            "Speed: 4.4ms preprocess, 258.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f68.jpg: 384x640 2 persons, 242.8ms\n",
            "Speed: 4.5ms preprocess, 242.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f46.jpg: 384x640 2 persons, 213.5ms\n",
            "Speed: 4.3ms preprocess, 213.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f57.jpg: 384x640 2 persons, 158.9ms\n",
            "Speed: 3.3ms preprocess, 158.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f43.jpg: 384x640 2 persons, 179.7ms\n",
            "Speed: 3.2ms preprocess, 179.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f30.jpg: 384x640 2 persons, 168.5ms\n",
            "Speed: 3.3ms preprocess, 168.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f32.jpg: 384x640 2 persons, 176.2ms\n",
            "Speed: 3.1ms preprocess, 176.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f71.jpg: 384x640 2 persons, 151.3ms\n",
            "Speed: 3.1ms preprocess, 151.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f60.jpg: 384x640 2 persons, 173.0ms\n",
            "Speed: 3.1ms preprocess, 173.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f66.jpg: 384x640 2 persons, 157.2ms\n",
            "Speed: 3.0ms preprocess, 157.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f92.jpg: 384x640 2 persons, 169.6ms\n",
            "Speed: 3.2ms preprocess, 169.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f93.jpg: 384x640 2 persons, 174.9ms\n",
            "Speed: 3.4ms preprocess, 174.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f80.jpg: 384x640 2 persons, 162.0ms\n",
            "Speed: 3.0ms preprocess, 162.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f115.jpg: 384x640 2 persons, 182.9ms\n",
            "Speed: 3.0ms preprocess, 182.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f74.jpg: 384x640 2 persons, 163.7ms\n",
            "Speed: 3.1ms preprocess, 163.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f102.jpg: 384x640 2 persons, 178.0ms\n",
            "Speed: 3.0ms preprocess, 178.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f110.jpg: 384x640 2 persons, 168.6ms\n",
            "Speed: 3.3ms preprocess, 168.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f107.jpg: 384x640 2 persons, 173.8ms\n",
            "Speed: 4.2ms preprocess, 173.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f83.jpg: 384x640 2 persons, 180.4ms\n",
            "Speed: 3.0ms preprocess, 180.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f87.jpg: 384x640 2 persons, 253.4ms\n",
            "Speed: 4.5ms preprocess, 253.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f119.jpg: 384x640 2 persons, 242.9ms\n",
            "Speed: 4.5ms preprocess, 242.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f118.jpg: 384x640 2 persons, 262.4ms\n",
            "Speed: 4.4ms preprocess, 262.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f97.jpg: 384x640 2 persons, 253.7ms\n",
            "Speed: 4.9ms preprocess, 253.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f96.jpg: 384x640 2 persons, 199.2ms\n",
            "Speed: 4.2ms preprocess, 199.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f84.jpg: 384x640 2 persons, 158.4ms\n",
            "Speed: 3.1ms preprocess, 158.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f106.jpg: 384x640 2 persons, 157.9ms\n",
            "Speed: 3.0ms preprocess, 157.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f154.jpg: 384x640 3 persons, 168.3ms\n",
            "Speed: 3.1ms preprocess, 168.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f0.jpg: 384x640 4 persons, 154.3ms\n",
            "Speed: 3.0ms preprocess, 154.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f10.jpg: 384x640 4 persons, 159.1ms\n",
            "Speed: 3.1ms preprocess, 159.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f138.jpg: 384x640 3 persons, 158.1ms\n",
            "Speed: 3.0ms preprocess, 158.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f133.jpg: 384x640 3 persons, 163.3ms\n",
            "Speed: 3.2ms preprocess, 163.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f144.jpg: 384x640 3 persons, 161.8ms\n",
            "Speed: 3.0ms preprocess, 161.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f147.jpg: 384x640 3 persons, 163.6ms\n",
            "Speed: 3.0ms preprocess, 163.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f3.jpg: 384x640 4 persons, 209.9ms\n",
            "Speed: 3.1ms preprocess, 209.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f128.jpg: 384x640 4 persons, 154.1ms\n",
            "Speed: 3.1ms preprocess, 154.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f160.jpg: 384x640 3 persons, 162.9ms\n",
            "Speed: 3.3ms preprocess, 162.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f163.jpg: 384x640 3 persons, 180.8ms\n",
            "Speed: 3.0ms preprocess, 180.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f161.jpg: 384x640 3 persons, 282.8ms\n",
            "Speed: 4.5ms preprocess, 282.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f121.jpg: 384x640 2 persons, 243.4ms\n",
            "Speed: 4.5ms preprocess, 243.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f145.jpg: 384x640 3 persons, 241.5ms\n",
            "Speed: 4.4ms preprocess, 241.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f125.jpg: 384x640 3 persons, 271.7ms\n",
            "Speed: 4.6ms preprocess, 271.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f152.jpg: 384x640 3 persons, 168.8ms\n",
            "Speed: 3.1ms preprocess, 168.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f157.jpg: 384x640 3 persons, 159.7ms\n",
            "Speed: 3.2ms preprocess, 159.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f148.jpg: 384x640 3 persons, 165.2ms\n",
            "Speed: 3.1ms preprocess, 165.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f137.jpg: 384x640 3 persons, 157.0ms\n",
            "Speed: 3.0ms preprocess, 157.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v24f129.jpg: 384x640 3 persons, 163.4ms\n",
            "Speed: 3.1ms preprocess, 163.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f30.jpg: 384x640 4 persons, 158.6ms\n",
            "Speed: 3.1ms preprocess, 158.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f29.jpg: 384x640 4 persons, 156.8ms\n",
            "Speed: 3.3ms preprocess, 156.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f43.jpg: 384x640 4 persons, 175.2ms\n",
            "Speed: 3.0ms preprocess, 175.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f12.jpg: 384x640 4 persons, 171.2ms\n",
            "Speed: 3.1ms preprocess, 171.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f39.jpg: 384x640 5 persons, 173.9ms\n",
            "Speed: 3.0ms preprocess, 173.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f44.jpg: 384x640 4 persons, 152.7ms\n",
            "Speed: 3.0ms preprocess, 152.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f36.jpg: 384x640 5 persons, 157.6ms\n",
            "Speed: 3.0ms preprocess, 157.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f60.jpg: 384x640 5 persons, 156.7ms\n",
            "Speed: 2.9ms preprocess, 156.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f56.jpg: 384x640 5 persons, 159.1ms\n",
            "Speed: 3.6ms preprocess, 159.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f11.jpg: 384x640 4 persons, 163.5ms\n",
            "Speed: 3.4ms preprocess, 163.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f38.jpg: 384x640 5 persons, 157.4ms\n",
            "Speed: 2.9ms preprocess, 157.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f54.jpg: 384x640 4 persons, 239.3ms\n",
            "Speed: 4.7ms preprocess, 239.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f20.jpg: 384x640 4 persons, 248.7ms\n",
            "Speed: 4.2ms preprocess, 248.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f105.jpg: 384x640 3 persons, 257.0ms\n",
            "Speed: 6.1ms preprocess, 257.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f76.jpg: 384x640 5 persons, 162.0ms\n",
            "Speed: 3.4ms preprocess, 162.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f100.jpg: 384x640 4 persons, 169.9ms\n",
            "Speed: 3.0ms preprocess, 169.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f94.jpg: 384x640 5 persons, 194.2ms\n",
            "Speed: 3.0ms preprocess, 194.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f106.jpg: 384x640 3 persons, 177.7ms\n",
            "Speed: 3.1ms preprocess, 177.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f90.jpg: 384x640 4 persons, 169.8ms\n",
            "Speed: 3.1ms preprocess, 169.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f99.jpg: 384x640 4 persons, 186.5ms\n",
            "Speed: 3.7ms preprocess, 186.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f86.jpg: 384x640 3 persons, 173.0ms\n",
            "Speed: 3.3ms preprocess, 173.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f69.jpg: 384x640 6 persons, 158.2ms\n",
            "Speed: 3.2ms preprocess, 158.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f87.jpg: 384x640 4 persons, 179.6ms\n",
            "Speed: 4.2ms preprocess, 179.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f82.jpg: 384x640 3 persons, 157.2ms\n",
            "Speed: 3.2ms preprocess, 157.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f102.jpg: 384x640 4 persons, 160.6ms\n",
            "Speed: 3.3ms preprocess, 160.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f70.jpg: 384x640 4 persons, 170.6ms\n",
            "Speed: 4.6ms preprocess, 170.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f75.jpg: 384x640 4 persons, 158.3ms\n",
            "Speed: 3.1ms preprocess, 158.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f67.jpg: 384x640 6 persons, 174.3ms\n",
            "Speed: 3.0ms preprocess, 174.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f65.jpg: 384x640 5 persons, 151.7ms\n",
            "Speed: 3.0ms preprocess, 151.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f92.jpg: 384x640 5 persons, 168.9ms\n",
            "Speed: 4.5ms preprocess, 168.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f18.jpg: 384x640 3 persons, 252.9ms\n",
            "Speed: 4.3ms preprocess, 252.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f26.jpg: 384x640 4 persons, 248.5ms\n",
            "Speed: 4.7ms preprocess, 248.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f20.jpg: 384x640 3 persons, 254.1ms\n",
            "Speed: 3.7ms preprocess, 254.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f12.jpg: 384x640 3 persons, 155.9ms\n",
            "Speed: 3.0ms preprocess, 155.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f108.jpg: 384x640 3 persons, 154.8ms\n",
            "Speed: 2.9ms preprocess, 154.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f29.jpg: 384x640 3 persons, 153.3ms\n",
            "Speed: 2.9ms preprocess, 153.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f117.jpg: 384x640 3 persons, 159.7ms\n",
            "Speed: 3.1ms preprocess, 159.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f111.jpg: 384x640 3 persons, 162.1ms\n",
            "Speed: 3.0ms preprocess, 162.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f0.jpg: 384x640 5 persons, 169.9ms\n",
            "Speed: 3.1ms preprocess, 169.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f3.jpg: 384x640 3 persons, 167.9ms\n",
            "Speed: 3.1ms preprocess, 167.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f7.jpg: 384x640 3 persons, 163.0ms\n",
            "Speed: 3.0ms preprocess, 163.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v9f113.jpg: 384x640 3 persons, 159.8ms\n",
            "Speed: 3.3ms preprocess, 159.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f10.jpg: 384x640 3 persons, 152.4ms\n",
            "Speed: 3.0ms preprocess, 152.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f33.jpg: 384x640 4 persons, 175.9ms\n",
            "Speed: 2.9ms preprocess, 175.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f27.jpg: 384x640 4 persons, 155.9ms\n",
            "Speed: 3.0ms preprocess, 155.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f21.jpg: 384x640 3 persons, 159.0ms\n",
            "Speed: 3.0ms preprocess, 159.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f9.jpg: 384x640 3 persons, 157.5ms\n",
            "Speed: 3.1ms preprocess, 157.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f62.jpg: 384x640 3 persons, 162.2ms\n",
            "Speed: 3.0ms preprocess, 162.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f75.jpg: 384x640 3 persons, 256.4ms\n",
            "Speed: 3.9ms preprocess, 256.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f40.jpg: 384x640 3 persons, 268.2ms\n",
            "Speed: 7.0ms preprocess, 268.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f80.jpg: 384x640 3 persons, 242.8ms\n",
            "Speed: 4.2ms preprocess, 242.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f39.jpg: 384x640 3 persons, 261.3ms\n",
            "Speed: 4.3ms preprocess, 261.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f57.jpg: 384x640 3 persons, 172.9ms\n",
            "Speed: 3.3ms preprocess, 172.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f60.jpg: 384x640 3 persons, 169.1ms\n",
            "Speed: 3.2ms preprocess, 169.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f35.jpg: 384x640 4 persons, 167.5ms\n",
            "Speed: 3.7ms preprocess, 167.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f77.jpg: 384x640 3 persons, 159.3ms\n",
            "Speed: 3.2ms preprocess, 159.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f78.jpg: 384x640 3 persons, 154.1ms\n",
            "Speed: 3.0ms preprocess, 154.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f64.jpg: 384x640 3 persons, 182.1ms\n",
            "Speed: 3.3ms preprocess, 182.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f38.jpg: 384x640 3 persons, 154.2ms\n",
            "Speed: 3.0ms preprocess, 154.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f52.jpg: 384x640 3 persons, 168.4ms\n",
            "Speed: 3.1ms preprocess, 168.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f46.jpg: 384x640 3 persons, 160.7ms\n",
            "Speed: 3.2ms preprocess, 160.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f45.jpg: 384x640 3 persons, 161.8ms\n",
            "Speed: 3.4ms preprocess, 161.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f47.jpg: 384x640 3 persons, 158.4ms\n",
            "Speed: 3.7ms preprocess, 158.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f56.jpg: 384x640 3 persons, 161.9ms\n",
            "Speed: 3.6ms preprocess, 161.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f74.jpg: 384x640 3 persons, 157.3ms\n",
            "Speed: 3.0ms preprocess, 157.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f42.jpg: 384x640 3 persons, 219.1ms\n",
            "Speed: 4.3ms preprocess, 219.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f125.jpg: 384x640 3 persons, 251.2ms\n",
            "Speed: 4.5ms preprocess, 251.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f107.jpg: 384x640 3 persons, 251.3ms\n",
            "Speed: 4.6ms preprocess, 251.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f98.jpg: 384x640 3 persons, 245.7ms\n",
            "Speed: 4.4ms preprocess, 245.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f100.jpg: 384x640 3 persons, 181.7ms\n",
            "Speed: 3.0ms preprocess, 181.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f103.jpg: 384x640 3 persons, 153.6ms\n",
            "Speed: 2.9ms preprocess, 153.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f119.jpg: 384x640 3 persons, 164.4ms\n",
            "Speed: 3.0ms preprocess, 164.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f112.jpg: 384x640 3 persons, 174.1ms\n",
            "Speed: 3.4ms preprocess, 174.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f110.jpg: 384x640 3 persons, 157.5ms\n",
            "Speed: 3.0ms preprocess, 157.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f89.jpg: 384x640 3 persons, 158.6ms\n",
            "Speed: 3.4ms preprocess, 158.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f120.jpg: 384x640 3 persons, 158.0ms\n",
            "Speed: 3.0ms preprocess, 158.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f86.jpg: 384x640 3 persons, 168.4ms\n",
            "Speed: 3.1ms preprocess, 168.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f102.jpg: 384x640 3 persons, 181.1ms\n",
            "Speed: 3.1ms preprocess, 181.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f105.jpg: 384x640 3 persons, 179.1ms\n",
            "Speed: 3.3ms preprocess, 179.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f124.jpg: 384x640 3 persons, 161.4ms\n",
            "Speed: 3.0ms preprocess, 161.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f108.jpg: 384x640 3 persons, 160.1ms\n",
            "Speed: 3.0ms preprocess, 160.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f106.jpg: 384x640 3 persons, 157.8ms\n",
            "Speed: 3.0ms preprocess, 157.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f94.jpg: 384x640 3 persons, 252.5ms\n",
            "Speed: 5.3ms preprocess, 252.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f93.jpg: 384x640 3 persons, 254.3ms\n",
            "Speed: 4.5ms preprocess, 254.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f83.jpg: 384x640 3 persons, 248.5ms\n",
            "Speed: 4.5ms preprocess, 248.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f181.jpg: 384x640 3 persons, 254.2ms\n",
            "Speed: 4.3ms preprocess, 254.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f149.jpg: 384x640 3 persons, 159.2ms\n",
            "Speed: 3.7ms preprocess, 159.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f126.jpg: 384x640 3 persons, 177.9ms\n",
            "Speed: 3.0ms preprocess, 177.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f155.jpg: 384x640 3 persons, 174.6ms\n",
            "Speed: 3.0ms preprocess, 174.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f175.jpg: 384x640 3 persons, 177.6ms\n",
            "Speed: 3.6ms preprocess, 177.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f162.jpg: 384x640 3 persons, 192.7ms\n",
            "Speed: 3.0ms preprocess, 192.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f177.jpg: 384x640 3 persons, 153.7ms\n",
            "Speed: 2.8ms preprocess, 153.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f154.jpg: 384x640 3 persons, 169.5ms\n",
            "Speed: 4.0ms preprocess, 169.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f176.jpg: 384x640 3 persons, 162.6ms\n",
            "Speed: 3.0ms preprocess, 162.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f130.jpg: 384x640 3 persons, 171.9ms\n",
            "Speed: 3.3ms preprocess, 171.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f170.jpg: 384x640 3 persons, 160.9ms\n",
            "Speed: 3.1ms preprocess, 160.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f171.jpg: 384x640 3 persons, 162.8ms\n",
            "Speed: 2.9ms preprocess, 162.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f144.jpg: 384x640 3 persons, 165.1ms\n",
            "Speed: 3.3ms preprocess, 165.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f140.jpg: 384x640 3 persons, 160.2ms\n",
            "Speed: 3.1ms preprocess, 160.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f173.jpg: 384x640 3 persons, 173.8ms\n",
            "Speed: 3.1ms preprocess, 173.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f169.jpg: 384x640 3 persons, 273.5ms\n",
            "Speed: 7.2ms preprocess, 273.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f138.jpg: 384x640 3 persons, 257.6ms\n",
            "Speed: 4.7ms preprocess, 257.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f196.jpg: 384x640 3 persons, 253.1ms\n",
            "Speed: 4.5ms preprocess, 253.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f0.jpg: 384x640 3 persons, 292.0ms\n",
            "Speed: 4.4ms preprocess, 292.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f25.jpg: 384x640 3 persons, 243.7ms\n",
            "Speed: 4.5ms preprocess, 243.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f15.jpg: 384x640 3 persons, 156.0ms\n",
            "Speed: 3.4ms preprocess, 156.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f26.jpg: 384x640 3 persons, 160.2ms\n",
            "Speed: 4.4ms preprocess, 160.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f11.jpg: 384x640 3 persons, 156.2ms\n",
            "Speed: 2.9ms preprocess, 156.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f4.jpg: 384x640 3 persons, 158.8ms\n",
            "Speed: 3.0ms preprocess, 158.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f1.jpg: 384x640 3 persons, 159.9ms\n",
            "Speed: 3.6ms preprocess, 159.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f191.jpg: 384x640 3 persons, 154.8ms\n",
            "Speed: 3.0ms preprocess, 154.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f194.jpg: 384x640 3 persons, 160.7ms\n",
            "Speed: 3.1ms preprocess, 160.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f2.jpg: 384x640 3 persons, 157.6ms\n",
            "Speed: 3.0ms preprocess, 157.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f16.jpg: 384x640 3 persons, 163.7ms\n",
            "Speed: 3.0ms preprocess, 163.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f27.jpg: 384x640 3 persons, 177.3ms\n",
            "Speed: 3.1ms preprocess, 177.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f193.jpg: 384x640 3 persons, 155.7ms\n",
            "Speed: 3.6ms preprocess, 155.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f197.jpg: 384x640 3 persons, 168.9ms\n",
            "Speed: 3.0ms preprocess, 168.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f22.jpg: 384x640 3 persons, 158.1ms\n",
            "Speed: 3.1ms preprocess, 158.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v60f189.jpg: 384x640 3 persons, 165.7ms\n",
            "Speed: 4.5ms preprocess, 165.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f7.jpg: 384x640 3 persons, 247.2ms\n",
            "Speed: 4.3ms preprocess, 247.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f35.jpg: 384x640 3 persons, 257.5ms\n",
            "Speed: 4.5ms preprocess, 257.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f32.jpg: 384x640 3 persons, 239.3ms\n",
            "Speed: 4.4ms preprocess, 239.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f60.jpg: 384x640 2 persons, 247.5ms\n",
            "Speed: 4.4ms preprocess, 247.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f71.jpg: 384x640 2 persons, 259.7ms\n",
            "Speed: 4.4ms preprocess, 259.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f54.jpg: 384x640 2 persons, 156.2ms\n",
            "Speed: 3.0ms preprocess, 156.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f38.jpg: 384x640 3 persons, 158.6ms\n",
            "Speed: 3.0ms preprocess, 158.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f63.jpg: 384x640 2 persons, 175.2ms\n",
            "Speed: 3.0ms preprocess, 175.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f61.jpg: 384x640 2 persons, 157.4ms\n",
            "Speed: 3.0ms preprocess, 157.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f45.jpg: 384x640 3 persons, 153.6ms\n",
            "Speed: 2.9ms preprocess, 153.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f64.jpg: 384x640 2 persons, 152.9ms\n",
            "Speed: 2.9ms preprocess, 152.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f40.jpg: 384x640 3 persons, 158.3ms\n",
            "Speed: 3.0ms preprocess, 158.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f77.jpg: 384x640 1 person, 164.9ms\n",
            "Speed: 3.1ms preprocess, 164.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f81.jpg: 384x640 1 person, 156.3ms\n",
            "Speed: 3.0ms preprocess, 156.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f39.jpg: 384x640 3 persons, 152.3ms\n",
            "Speed: 2.9ms preprocess, 152.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f62.jpg: 384x640 2 persons, 184.4ms\n",
            "Speed: 4.4ms preprocess, 184.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f52.jpg: 384x640 2 persons, 165.3ms\n",
            "Speed: 3.4ms preprocess, 165.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f43.jpg: 384x640 3 persons, 158.9ms\n",
            "Speed: 3.1ms preprocess, 158.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f67.jpg: 384x640 2 persons, 254.0ms\n",
            "Speed: 4.6ms preprocess, 254.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f80.jpg: 384x640 1 person, 237.1ms\n",
            "Speed: 4.6ms preprocess, 237.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f82.jpg: 384x640 1 person, 239.5ms\n",
            "Speed: 4.4ms preprocess, 239.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f86.jpg: 384x640 1 person, 235.9ms\n",
            "Speed: 4.5ms preprocess, 235.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f100.jpg: 384x640 1 person, 158.1ms\n",
            "Speed: 3.0ms preprocess, 158.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f121.jpg: 384x640 1 person, 168.0ms\n",
            "Speed: 3.2ms preprocess, 168.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f107.jpg: 384x640 1 person, 166.6ms\n",
            "Speed: 3.0ms preprocess, 166.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f132.jpg: 384x640 1 person, 153.8ms\n",
            "Speed: 3.0ms preprocess, 153.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f99.jpg: 384x640 2 persons, 161.0ms\n",
            "Speed: 2.9ms preprocess, 161.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f89.jpg: 384x640 1 person, 155.2ms\n",
            "Speed: 2.9ms preprocess, 155.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f88.jpg: 384x640 1 person, 178.3ms\n",
            "Speed: 3.5ms preprocess, 178.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f102.jpg: 384x640 1 person, 161.9ms\n",
            "Speed: 3.5ms preprocess, 161.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f133.jpg: 384x640 1 person, 159.3ms\n",
            "Speed: 2.9ms preprocess, 159.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f93.jpg: 384x640 1 person, 169.1ms\n",
            "Speed: 3.0ms preprocess, 169.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f96.jpg: 384x640 2 persons, 155.6ms\n",
            "Speed: 2.9ms preprocess, 155.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f127.jpg: 384x640 1 person, 161.1ms\n",
            "Speed: 3.1ms preprocess, 161.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f130.jpg: 384x640 1 person, 153.3ms\n",
            "Speed: 3.1ms preprocess, 153.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f129.jpg: 384x640 1 person, 267.9ms\n",
            "Speed: 4.4ms preprocess, 267.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f85.jpg: 384x640 1 person, 238.3ms\n",
            "Speed: 6.5ms preprocess, 238.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f173.jpg: 384x640 2 persons, 253.5ms\n",
            "Speed: 4.2ms preprocess, 253.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f159.jpg: 384x640 1 person, 176.3ms\n",
            "Speed: 2.9ms preprocess, 176.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f138.jpg: 384x640 1 person, 152.0ms\n",
            "Speed: 3.0ms preprocess, 152.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f141.jpg: 384x640 2 persons, 157.8ms\n",
            "Speed: 3.0ms preprocess, 157.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f158.jpg: 384x640 1 person, 159.0ms\n",
            "Speed: 3.0ms preprocess, 159.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f169.jpg: 384x640 2 persons, 165.9ms\n",
            "Speed: 3.5ms preprocess, 165.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f162.jpg: 384x640 2 persons, 161.5ms\n",
            "Speed: 3.0ms preprocess, 161.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f177.jpg: 384x640 2 persons, 170.3ms\n",
            "Speed: 3.0ms preprocess, 170.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f160.jpg: 384x640 1 person, 163.1ms\n",
            "Speed: 3.6ms preprocess, 163.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AIRTLab/train/non-violent/v32f135.jpg: 384x640 1 person, 162.7ms\n",
            "Speed: 3.0ms preprocess, 162.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.to_pickle('keyframes_keypoints.pkl')"
      ],
      "metadata": {
        "id": "MPOMEA-RnlZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AfUNvrjP5Q4G",
        "outputId": "5c4c1c33-f96f-443c-9fe5-ccc936bfb693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     filename                                          keypoints        label  \\\n",
              "0   v37f2.jpg  [[[[     1504.3      260.41], [     1521.8    ...  non-violent   \n",
              "1   v37f4.jpg  [[[[     735.88      116.31], [     746.55    ...  non-violent   \n",
              "2   v37f1.jpg  [[[[     1510.4       265.2], [     1526.1    ...  non-violent   \n",
              "3   v37f8.jpg  [[[[     770.74      122.25], [     781.85    ...  non-violent   \n",
              "4  v37f26.jpg  [[[[     1113.5      347.87], [     1126.2    ...  non-violent   \n",
              "\n",
              "  set_split  \n",
              "0      test  \n",
              "1      test  \n",
              "2      test  \n",
              "3      test  \n",
              "4      test  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3eb58db7-551f-4549-97b7-d44f97821315\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>keypoints</th>\n",
              "      <th>label</th>\n",
              "      <th>set_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v37f2.jpg</td>\n",
              "      <td>[[[[     1504.3      260.41], [     1521.8    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v37f4.jpg</td>\n",
              "      <td>[[[[     735.88      116.31], [     746.55    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v37f1.jpg</td>\n",
              "      <td>[[[[     1510.4       265.2], [     1526.1    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v37f8.jpg</td>\n",
              "      <td>[[[[     770.74      122.25], [     781.85    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v37f26.jpg</td>\n",
              "      <td>[[[[     1113.5      347.87], [     1126.2    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3eb58db7-551f-4549-97b7-d44f97821315')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3eb58db7-551f-4549-97b7-d44f97821315 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3eb58db7-551f-4549-97b7-d44f97821315');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f248dfc0-134c-4a0e-9f26-e3180aa4c918\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f248dfc0-134c-4a0e-9f26-e3180aa4c918')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f248dfc0-134c-4a0e-9f26-e3180aa4c918 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4178,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3973,\n        \"samples\": [\n          \"v32f130.jpg\",\n          \"v49f127.jpg\",\n          \"v24f115.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keypoints\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"violent\",\n          \"non-violent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"set_split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"train\",\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def keypoint_statistics_features(keypoints_sequence):\n",
        "    if keypoints_sequence.shape[0] < 1:\n",
        "        return None  # Not enough frames\n",
        "\n",
        "    smoothed_sequence = uniform_filter1d(keypoints_sequence, size=3, axis=0, mode='nearest')\n",
        "\n",
        "    stats = [\n",
        "        np.min(smoothed_sequence, axis=0),\n",
        "        np.max(smoothed_sequence, axis=0),\n",
        "        np.mean(smoothed_sequence, axis=0),\n",
        "        np.std(smoothed_sequence, axis=0),\n",
        "        np.median(smoothed_sequence, axis=0),\n",
        "        np.max(smoothed_sequence, axis=0) - np.min(smoothed_sequence, axis=0),\n",
        "        np.percentile(smoothed_sequence, 25, axis=0),\n",
        "        np.percentile(smoothed_sequence, 75, axis=0),\n",
        "    ]\n",
        "\n",
        "\n",
        "    features = [stat.flatten() for stat in stats]\n",
        "    return np.concatenate(features)\n",
        "\n",
        "def aggregate_video_features2(video_person_skeletons, pooling='mean'):\n",
        "    person_features = []\n",
        "\n",
        "    for keypoints_sequence in video_person_skeletons:\n",
        "        feats = keypoint_statistics_features(np.array(keypoints_sequence))\n",
        "        if feats is not None:\n",
        "            person_features.append(feats)\n",
        "\n",
        "    if not person_features:\n",
        "        return np.zeros(204)\n",
        "\n",
        "    person_features = np.array(person_features)\n",
        "\n",
        "    if pooling == 'mean':\n",
        "        return np.mean(person_features, axis=0)\n",
        "    elif pooling == 'max':\n",
        "        return np.max(person_features, axis=0)\n",
        "    else:\n",
        "        raise ValueError(\"Pooling must be 'mean' or 'max'\")"
      ],
      "metadata": {
        "id": "ypXXb7_Jfjfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle('keyframes_keypoints.pkl')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZH4MvMtcfmj9",
        "outputId": "df8492f8-16cf-45af-9824-9feebd9e1f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     filename                                          keypoints        label  \\\n",
              "0   v37f2.jpg  [[[[     1504.3      260.41], [     1521.8    ...  non-violent   \n",
              "1   v37f4.jpg  [[[[     735.88      116.31], [     746.55    ...  non-violent   \n",
              "2   v37f1.jpg  [[[[     1510.4       265.2], [     1526.1    ...  non-violent   \n",
              "3   v37f8.jpg  [[[[     770.74      122.25], [     781.85    ...  non-violent   \n",
              "4  v37f26.jpg  [[[[     1113.5      347.87], [     1126.2    ...  non-violent   \n",
              "\n",
              "  set_split  \n",
              "0      test  \n",
              "1      test  \n",
              "2      test  \n",
              "3      test  \n",
              "4      test  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26a3db93-d2d8-4238-9313-3271fb8f4943\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>keypoints</th>\n",
              "      <th>label</th>\n",
              "      <th>set_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v37f2.jpg</td>\n",
              "      <td>[[[[     1504.3      260.41], [     1521.8    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v37f4.jpg</td>\n",
              "      <td>[[[[     735.88      116.31], [     746.55    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v37f1.jpg</td>\n",
              "      <td>[[[[     1510.4       265.2], [     1526.1    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v37f8.jpg</td>\n",
              "      <td>[[[[     770.74      122.25], [     781.85    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v37f26.jpg</td>\n",
              "      <td>[[[[     1113.5      347.87], [     1126.2    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26a3db93-d2d8-4238-9313-3271fb8f4943')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-26a3db93-d2d8-4238-9313-3271fb8f4943 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-26a3db93-d2d8-4238-9313-3271fb8f4943');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ee0de763-8aaf-4659-bafe-646f4512dcfc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee0de763-8aaf-4659-bafe-646f4512dcfc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ee0de763-8aaf-4659-bafe-646f4512dcfc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4178,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3973,\n        \"samples\": [\n          \"v32f130.jpg\",\n          \"v49f127.jpg\",\n          \"v24f115.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keypoints\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"violent\",\n          \"non-violent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"set_split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"train\",\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keypoints_list = np.array(df['keypoints'].copy())\n",
        "video_features = []\n",
        "for video in keypoints_list:\n",
        "    # `video` is a dict: {person_id: [skeletons_per_frame]}\n",
        "    fvec = aggregate_video_features2(video, pooling='max')\n",
        "    video_features.append(fvec)"
      ],
      "metadata": {
        "id": "5UPjuXS-f1mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['feature_vectors'] = video_features\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "b84MVAokf4JO",
        "outputId": "67dd2f58-9691-46d3-8cc7-3d50a2dadf1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     filename                                          keypoints        label  \\\n",
              "0   v37f2.jpg  [[[[     1504.3      260.41], [     1521.8    ...  non-violent   \n",
              "1   v37f4.jpg  [[[[     735.88      116.31], [     746.55    ...  non-violent   \n",
              "2   v37f1.jpg  [[[[     1510.4       265.2], [     1526.1    ...  non-violent   \n",
              "3   v37f8.jpg  [[[[     770.74      122.25], [     781.85    ...  non-violent   \n",
              "4  v37f26.jpg  [[[[     1113.5      347.87], [     1126.2    ...  non-violent   \n",
              "\n",
              "  set_split                                    feature_vectors  \n",
              "0      test  [468.48264, 206.82518, 468.3542, 200.43353, 45...  \n",
              "1      test  [728.47546, 165.11365, 731.4533, 159.02248, 72...  \n",
              "2      test  [592.66693, 224.14517, 598.10016, 212.28174, 5...  \n",
              "3      test  [753.60345, 177.83728, 754.611, 169.4323, 747....  \n",
              "4      test  [820.91174, 302.14297, 821.9999, 294.54153, 81...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e16d6f47-a023-457e-b630-ba966f8771b3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>keypoints</th>\n",
              "      <th>label</th>\n",
              "      <th>set_split</th>\n",
              "      <th>feature_vectors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v37f2.jpg</td>\n",
              "      <td>[[[[     1504.3      260.41], [     1521.8    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "      <td>[468.48264, 206.82518, 468.3542, 200.43353, 45...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v37f4.jpg</td>\n",
              "      <td>[[[[     735.88      116.31], [     746.55    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "      <td>[728.47546, 165.11365, 731.4533, 159.02248, 72...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v37f1.jpg</td>\n",
              "      <td>[[[[     1510.4       265.2], [     1526.1    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "      <td>[592.66693, 224.14517, 598.10016, 212.28174, 5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v37f8.jpg</td>\n",
              "      <td>[[[[     770.74      122.25], [     781.85    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "      <td>[753.60345, 177.83728, 754.611, 169.4323, 747....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v37f26.jpg</td>\n",
              "      <td>[[[[     1113.5      347.87], [     1126.2    ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "      <td>[820.91174, 302.14297, 821.9999, 294.54153, 81...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e16d6f47-a023-457e-b630-ba966f8771b3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e16d6f47-a023-457e-b630-ba966f8771b3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e16d6f47-a023-457e-b630-ba966f8771b3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-642b3eb5-d302-4220-ab94-2ec080050634\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-642b3eb5-d302-4220-ab94-2ec080050634')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-642b3eb5-d302-4220-ab94-2ec080050634 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4178,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3973,\n        \"samples\": [\n          \"v32f130.jpg\",\n          \"v49f127.jpg\",\n          \"v24f115.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keypoints\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"violent\",\n          \"non-violent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"set_split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"train\",\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_vectors\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.feature_vectors[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_PWBQaogofO",
        "outputId": "28b675dc-9c72-495d-f903-a65037f8c70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(272,)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fusion Model Training"
      ],
      "metadata": {
        "id": "9G-eR60sAo0v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKyuj9jMikdH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQSCla9jmvHH",
        "outputId": "75ca9bcd-cbee-409a-9537-07f676d5662c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_TEST_PATH = '/content/drive/MyDrive/AIRTLab/test/'\n",
        "\n",
        "DATASET_TRAIN_PATH = '/content/drive/MyDrive/AIRTLab/train/'\n",
        "\n",
        "TARGET_SIZE = (120,160)\n",
        "\n",
        "DESIRED_ACCURACY = 0.995"
      ],
      "metadata": {
        "id": "2lGHsfOSnHB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def violence_test_dataset():\n",
        "    return  image_dataset_from_directory(DATASET_TEST_PATH,\n",
        "                                         batch_size=32,\n",
        "                                         label_mode='categorical',\n",
        "                                         image_size=TARGET_SIZE)\n",
        "\n",
        "def violence_train_dataset():\n",
        "    return  image_dataset_from_directory(DATASET_TRAIN_PATH,\n",
        "                                         batch_size=32,\n",
        "                                         label_mode='categorical',\n",
        "                                         image_size=TARGET_SIZE)"
      ],
      "metadata": {
        "id": "hDv4voOgm7N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_fusion_model(input_shape_video=(120, 160, 3), feature_dim=272, num_classes=2):\n",
        "    # ----- CNN Branch -----\n",
        "    mobilenet_base = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=input_shape_video,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    mobilenet_base.trainable = False\n",
        "\n",
        "    video_input = layers.Input(shape=input_shape_video, name=\"video_input\")\n",
        "    x1 = mobilenet_base(video_input)\n",
        "    x1 = layers.GlobalAveragePooling2D()(x1)\n",
        "\n",
        "    # ----- MLP Branch -----\n",
        "    keypoint_input = layers.Input(shape=(feature_dim,), name=\"keypoint_input\")\n",
        "    x2 = layers.Dense(128, activation=\"relu\")(keypoint_input)\n",
        "    x2 = layers.Dense(64, activation=\"relu\")(x2)\n",
        "\n",
        "    # ----- Fusion -----\n",
        "    combined = layers.Concatenate()([x1, x2])\n",
        "    x = layers.Dense(64, activation=\"relu\")(combined)\n",
        "    output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = models.Model(inputs=[video_input, keypoint_input], outputs=output)\n",
        "\n",
        "    model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gl1dC8a9AtrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fusion_model = build_fusion_model()\n",
        "\n",
        "fusion_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "1THPBc6Yt7hT",
        "outputId": "f934e196-e872-4a79-93de-425003ef2116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2085389589.py:3: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  mobilenet_base = tf.keras.applications.MobileNetV2(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ video_input         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m160\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ keypoint_input      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m272\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_2â€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m5\u001b[0m,      â”‚  \u001b[38;5;34m2,257,984\u001b[0m â”‚ video_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
              "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚ \u001b[38;5;34m1280\u001b[0m)             â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚     \u001b[38;5;34m34,944\u001b[0m â”‚ keypoint_input[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ mobilenetv2_1.00â€¦ â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m8,256\u001b[0m â”‚ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ concatenate_1       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1344\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ global_average_pâ€¦ â”‚\n",
              "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚     \u001b[38;5;34m86,080\u001b[0m â”‚ concatenate_1[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         â”‚        \u001b[38;5;34m130\u001b[0m â”‚ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ video_input         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ keypoint_input      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_2â€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚ video_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">34,944</span> â”‚ keypoint_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ mobilenetv2_1.00â€¦ â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ concatenate_1       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1344</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ global_average_pâ€¦ â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">86,080</span> â”‚ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> â”‚ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,387,394\u001b[0m (9.11 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,387,394</span> (9.11 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m129,410\u001b[0m (505.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">129,410</span> (505.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labels(ds):\n",
        "    labels = []\n",
        "    for _, y in ds:\n",
        "        labels.append(y.numpy())\n",
        "    return np.concatenate(labels)\n",
        "\n",
        "y_train = extract_labels(train_img_ds)\n",
        "y_val   = extract_labels(val_img_ds)\n",
        "\n",
        "\n",
        "def extract_images(ds):\n",
        "    imgs = []\n",
        "    for x, _ in ds:\n",
        "        imgs.append(x.numpy())\n",
        "    return np.concatenate(imgs)\n",
        "\n",
        "x_train_img = extract_images(train_img_ds)\n",
        "x_val_img   = extract_images(val_img_ds)"
      ],
      "metadata": {
        "id": "S28bYGhP0fch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    ((x_train_img, train_keypoints), y_train)\n",
        ").batch(32).shuffle(1000)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    ((x_val_img, val_keypoints), y_val)\n",
        ").batch(32)"
      ],
      "metadata": {
        "id": "P5ah03g1279O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle('keyframes_keypoints.pkl')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DUjn9oAemj81",
        "outputId": "936751b8-98ca-4a12-d803-2f46115e802f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     filename                                          keypoints        label  \\\n",
              "0   v37f2.jpg  [[[[1504.3322  260.4124], [1521.8273   255.220...  non-violent   \n",
              "1   v37f4.jpg  [[[[735.87787  116.309715], [746.5505  110.004...  non-violent   \n",
              "2   v37f1.jpg  [[[[1510.4242   265.19766], [1526.0929  258.19...  non-violent   \n",
              "3   v37f8.jpg  [[[[770.736   122.24625], [781.85016 114.03099...  non-violent   \n",
              "4  v37f26.jpg  [[[[1113.5089  347.87  ], [1126.2001  341.211 ...  non-violent   \n",
              "\n",
              "  set_split  \n",
              "0      test  \n",
              "1      test  \n",
              "2      test  \n",
              "3      test  \n",
              "4      test  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77fd66d0-84ef-4508-bd7f-555f3bcd1db3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>keypoints</th>\n",
              "      <th>label</th>\n",
              "      <th>set_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v37f2.jpg</td>\n",
              "      <td>[[[[1504.3322  260.4124], [1521.8273   255.220...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v37f4.jpg</td>\n",
              "      <td>[[[[735.87787  116.309715], [746.5505  110.004...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v37f1.jpg</td>\n",
              "      <td>[[[[1510.4242   265.19766], [1526.0929  258.19...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v37f8.jpg</td>\n",
              "      <td>[[[[770.736   122.24625], [781.85016 114.03099...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v37f26.jpg</td>\n",
              "      <td>[[[[1113.5089  347.87  ], [1126.2001  341.211 ...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77fd66d0-84ef-4508-bd7f-555f3bcd1db3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-77fd66d0-84ef-4508-bd7f-555f3bcd1db3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-77fd66d0-84ef-4508-bd7f-555f3bcd1db3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-35e86740-19ee-419c-afdf-86fd22d028c1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35e86740-19ee-419c-afdf-86fd22d028c1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-35e86740-19ee-419c-afdf-86fd22d028c1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4178,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3973,\n        \"samples\": [\n          \"v32f130.jpg\",\n          \"v49f127.jpg\",\n          \"v24f115.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keypoints\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"violent\",\n          \"non-violent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"set_split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"train\",\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keypoints_train = df[df.set_split == \"train\"]\n",
        "keypoints_test = df[df.set_split == \"test\"]"
      ],
      "metadata": {
        "id": "u10ZqA-a4wns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_df_by_paths(df, file_paths):\n",
        "    df[\"path\"] = \"/content/drive/MyDrive/AIRTLab/\" + df[\"set_split\"] + \"/\" + df[\"label\"] + \"/\" + df[\"filename\"]\n",
        "\n",
        "    order_df = pd.DataFrame({\"path\": file_paths})\n",
        "    order_df[\"order\"] = range(len(order_df))\n",
        "\n",
        "    merged = df.merge(order_df, on=\"path\", how=\"inner\")\n",
        "    merged = merged.sort_values(\"order\").drop(columns=[\"path\", \"order\"])\n",
        "\n",
        "    return merged.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DZeVA1XHq1Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = treino.file_paths\n",
        "df = pd.read_pickle('keyframes_keypoints.pkl')\n",
        "new_df = reorder_df_by_paths(df, file_paths)"
      ],
      "metadata": {
        "id": "S0GXTQKwq3-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "\n",
        "def keypoint_statistics_features(keypoints_sequence):\n",
        "    if keypoints_sequence.shape[0] < 1:\n",
        "        return None  # Not enough frames\n",
        "\n",
        "    smoothed_sequence = uniform_filter1d(keypoints_sequence, size=3, axis=0, mode='nearest')\n",
        "\n",
        "    stats = [\n",
        "        np.min(smoothed_sequence, axis=0),\n",
        "        np.max(smoothed_sequence, axis=0),\n",
        "        np.mean(smoothed_sequence, axis=0),\n",
        "        np.std(smoothed_sequence, axis=0),\n",
        "        np.median(smoothed_sequence, axis=0),\n",
        "        np.max(smoothed_sequence, axis=0) - np.min(smoothed_sequence, axis=0),\n",
        "        np.percentile(smoothed_sequence, 25, axis=0),\n",
        "        np.percentile(smoothed_sequence, 75, axis=0),\n",
        "    ]\n",
        "\n",
        "\n",
        "    features = [stat.flatten() for stat in stats]\n",
        "    return np.concatenate(features)\n",
        "\n",
        "def aggregate_video_features(video_person_skeletons, pooling='mean'):\n",
        "    person_features = []\n",
        "\n",
        "    for keypoints_sequence in video_person_skeletons:\n",
        "        feats = keypoint_statistics_features(np.array(keypoints_sequence))\n",
        "        if feats is not None:\n",
        "            person_features.append(feats)\n",
        "\n",
        "    if not person_features:\n",
        "        return np.zeros(204)\n",
        "\n",
        "    person_features = np.array(person_features)\n",
        "\n",
        "    if pooling == 'mean':\n",
        "        return np.mean(person_features, axis=0)\n",
        "    elif pooling == 'max':\n",
        "        return np.max(person_features, axis=0)\n",
        "    else:\n",
        "        raise ValueError(\"Pooling must be 'mean' or 'max'\")"
      ],
      "metadata": {
        "id": "eV9dodmnvCCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keypoints_list = np.array(new_df['keypoints'].copy())\n",
        "video_features = []\n",
        "for video in keypoints_list:\n",
        "    # `video` is a dict: {person_id: [skeletons_per_frame]}\n",
        "    fvec = aggregate_video_features(video, pooling='max')\n",
        "    video_features.append(fvec)"
      ],
      "metadata": {
        "id": "fcPi7F0bxM3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df['feature_vectors'] = video_features"
      ],
      "metadata": {
        "id": "io-8kJVVymRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tZAUHkq6yvDj",
        "outputId": "cbd1879c-d350-44ba-8ee9-0d618656dea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      filename                                          keypoints  \\\n",
              "0  v58f122.jpg  [[[[691.67084 749.1396 ], [689.95734 745.5522 ...   \n",
              "1    v4f24.jpg  [[[[1421.1746   226.76962], [1432.3367   218.3...   \n",
              "2  v35f119.jpg  [[[[1391.4457   361.47284], [1395.4491   351.2...   \n",
              "3    v9f92.jpg  [[[[914.76764 323.44522], [917.78033 311.57108...   \n",
              "4  v86f107.jpg  [[[[1446.7942   494.67288], [1461.2084   485.6...   \n",
              "\n",
              "         label set_split                                    feature_vectors  \n",
              "0      violent     train  [696.0596, 173.64444, 698.8789, 163.12158, 700...  \n",
              "1  non-violent     train  [1270.2258, 248.55927, 1274.2936, 240.27287, 1...  \n",
              "2  non-violent     train  [1298.3293, 349.26022, 1298.6206, 338.50943, 1...  \n",
              "3  non-violent     train  [901.1548, 147.03871, 909.8066, 135.22134, 896...  \n",
              "4      violent     train  [849.5586, 403.15393, 857.6884, 394.13953, 841...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3673be1b-81fc-4856-8c8a-8546bd725bc1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>keypoints</th>\n",
              "      <th>label</th>\n",
              "      <th>set_split</th>\n",
              "      <th>feature_vectors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v58f122.jpg</td>\n",
              "      <td>[[[[691.67084 749.1396 ], [689.95734 745.5522 ...</td>\n",
              "      <td>violent</td>\n",
              "      <td>train</td>\n",
              "      <td>[696.0596, 173.64444, 698.8789, 163.12158, 700...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v4f24.jpg</td>\n",
              "      <td>[[[[1421.1746   226.76962], [1432.3367   218.3...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>train</td>\n",
              "      <td>[1270.2258, 248.55927, 1274.2936, 240.27287, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v35f119.jpg</td>\n",
              "      <td>[[[[1391.4457   361.47284], [1395.4491   351.2...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>train</td>\n",
              "      <td>[1298.3293, 349.26022, 1298.6206, 338.50943, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v9f92.jpg</td>\n",
              "      <td>[[[[914.76764 323.44522], [917.78033 311.57108...</td>\n",
              "      <td>non-violent</td>\n",
              "      <td>train</td>\n",
              "      <td>[901.1548, 147.03871, 909.8066, 135.22134, 896...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v86f107.jpg</td>\n",
              "      <td>[[[[1446.7942   494.67288], [1461.2084   485.6...</td>\n",
              "      <td>violent</td>\n",
              "      <td>train</td>\n",
              "      <td>[849.5586, 403.15393, 857.6884, 394.13953, 841...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3673be1b-81fc-4856-8c8a-8546bd725bc1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3673be1b-81fc-4856-8c8a-8546bd725bc1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3673be1b-81fc-4856-8c8a-8546bd725bc1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4a852f72-9a7f-4443-b445-b8d03ac9d72c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4a852f72-9a7f-4443-b445-b8d03ac9d72c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4a852f72-9a7f-4443-b445-b8d03ac9d72c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "new_df",
              "summary": "{\n  \"name\": \"new_df\",\n  \"rows\": 2926,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2821,\n        \"samples\": [\n          \"v41f45.jpg\",\n          \"v35f4.jpg\",\n          \"v32f52.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keypoints\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"non-violent\",\n          \"violent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"set_split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"train\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_vectors\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    }
  ]
}